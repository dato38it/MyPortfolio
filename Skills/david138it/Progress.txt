Feedback: david138it@gmail.com; telegram - @david138it; http://www.linkedin.com/in/david-gabuniya-3bb954237; https://github.com/David138it;

Виртуализация в Yandex Cloud
  Task:
  Для защиты практической работы по теме "Виртуализация в Yandex Cloud" настроил в Yandex Cloud больше десятка виртуальных машин на базе OC Linux
  Task:
  Создание аккаунта. Для того, чтобы работать в облаке, нужен платёжный аккаунт. Если это ваш первый аккаунт в Yandex Cloud, то после его создания вы сможете активировать 60-дневный пробный период и получить стартовый грант. Это позволит вам выполнять практические работы, не тратя собственных средств.
  Decision:
  Если у вас нет личного платёжного аккаунта: Откройте в браузере сайт cloud.yandex.ru. Если вы не авторизованы, в правом верхнем углу или на баннере нажмите кнопку Подключиться. - Войдите в свой аккаунт на Яндексе. - Если у вас его нет, то вам будет предложено создать новый Яндекс ID. При регистрации в Яндекс ID заполните все поля, в том числе номер телефона: он потребуется, чтобы создать платёжный аккаунт в Yandex Cloud. - Примите условия использования Yandex Cloud. - После аутентификации при первом входе в консоль управления вам будет предложено создать облако. Укажите название облака и нажмите кнопку Создать. - Вам будет назначена роль владельца — owner. - На этом этапе у вас есть облако, но нет платёжного аккаунта. Если вы перейдете в раздел Биллинг (в левом верхнем углу выберите меню Все сервисы -> Биллинг), то увидите, что в списке аккаунтов пока пусто. - Чтобы создать платёжный аккаунт, нажмите кнопку Создать аккаунт в разделе Биллинг или на стартовой странице. При создании аккаунта заполните все данные (выберите тип плательщика — Физическое лицо) и добавьте банковскую карту. Для проверки система спишет с неё небольшую сумму денег, а затем сразу вернёт на счёт. - Важно! Обязательно выберите опцию Включить пробный период. Если этого не сделать, то ваш платежный аккаунт будет сразу переведен в режим платного потребления. - Пробный период позволяет использовать ресурсы Yandex Cloud в ограниченном режиме в течение 60 дней. Потреблённые ресурсы оплачиваются из стартового гранта. После завершения пробного периода ваши ресурсы в облаке будут остановлены, а чтобы возобновить работу в полном объёме потребуется перейти на платную версию. - Важно! Не отвязывайте банковскую карту во время пробного периода: в этом случае облако заблокируется. Если вы не перейдёте на платную версию, средства с карты списываться не будут.
  $ google-chrome https://cloud.yandex.ru/ &
  После аутентификации при первом входе в консоль управления вам будет предложено создать облако. Укажите название облака и нажмите кнопку Создать. Вам будет назначена роль владельца — owner.
  Чтобы создать платёжный аккаунт, нажмите кнопку Создать аккаунт в разделе Биллинг или на стартовой странице. При создании аккаунта заполните все данные (выберите тип плательщика — Физическое лицо) и добавьте банковскую карту. Для проверки система спишет с неё небольшую сумму денег, а затем сразу вернёт на счёт.
  Task:
  Создать ВМ на базе OC Linux из консоли управления.
  Decision:
  Откройте консоль управления облаком. Если это новое облако, которое вы создали в предыдущей практической работе, то вы окажетесь на странице дашборда каталога с именем default. Этот каталог и облачная сеть в нём с таким же именем создаются вместе с облаком автоматически.
  Выберите сервис Compute Cloud из списка (Дашборд каталога → Все сервисы → Compute Cloud) и нажмите кнопку Создать ВМ.
  В открывшемся окне понадобится указать параметры создаваемой ВМ. В блоке Базовые параметры укажите Имя ВМ, оно может содержать строчные латинские буквы, цифры и дефисы. Поле Описание необязательное — его обычно заполняют, чтобы не запутаться, если ВМ несколько. Выберите из списка Зону доступности ru-central1-a.
  На вкладке Операционные системы В блоке Выбор образа/загрузочного диска выберите ОС, которая будет установлена на ВМ, — Ubuntu 22.04.
  В блоке Диски оставьте значения по умолчанию: тип — HDD, размер — 18 ГБ.
  В блоке Вычислительные ресурсы оставьте значения по умолчанию: платформа Intel Ice Lake, 2 ядра виртуального процессора (vCPU), гарантированная доля vCPU 100% и объём оперативной памяти (RAM) 2 ГБ.
  В блоке Сетевые настройки оставьте используемую по умолчанию подсеть, а для Публичного адреса и Внутреннего адреса — опцию Автоматически.
  В блоке Доступ заполните поле Логин  (имя пользователя создаваемой ВМ).
  Не указывайте идентификатор root или другие имена, зарезервированные операционной системой. Для операций, требующих прав суперпользователя, нужно будет использовать команду sudo.
  Чтобы подключиться к создаваемой ВМ по протоколу SSH, понадобится пара SSH-ключей. Открытый ключ хранится на ВМ, а закрытый — у пользователя. В публичных образах Linux, предоставляемых Yandex Cloud, возможность подключения по SSH с использованием логина и пароля по умолчанию отключена.
  создать SSH-ключи
  После выполнения команды укажите имена файлов, куда сохранятся ключи, и введите пароль для закрытого ключа. По умолчанию используется имя id_rsa, ключи создаются в папке ~/.ssh текущего пользователя.
  Открытый ключ сохранится в файле <имя_ключа>.pub. Содержимое этого файла вставляется в поле SSH-ключ виртуальной машины.
  В поле SSH-ключ вставьте содержимое созданного открытого ключа. Убедитесь, что вставляете ключ без переносов строки. Например: ssh-rsa AAAAB3NzaC1yc2EAAAABJQAAAQEAoIT+oFLFEHwNlGO71wZiamqHkzduK7V/B8ITxgLnddm725QZJbaO1JAUfaOryGckWqEHr0NQxZ+CfozLjtYwcYhnPfNs1vw7Ii5gnL4ne+Vu5Kl4f8rb+tOXAv6GAZIO1+05kB8K3nINfBkKFD1J0VmOr5P2MWy7aqdbyIqVJCH+YeU4SW5RGFPJbl5zGhlwSavVU0bgTYQmqWAOnR95bQVx1vRf4SyB003C8MYl8ccZ+emixM12eQPJ74fJyy1kKLRmU/IAlxyEiYESQglAaNQKN2ivnbfMaSVBnxMlYipxyeMDyCs8RD7zVUndTOJQg8PV7QVWqfAQjlY4uYlk8Q== rsa-key-20210429 
  Изменяя параметры ВМ, в блоке Тарифы и цены (справа) вы увидите, как меняется её месячная стоимость.
  Нажмите кнопку Создать ВМ.
  Развёртывание ВМ занимает несколько минут. Вы можете отслеживать этот процесс по смене статуса.
  Статус ВМ влияет на то, какие операции вы можете с ней выполнять. Например, статус Stopped означает, что машина остановлена и к ней невозможно подключиться. При запуске статус меняется на Provisioning (Yandex Cloud выделяет ВМ ресурсы), а после загрузки операционной системы — на Running.
  Теперь, когда ВМ создана и запущена, к ней можно подключиться. Для этого понадобятся логин пользователя и публичный IP-адрес ВМ, который можно скопировать из строки с информацией о ней на странице Виртуальные машины.
  Чтобы подключиться к запущенной ВМ (статус RUNNING) по протоколу SSH, используют утилиту ssh в Linux/macOS или Windows 10/11, программу PuTTY в Windows 7/8 или любой другой клиент SSH.
  Если у вас несколько закрытых ключей, укажите нужный: ssh -i <путь_к_ключу/имя_файла_ключа> <имя_пользователя>@<публичный_IP-адрес_ВМ> 
  Установите обновления.
  ВМ готова к работе
  Decision:
  $ ssh-keygen -t rsa -b 2048
  $ cat davidk.pub
  $ ssh -i davidk david@178.154.223.142
  $ sudo apt-get update
  $ sudo apt-get upgrade
  Task:
  Работа серийной консоли зависит от настроек операционной системы. Yandex Compute Cloud обеспечивает канал связи между пользователем и COM-портом ВМ, но не гарантирует стабильность работы консоли со стороны операционной системы ВМ.
  Доступ к серийной консоли ВМ с ОС на базе Linux возможен следующими способами: по протоколу SSH с другого компьютера; через консоль управления Yandex Cloud; с помощью интерфейса командной строки Yandex Cloud CLI. 
  Task:
  Вход по протоколу SSH. Подключитесь к ВМ по протоколу SSH. Установите пароль текущему пользователю с помощью утилиты passwd в привилегированном режиме: sudo passwd <имя_пользователя> 
  После ввода команды дважды наберите одинаковый пароль.
  Для доступа к серийной консоли ВМ необходимо знать её идентификатор (ID). В консоли управления перейдите в раздел Compute Cloud. По умолчанию откроется страница со списком ВМ. В столбце справа указан идентификатор каждой ВМ.
  Используйте для входа идентификатор ВМ и имя (логин) созданного в ней пользователя. Вот шаблон команды подключения для Linux: ssh -t -p 9600 -o IdentitiesOnly=yes -i ~/.ssh/<имя закрытого ключа> <ID виртуальной машины>.<имя пользователя>@serialssh.cloud.yandex.net 
  Вот так вы подключитесь к консоли, если в ВМ с ID fhm0b28lgfp4tkoa3jl6 есть пользователь yc-user: ssh -t -p 9600 -o IdentitiesOnly=yes -i ~/.ssh/id_rsa fhm0b28lgfp4tkoa3jl6.yc-user@serialssh.cloud.yandex.net 
  Чтобы отключиться от серийной консоли, нажмите клавишу Enter, а затем введите символы ~. (тильда и точка). В терминалах Linux для отключения также можно использовать комбинацию клавиш Ctrl + D.
  Task:
  Вход через консоль управления
  Decision:
  В консоли управления откройте страницу ВМ и через меню слева перейдите на страницу Серийная консоль. При авторизации используйте логин, указанный при создании ВМ, и пароль, который вы установили после подключения к ней.
  Decision:
  $ sudo passwd david
  $ exit
  $ ssh -t -p 9600 -o IdentitiesOnly=yes -i davidk fhmm3t1r6n0e6s5q2u3h.david@serialssh.cloud.yandex.net
  Task:
  Создаем ВМ с 5% vCPU и учимся использовать мониторинг
  Давайте создадим ВМ с гарантированной долей vCPU, равной 5%, и проверим её производительность.
  Decision:
  В консоли управления перейдите в раздел Compute Cloud и нажмите кнопку Создать ВМ. Заполните имя и описание, выберите операционную систему CentOS 7.
  В блоке Вычислительные ресурсы выберите платформу Intel Cascade Lake и укажите гарантированную долю vCPU 5%. Другие параметры оставьте по умолчанию.
  После создания и запуска ВМ в списке машин нажмите её название. Вы перейдёте на страницу ВМ. Затем на левой боковой панели выберите Мониторинг. Откроется страница, где в динамике показывается информация о загрузке процессора, операциях с диском и сетевой активности. По умолчанию видны данные за одни сутки (1d — 1 day).
  Переключитесь на один час: вверху слева нажмите 1h (1 hour).
  На графике видно, что при запуске использование процессорных ресурсов было высоким, а позже снизилось до приемлемого. Чтобы посмотреть точные значения в определённый момент, поместите указатель над линией графика. Вы увидите всплывающее окно с показателями для этой точки времени.
  Теперь удалите ВМ. Для этого сначала остановите её — вернитесь в список ВМ, отметьте нужную ВМ и на появившейся внизу контекстной панели нажмите Остановить.
  Во всплывающем окне подтвердите действие и нажмите кнопку Остановить. Дождитесь смены статуса на Stopped.
  Чтобы удалить ВМ, в списке ВМ справа напротив машины нажмите ... и в раскрывшемся меню выберите Удалить. Подтвердите действие. Через некоторое время ВМ будет удалена.
  Task:
  Создаем снимок диска ВМ
  Допустим, вы планируете обновить ПО на виртуальной машине ВМ. Вы знаете, что взаимодействие приложений и системных сервисов после обновления может нарушиться, а данные могут быть повреждены. Поэтому хотите иметь резервную копию полностью работоспособной системы на случай неудачи.
  Давайте на практике разберём, как сделать снимок и восстановить из него ВМ при повреждении системы. Для этого используем ВМ, на которой развернута система на основе Ubuntu или CentOS.
  Decision:
  В первую очередь обеспечьте целостность данных. Для этого подключитесь к ВМ по SSH. Чтобы записать кеш операционной системы на диск, выполните команду sync (иначе изменения файлов, хранящиеся в оперативной памяти, будут потеряны). Диски в Linux монтируются в ОС в виде файлов. Чтобы узнать нужный файл устройства диска, выполните команду df -h для вывода полного списка устройств и соответствующих точек монтирования. Затем, чтобы заморозить файловую систему, выполните команду fsfreeze -f <точка монтирования>.
  В консоли управления откройте раздел Compute Cloud и перейдите на вкладку Диски. Справа от диска нажмите ... и выберите Создать снимок.
  В открывшемся окне вы увидите автоматически сформированное имя снимка диска. Оно состоит из идентификатора диска и идентификатора снимка. Вы можете переименовать снимок и заполнить его описание. После этого нажмите кнопку Создать.
  Откройте Снимки дисков. Как только снимок будет создан, статус операции сменится с Creating на Ready.
  Разморозьте файловую систему. Для этого в командной строке с интерфейсом подключения к ВМ по SSH выполните команду fsfreeze --unfreeze <точка монтирования>.
  Теперь протестируйте обновление системы: в командной строке с интерфейсом подключения к ВМ по SSH последовательно выполните команды apt-get update и apt-get dist-upgrade. Дождитесь, пока обновление завершится. Важно! Перед выполнением следующей команды убедитесь в том, что находитесь в консоли именно тестовой ВМ.
  Сымитируйте повреждение системы: выполните команду sudo rm -rf --no-preserve-root /. Вы увидите предупреждение, что все данные на диске будут удалены. Подтвердите своё намерение.
  Поскольку снимок создан с загрузочного диска, который всегда подключён к ВМ, для восстановления создайте новую ВМ вместо старой. При создании загрузочного диска машины выберите готовый снимок диска. Для этого в блоке Выбор образа/загрузочного диска перейдите на вкладку Пользовательские и нажмите кнопку Выбрать. В открывшемся окне перейдите на вкладку Снимок, выберите нужный снимок и нажмите кнопку Применить.
  Дождитесь завершения создания и запуска новой ВМ. Теперь старую ВМ можно остановить и удалить.
  Decision:
  $ ssh -i ubcloud test138@51.250.93.58
  $ sudo fsfreeze -f /mnt
  $ sudo fsfreeze --unfreeze /mnt
  $ sudo apt-get update
  $ sudo apt-get dist-upgrade
  $ sudo rm -rf --no-preserve-root /
  Task:
  Создание новой сети с подсетями и ВМ. 
  Облачные сети (сервис Virtual Private Cloud или VPC) являются частью публичного облака, которая связывает пользовательские, инфраструктурные, платформенные и прочие ресурсы воедино, где бы они ни находились — в нашем облаке или за его пределами. При этом VPC позволяет не публиковать эти ресурсы в интернете без необходимости, они остаются в пределах вашей изолированной сети.
  Когда вы создаёте облако, в нём автоматически создаются сеть default и подсети в каждой зоне доступности. Но иногда их бывает недостаточно. В этой практической работе вы научитесь создавать облачную сеть и добавлять подсети самостоятельно.
  Рассмотрим пример, как настроить облачную сеть, чтобы организовать работу сервера с доступом из интернета. 
  Сначала создадим единую для всех ресурсов облака изолированную сеть с подсетями и виртуальной машиной.
  Decision:
  В консоли управления перейдите на страницу сервиса Virtual Private Cloud (Дашборд каталога → Virtual Private Cloud) и нажмите кнопку Создать сеть вверху справа. Введите имя сети (пусть она называется yc), поле Описание заполнять необязательно. Оставьте выбранной опцию Создать подсети и нажмите кнопку Создать сеть. В результате будут созданы сеть yc и три подсети: yc-ru-central1-a, yc-ru-central1-b и yc-ru-central1-c.
  Для сервера создадим ещё одну подсеть с маской /28. Перейдите на страницу сети yc (Дашборд каталога → Virtual Private Cloud → сеть yc) и нажмите кнопку Добавить подсеть. Введите параметры подсети: Имя — yc-public, Зона — ru-central1-a, CIDR — 192.168.0.0/28. Нажмите кнопку Создать подсеть.
  Доступом пользователей облака к сетевым ресурсам управляют с помощью назначения ролей.
  Теперь создайте ВМ с именем web-server и ОС Ubuntu 22.04. Убедитесь, что в блоке Базовые параметры выбрана зона доступности ru-central1-a. В блоке Сетевые настройки выберите Подсеть yc-public. В блоке Доступ введите логин пользователя (например user) и вставьте открытый SSH-ключ в соответствующее поле. Чтобы ВМ полноценно заработала, для неё нужно организовать доступ в интернет. Есть три способа:
  - Назначить ВМ публичный IP-адрес. Для этого выберите автоматический способ назначения IP-адреса, когда создаёте ВМ, или привяжите его к уже созданной.
  - Использовать NAT-шлюз, который позволяет дать ВМ доступ в интернет без назначения ей публичного IP-адреса. Самому шлюзу адрес выделяется из отдельного диапазона публичных IP-адресов.
  - Использовать NAT-инстанс — отдельную ВМ со статическим публичным IP-адресом и настроенными правилами маршрутизации трафика.
  С точки зрения безопасности лучше выбирать второй или третий способ. В этом примере для упрощения присвойте ВМ публичный IP-адрес — в поле Публичный адрес выберите опцию Автоматически.
  После создания ВМ проверьте её доступность, чтобы убедиться в том, что сетевая конфигурация настроена правильно. Для этого перейдите на страницу Обзор созданной ВМ (Дашборд каталога → Compute Cloud → ВМ web-server) и в блоке Сеть найдите её публичный IP-адрес. Откройте интерфейс командной строки на своём компьютере и введите команду:
  $ ping 51.250.78.143 
  Task:
  Давайте попробуем создать группу безопасности и сделать доступными страницы, предоставляемые веб-сервером NGINX.
  Decision:
  Перейдите по ссылке https://console.cloud.yandex.ru/link/vpc/security-groups и нажмите кнопку Создать группу.
  Введите имя группы yc-security и выберите сеть yc.
  В блоке Правила добавьте правила для исходящего трафика. Опишите правило, укажите диапазон портов 80 (HTTP) и протокол TCP, выберите назначение "CIDR" 0.0.0.0/0 . Создайте аналогичные исходящие правила для портов 443 (HTTPS) и 22 (SSH). Для входящего трафика добавьте правила для 80 и 22 портов, чтобы подключаться к веб-серверу и управлять ВМ извне. Вы увидите созданную группу в списке групп безопасности. Если инициировано соединение по определенному порту и протоколу с ВМ и есть исходящее правило, то значит и на входящий трафик будет разрешена передача данных в эту же сеть, на этот же протокол и порт. Если назначить сетевому интерфейсу ВМ группу безопасности без правил, ВМ не сможет передавать и принимать трафик.
  Создайте виртуальную машину на базе Ubuntu 20.04, выберите сеть yc и вновь созданную группу безопасности yc-security в сетевых настройках создаваемой ВМ, дождитесь запуска машины, подключитесь к ВМ по SSH и установите веб-сервер NGINX (по умолчанию он отсутствует). Для этого выполните команду:
  Установка возможна, поскольку вы открыли порт 80: команда apt-get использует его для получения пакетов с ПО.
  После установки сервер автоматически запустится и будет доступен извне благодаря открытому порту 80. Проверьте это, зайдя через браузер на публичный IP-адрес ВМ, который найдёте на странице параметров машины в консоли управления.
  Decision:
  $ sudo apt-get install nginx
  Task:
  Знакомство с Yandex Cloud CLI.
  Decision:
  Развернём две ВМ, запустим на них веб-серверы NGINX и поместим их за балансировщик. Создавать ВМ через консоль управления облаком вы уже умеете. Вот только для реальных проектов часто приходится разворачивать далеко не одну-две ВМ. В этом случае работать через консоль управления долго и неудобно, да и вероятность ошибиться, выполняя много однотипных операций вручную, растёт. Решить эту проблему можно с помощью автоматизации, а один из самых простых способов это сделать — использовать интерфейс командной строки (CLI, Command Line Interface).
  Установите на свой компьютер утилиту yc, которая представляет собой интерфейс командной строки Yandex Cloud CLI, и настройте её (создайте профиль) по инструкции в документации.
  Создайте файл startup.sh Скрипт, который получает список актуальных пакетов с софтом, устанавливает и запускает NGINX, а затем меняет информационную страницу работающего сервера.
  Создайте первую ВМ с помощью команды yc compute instance create. Чтобы указать, какую именно ВМ мы хотим создать, в команде используются параметры. В данном случае — имя ВМ (--name demo-1), откуда взять метаданные (из файла скрипта startup.sh, созданного ранее), из какого образа создать загрузочный диск (ubuntu-2004-lts), в какой зоне создать машину (--zone ru-central1-a) и в какую подсеть подключить сетевой интерфейс с IPv4-адресом (--network-interface ...).
  Самостоятельно измените и запустите еще раз эту команду, чтобы создать такую же ВМ с именем demo-2.
  Теперь проверим, что обе ВМ успешно созданы. Это можно сделать, заглянув в консоль управления. А можно и из командной строки с помощью команды yc compute instance list. Попробуйте этот способ. 
  Убедитесь, что статус машин сменился на Running, а скрипт выполнился (иногда нужно подождать до одной минуты, пока обновится список пакетов и установится NGINX).
  Введите публичные IP-адреса ВМ в браузере и проверьте, что главные страницы веб-серверов доступны
  Decision:
  $ curl -sSL https://storage.yandexcloud.net/yandexcloud-yc/install.sh | bash
  $ sudo reboot
  $ yc init
  $ yc config list
  $ yc resource-manager folder add-access-binding default \
  --role compute.images.user \
  --subject system:allAuthenticatedUsers
  $ yc compute image list --folder-id standard-images
  $ vim startup.sh
  $ cat startup.sh
  #!/bin/bash
  apt-get update
  apt-get install -y nginx
  service nginx start
  sed -i -- "s/nginx/Yandex Cloud - ${HOSTNAME}/" /var/www/html/index.nginx-debian.html
  EOF
  $ chmod +x startup.sh
  $ yc compute instance create \
  --name tcloud-ubuntu3 \
  --hostname tcloud-ubuntu3 \
  --metadata-from-file user-data=startup.sh \
  --create-boot-disk image-folder-id=standard-images,image-family=ubuntu-2004-lts \
  --zone ru-central1-a \
  --network-interface subnet-name=default-ru-central1-a,nat-ip-version=ipv4
  $ yc compute instance create \
  --name tcloud-ubuntu4 \
  --hostname tcloud-ubuntu4 \
  --metadata-from-file user-data=startup.sh \
  --create-boot-disk image-folder-id=standard-images,image-family=ubuntu-2004-lts \
  --zone ru-central1-a \
  --network-interface subnet-name=default-ru-central1-a,nat-ip-version=ipv4
  $ yc compute instance list
  Task:
  Создание балансировщика
  Итак, у вас есть виртуальные машины. Можно сразу создать и балансировщик, и целевую группу, но мы поступим иначе: сначала создадим целевую группу, затем подключим её к балансировщику.
  Decision:
  В консоли управления откройте раздел Network Load Balancer, на вкладке Целевые группы нажмите кнопку Создать целевую группу. На открывшейся странице введите имя целевой группы (например demo-web), выберите обе ВМ, созданные на предыдущем уроке, и нажмите кнопку Создать.
  Остаётся создать балансировщик. Для этого сначала создайте обработчик и настройте проверку состояния ресурсов в целевой группе:
  На вкладке Балансировщики нажмите кнопку Создать сетевой балансировщик.
  Заполните имя балансировщика (например lb-demo-web) и нажмите кнопку Добавить обработчик.
  В открывшемся окне введите имя обработчика (например demo-web-listener). В качестве портов укажите 80и нажмите кнопку Добавить.
  После создания обработчика нажмите кнопку Добавить целевую группу. Укажите имя проверки состояния (например hc-demo-web), тип проверки (HTTP), порт (80), интервал отправки проверок состояния в секундах, порог работоспособности и порог неработоспособности. Оставьте указанный по умолчанию путь для проверок, используйте значения по умолчанию и для других параметров. Нажмите кнопку Применить, а затем кнопку Создать.
  После создания балансировщика проверьте состояние ресурсов: в консоли управления откройте страницу балансировщика и убедитесь, что его статус — Active. Значит, балансировщик готов передавать трафик целевым ресурсам.
  Перейдите на страницу балансировщика и посмотрите на блок Целевые группы. У запущенных ВМ, готовых принимать трафик, будет статус Healthy.
  Введите внешний IP-адрес балансировщика в адресную строку браузера — и балансировщик перенаправит вас на одну из машин целевой группы. Обратите внимание на имя ВМ, указанное во второй строке веб-страницы.
  Чтобы протестировать отказоустойчивость, в консоли управления перейдите в раздел Compute Cloud и остановите одну из ВМ целевой группы.
  Вернитесь на страницу балансировщика и убедитесь, что статус остановленной ВМ изменился на Unhealthy. Это означает, что целевой ресурс группы не прошёл проверку состояния и не готов принимать трафик.
  Обновите страницу с IP-адресом балансировщика, и вы увидите, что трафик перенаправлен на другую ВМ (изменилось имя ВМ, указанное во второй строке веб-страницы).
  После завершения работы не забудьте удалить использованные ресурсы: две ВМ и балансировщик.
  Task:
  Создание группы виртуальных машин
  Иногда вам требуется не автоматическое масштабирование, а автоматическое восстановление ВМ. 
  Например, если вы отлаживаете работу веб-сервиса, который периодически падает. 
  Для этого подойдут группы ВМ фиксированного размера. Давайте создадим и настроим такую группу.
  Decision:
  В консоли управления откройте раздел Compute Cloud, перейдите на вкладку Группы виртуальных машин и нажмите кнопку Создать группу.
  Откроется страница Создание группы виртуальных машин.
  В блоке Базовые параметры введите имя и описание группы ВМ. Создайте новый сервисный аккаунт. Чтобы иметь возможность создавать, обновлять и удалять ВМ в группе, назначьте сервисному аккаунту роль editor. По умолчанию все операции в группе ВМ выполняются от имени сервисного аккаунта.
  ВМ группы могут находиться в разных зонах и регионах. В блоке Распределение выберите две зоны доступности, чтобы обеспечить доступность сервиса, если в одной из них случится сбой.
  В блоке Шаблон виртуальной машины нажмите кнопку Задать.
  Шаблон создается так же, как и сама ВМ. В блоке Базовые параметры введите описание шаблона конфигурации, затем в блоке Выбор образа/загрузочного диска на вкладке Операционные системы выберите Ubuntu.
  В блоках Диски и Вычислительные ресурсы для загрузочного диска оставьте значения по умолчанию. В блоке Сетевые настройки выберите существующую сеть и подсеть или создайте новые. В блоке Доступ выберите существующий или создайте новый сервисный аккаунт, укажите логин, вставьте в поле SSH-ключ содержимое файла с публичным ключом, доступ к серийной консоли не разрешайте.
  Сохраните параметры и вы вернётесь на страницу Создание группы виртуальных машин.
  В блоке В процессе создания и обновления разрешено установите политику развертывания: Добавлять выше целевого значения (на сколько ВМ можно превышать размер группы) — 2. Уменьшать относительно целевого значения (на сколько ВМ можно уменьшать размер группы) — 1. Одновременно создавать (сколько ВМ можно сразу создавать в группе) — 2. Время запуска (сколько времени должно пройти, прежде чем будут пройдены все проверки состояния и ВМ начнет получать нагрузку) — 2 минуты. Одновременно останавливать (сколько ВМ можно сразу удалять) — 1. Останавливать машины по стратегии — Принудительная. При принудительной стратегии Instance Groups самостоятельно выбирает, какие ВМ остановить.
  В блоке Масштабирование выберите фиксированный тип, Размер (количество ВМ) — 3.
  В блоке Интеграция с Load Balancer оставьте опцию Создать целевую группу выключенной. Не включайте пока проверку состояний, которая позволяет Instance Groups получать сведения о состоянии ВМ.
  Нажмите кнопку Создать и вернитесь на страницу Группы виртуальных машин. В правом нижнем углу появится сообщение «Группа виртуальных машин создаётся». Одновременно можно создавать не более двух ВМ. Поэтому сначала будут созданы две ВМ, потом — третья.
  После того как вы создали группу, протестируйте включение и выключение всех машин сразу. Обратите внимание: в соответствии с настройками сервис инициирует запуск не более двух машин одновременно. Третья ВМ будет оставаться остановленной. Как только первая будет запущена, один слот на запуск освободится, поэтому сразу будет инициирован запуск третьей и последней ВМ.
  Task:
  Автоматическое масштабирование под нагрузкой
  Давайте разберёмся, как обеспечить доступность сервиса под высокой нагрузкой. Вы уже научились создавать группы ВМ. Теперь создадим автоматически масштабируемую группу ВМ.
  Decision:
  В консоли управления откройте раздел Compute Cloud. Перейдите на вкладку Группы виртуальных машин и нажмите кнопку Создать группу. Задайте имя группе ВМ.
  Создайте сервисный аккаунт. Чтобы иметь возможность создавать, обновлять и удалять ВМ в группе, назначьте сервисному аккаунту роль editor. По умолчанию все операции в группе ВМ выполняются от имени сервисного аккаунта.
  В блоке Распределение выберите только одну зону доступности.
  В блоке Шаблон виртуальной машины нажмите кнопку Задать. В открывшемся окне выберите: ОС: Ubuntu 20.04. Размер загрузочного диска: 50 ГБ. Тип загрузочного диска: SSD.
  Остальные параметры — по умолчанию. Не забудьте добавить публичный SSH-ключ. Он понадобится нам на следующем практическом занятии.
  В блоке В процессе создания и обновления разрешено оставьте параметры по умолчанию.
  Перейдите к блоку Масштабирование и выберите тип Автоматический.
  Задайте параметры масштабирования: Тип автомасштабирования — зональное. При зональном автомасштабировании количество ВМ регулируется отдельно в каждой зоне доступности, указанной в настройках группы. Минимальное количество ВМ в зоне — 2. Сервис Instance Groups не будет удалять ВМ в зоне доступности, если их там всего две. Максимальный размер группы — 4. Instance Groups не будет создавать ВМ, если их уже четыре. В этот раз размер загрузочного диска ВМ — 50 ГБ, поэтому с учётом квот на суммарный объём SSD-дисков в одном облаке смогут запуститься четыре ВМ. Промежуток измерения загрузки (это период усреднения: время, за которое следует усреднять замеры нагрузки для каждой ВМ в группе) — 60 секунд. Время на разогрев ВМ — 3 минуты. В течение этого времени ВМ не учитывается в измерении средней нагрузки на группу. Фактически данное время мы можем определить, измерив, как быстро запускается ВМ. Период стабилизации — 5 минут. Отсчитывается с момента, когда Compute Cloud принял последнее решение о том, что количество ВМ в группе нужно увеличить. Начальный размер группы — 4. Это количество ВМ, которое следует создать вместе с группой.
  В блоке Метрики укажите: Тип — CPU. Целевой уровень загрузки CPU, % — 80. Instance Groups будет управлять размером группы так, чтобы поддерживать указанную нагрузку CPU.
  Нажмите кнопку Создать. Сервис начнет создавать ВМ. После создания статус группы изменится на Active. Обратите внимание, как меняются Состояния ВМ.
  Creating instance — ВМ создаётся и запускается. Awaiting warmup duration — ВМ начинает принимать сетевой трафик. В этом статусе ВМ находится в течение периода прогрева, указанного в настройках автоматического масштабирования. Значения метрик ВМ в этом статусе заменяются средними значениями ВМ из той же зоны доступности. Running actual — ВМ запущена, на неё подается сетевой трафик, пользовательские приложения работают.
  Группа ВМ готова принимать рабочую нагрузку.
  Task:
  Воссоздание виртуальных машин в группе.
  Давайте сымитируем рост нагрузки на ВМ и посмотрим, как сервис на это отреагирует.
  Decision:
  В консоли управления откройте раздел Compute Cloud и перейдите на страницу группы ВМ, которую вы создали на прошлом практическом занятии. Откройте в двух вкладках браузера страницы Группы виртуальных машин и Мониторинг (открывается из раздела Группы виртуальных машин).
  После запуска группы зайдите по SSH на каждую из двух ВМ и установите приложение для стресс-тестирования Linux-систем. Для этого выполните команду: sudo apt-get install stress 
  После этого для каждой ВМ запустите установленное приложение: stress -c 2 
  Аргумент -c означает, что при тестировании будет нагружаться процессор, а число после аргумента задаёт количество ядер процессора, которые будут нагружаться. Чтобы эксперимент удался — укажите количество ядер, которое вы выбрали в шаблоне ВМ. На вкладке со страницей мониторинга на графике Average CPU utilization in ru-central1-a следите за тем, как усреднённое значение нагрузки будет постепенно расти.
  Как только усреднённое значение нагрузки превысит порог, сервис Instance Groups начнёт прогревать две дополнительные ВМ и вводить их в строй. Это будет видно на странице Группы виртуальных машин.
  Поскольку стресс-тест не остановлен, сервис завершает запуск двух ВМ.
  Через некоторое время усреднённое значение нагрузки процессоров в группе упадёт до 50%, поскольку первая половина ВМ загружена полностью, а вторая не загружена вовсе. Остановите работу стресс-теста на первой ВМ. В командной строке используйте сочетание клавиш Ctrl + C.
  Через некоторое время усреднённое значение достигнет 25%, тогда Instance Groups удалит лишнюю ВМ. Остановите второй стресс-тест. Через некоторое время после того, как усредненное значение достигнет нуля, Instance Groups удалит вторую дополнительную ВМ. При минимальной нагрузке остаются работать две машины.
  Вот так при растущей нагрузке группа ВМ автоматически масштабируется, чтобы обеспечить доступность ресурса.
  Decision:
  $ ssh -i YOUR-KEY YOUR-USERNAME1@YOUR-IP1
  $ sudo apt-get install stress 
  $ stress -c 2
  $ ssh -i YOUR-KEY YOUR-USERNAME2@YOUR-IP2
  $ sudo apt-get install stress 
  $ stress -c 2

Хранение и анализ данных в Yandex Cloud
  Task:
  Для защиты практической работы по теме "Хранение и анализ данных в Yandex Cloud" развернул пять кластеров баз данных MySQL, PostgreSQL, MongoDB, ClickHouse и Ydb, добавил данные из файлов в БД ClickHouse для анализа прогноза за всю историю наблюдений за последние несколько лет с помощью SQL-запросов, добавил данные из тестового приложения для подключения к БД YDB, запуска тестового приложения, чтобы создать в ней несколько таблиц с данными о популярных сериалах, реализовал систему хранения рентгеновских снимков для клиники, развернул кластер Hadoop с помощью сервиса Yandex Data Proc
  Task:
  Создание бакетов и загрузка объектов
  Потренируемся работать с объектным хранилищем на практике. Представьте, что вы создаёте облачную систему хранения рентгеновских снимков для крупной клиники.
  Рентгеновские снимки — это неструктурированные данные, которые нельзя изменять, нужно надежно хранить и легко находить. 
  Загруженные файлы будут скачивать нечасто. Также важно предоставлять доступ к файлам другим клиникам (это пригодится, если пациента переводят или врачу надо посоветоваться с коллегами). Объектное хранилище — подходящее решение задачи.
  Decision:
  Выберите на стартовой странице консоли управления сервис Object Storage.
  Давайте создадим бакет для рентгеновских снимков.
  Нажмите кнопку Создать бакет. Откроется окно с основными параметрами: 
  Имя. Придумайте его с учетом правил. Обратите внимание, что дать бакету имя hospital не получится. Имена бакетов во всем Yandex Object Storage уникальны — назвать два бакета одинаково нельзя даже в разных облаках. Помните об этом, если будете создавать бакеты автоматически.
  Макс. размер. У вас есть два варианта: Выбрать опцию Без ограничения. Размер бакета будет увеличиваться, сколько бы объектов в него ни помещали. Указать максимальный размер. Это убережёт вас от финансовых потерь, если что-то пойдёт не так и в бакет загрузится слишком много объектов.
  Другие опции. Далее для всех типов операций оставьте ограниченный доступ (публичный позволяет выполнять операции всем пользователям интернета), выберите стандартный класс хранилища и нажмите кнопку Создать бакет.
  На странице объектного хранилища появился пустой бакет. Мы приготовили два рентгеновских снимка: image01.dat и image02.dat. Файлы можно загрузить в бакет с помощью: консоли управления; приложений; S3-совместимого HTTP API; HTML-форм на сайте.
  Разберём два способа: ручную загрузку через консоль управления и автоматическую с помощью утилиты S3cmd.
  Для загрузки файла через консоль управления выберите созданный бакет и в открывшемся окне нажмите кнопку Загрузить объекты.
  Выберем файл image01.dat. В появившейся форме нажмите кнопку Загрузить — и вы увидите, что файл оказался в хранилище.
  Загрузите второй файл с помощью утилиты S3cmd — консольного клиента для Linux и MacOS, предназначенного для работы с S3-совместимым HTTP API. Для работы в Windows используйте один из вариантов:  установите другой консольный клиент для объектных хранилищ, например AWS CLI;  установите подсистему Linux на Windows с помощью утилиты WSL (Windows Subsystem for Linux) и работайте с S3cmd в ней; создайте в облаке виртуальную машину с Ubuntu и работайте с S3cmd в ней. Для загрузки файла-примера в виртуальную машину воспользуйтесь командой:
  $ wget "https://disk.yandex.ru/i/2UlugGkurhcxWw" -O image02.dat 
  Установите S3cmd (в Ubuntu, например, это делается с помощью команды sudo apt-get install s3cmd). Теперь настройте S3cmd для работы с Yandex Object Storage:
  $ s3cmd --configure
  Инструкции о настройке клиента вы найдете в документации.
  После ввода параметров утилита попытается установить соединение с объектным хранилищем и в случае успеха покажет такое сообщение: Success. Your access key and secret key worked fine :-)
  Загрузим в бакет второй файл (image02.dat) и затем получим список хранящихся в бакете объектов:
  s3cmd put <путь к второму файлу>/image02.dat s3://<имя бакета>
  s3cmd ls s3://<имя бакета>
  Вернемся в консоль управления.
  Мы видим, что класс хранилища у обоих объектов — стандартное. Напомним: стандартное хранилище подходит для данных, к которым обращаются часто, а тариф за размещение данных в нем примерно в два раза выше, чем в холодном хранилище.
  Спустя несколько недель после того, как рентгеновский снимок сделан, к нему будут редко обращаться (если вообще будут), потому что пациент, скорее всего, выздоровеет.
  Чтобы оптимизировать затраты на хранение данных, настроим жизненный цикл объектов в бакете. Создадим правило, согласно которому через 30 дней после загрузки объектов в бакет класс их хранилища будет автоматически меняться со стандартного на холодное.
  Перейдите на вкладку Жизненный цикл и нажмите кнопку Настроить. Задайте произвольное описание. В поле Префикс укажите Все объекты. Выберите тип операции Transition. В качестве Условия срабатывания правила задайте Точную дату или Количество дней. В первом случае правило сработает в 00:00 установленной даты. Во втором — через указанное количество дней после загрузки объекта в бакет.
  Если понадобится настроить автоматическое удаление объектов, выберите тип операции Expiration. Нажмите кнопку Сохранить.
  Представим теперь, что объекты в хранилище — это оцифрованные рентгеновские снимки пациента Петрова. Первый из них (image01.dat) сделали несколько месяцев назад в ходе профосмотра, а второй (image02.dat) — вчера, после того как Петров обратился к врачу с жалобой на недомогание. В обоих случаях на снимках не увидели патологий.
  Опишите с помощью пользовательских метаданных эти снимки, и позже вы быстро найдете их среди множества объектов в бакете.
  С помощью утилиты S3cmd задайте для загруженных объектов метаданные с фамилией пациента (x-amz-meta-patient:petrov) и с результатами обследования (x-amz-meta-status:ok):
  s3cmd modify --add-header=x-amz-meta-patient:petrov --add-header=x-amz-meta-status:ok s3://hospital/image01.dat s3://hospital/image02.dat
  Выведите на экран информацию об этих объектах, чтобы проверить, что получилось:
  s3cmd info s3://hospital/image01.dat s3://hospital/image02.dat
  В результате вы должны увидеть информацию об объектах в бакете.
  s3://hospital/image01.dat (object):
    File size: 33
    Last mod:  Thu, 04 Mar 2021 22:05:31 GMT
    MIME type: application/x-www-form-urlencoded
    Storage:   STANDARD
    MD5 sum:   6f6d5a1cb79839e523582ed8810a42fd
    SSE:       none
    Policy:    none
    CORS:      none
    x-amz-meta-patient: petrov
    x-amz-meta-status: ok
  s3://hospital/image02.dat (object):
    File size: 16
    Last mod:  Thu, 04 Mar 2021 23:11:27 GMT
    MIME type: text/plain
    Storage:   STANDARD
    MD5 sum:   0366a1d19e584ce79d5c05ddedc69310
    SSE:       none
    Policy:    none
    CORS:      none
    x-amz-meta-patient: petrov
    x-amz-meta-status: ok 
  Предположим, Петров чувствует себя хуже. Судя по анализам, он действительно болен. Лечащий врач решает проконсультироваться с более опытной коллегой Ивановой из профильной клиники. Объекты в бакете недоступны для внешних пользователей, поскольку при его создании мы ограничили доступ. Чтобы Иванова увидела рентгеновский снимок Петрова, отправим ей временную ссылку на объект image02.dat.
  Для этого в консоли управления кликните на объект и в открывшемся окне информации об объекте нажмите кнопку Получить ссылку. Укажите время жизни ссылки в часах или днях.
  Можно поделиться ссылкой или использовать ее в любом сервисе для доступа к файлу.
  После консультации Иванова поставила Петрову правильный диагноз: вирусная пневмония (код J12 по Международной классификации болезней). Вам осталось исправить метаданные объекта image02.dat. Замените значение метаданных с результатами обследования с ok на J12 самостоятельно.
  Decision:
  $ wget "https://disk.yandex.ru/i/2UlugGkurhcxWw" -O image02.dat 
  $ wget "https://disk.yandex.ru/i/qpl0T4u-nWPgiw" -O image01.dat 
  $ ls *.dat
  image01.dat  image02.dat
  $ sudo apt-get install s3cmd
  $ s3cmd --configure
  $ s3cmd ls
  2023-12-06 02:37  s3://klinika138
  $ s3cmd put /YOUR-DIR/image02.dat s3://klinika138
  $ s3cmd modify \
  --add-header=x-amz-meta-patient:petrov \
  --add-header=x-amz-meta-status:ok \
  s3://klinika138/image01.dat \
  s3://klinika138/image02.dat
  $ s3cmd info \
  s3://klinika138/image01.dat \
  s3://klinika138/image02.dat 
  Task:
  Хранение статических веб-сайтов в Object Storage
  Представьте, что вам нужно выбрать оптимальный хостинг для сайта клиники. Главные критерии: отказоустойчивый, недорогой и простой в обслуживании. 
  Один из вариантов решения такой задачи — использовать объектное хранилище. Вы можете, не настраивая никаких серверов, просто загрузить HTML-файлы, скрипты, стили и другие файлы в хранилище. Пользователи будут открывать в браузере ваш сайт, а по сути — скачивать файлы прямо из бакета.
  Важно понимать, что этот вариант подойдет только для полностью статических сайтов. Иными словами, сайт должен быть сделан с помощью клиентских технологий (HTML, CSS и JavaScript) и не требовать запуска чего-либо на стороне веб-сервера.
  Предположим, что сайт нашей клиники как раз такой — полностью статический. Опубликуйте его с помощью объектного хранилища. Прежде всего для него нужно создать бакет:
  Decision
  Обратите внимание на несколько особенностей: Если вы планируете использовать собственный домен (например www.example.com), то присвойте бакету точно такое же имя. Откройте публичный доступ на чтение объектов. Это позволит пользователям интернета скачивать объекты из бакета и просматривать сайт в браузере.
  Задайте необходимые настройки и нажмите кнопку Создать бакет.
  Теперь загрузите в бакет файлы сайта (например, этот и этот) любым удобным способом.
  Чтобы настроить хостинг, перейдите на страницу бакета в консоли управления. Выберите вкладку Веб-сайт на левой панели и включите опцию Хостинг.
  Укажите файл с главной страницей сайта (как правило, это index.html), а поле со страницей ошибки можно не заполнять.
  Сохраните настройки, и сайт станет доступен по адресам: 
  http(s)://<имя_бакета>.website.yandexcloud.net
  http(s)://website.yandexcloud.net/<имя_бакета>
  По умолчанию сайт будет доступен только по протоколу HTTP. Для поддержки HTTPS нужно загрузить в объектное хранилище TLS-сертификат. Вам предстоит это сделать в одной из практических работ курса «Безопасность».
  Если у вас есть собственный домен и вы хотите опубликовать сайт на нём, то настройте CNAME-запись у DNS-провайдера или на своем DNS-сервере. Например, для домена www.example.com CNAME-запись выглядела бы так: www.example.com CNAME www.example.com.website.yandexcloud.net 
  В этом случае можно использовать домены не ниже третьего уровня (то есть использовать домен example.com не получится, только www.example.com). Это связано с особенностями обработки CNAME-записей на DNS-хостингах.
  Decision:
  $ wget "https://disk.yandex.ru/d/KcpMuYBwKjIa6Q" -O index.html
  $ wget "https://disk.yandex.ru/i/uTai62_esPSaEw" -O doctor.png
  $ s3cmd put /YOUR-DIR/doctor.png s3://www.aibloit.healthcare138
  $ s3cmd put /YOUR-DIR/index.html s3://www.aibloit.healthcare138
  Task:
  Создание кластера базы данных MySQL. Object Storage — удобный и полезный инструмент для хранения данных в облаке. Но для решения практических задач важно не просто хранить данные, но и иметь возможность их изменять и выполнять с ними различные операции (сортировать, группировать, делать выборки и так далее). Для этого используются базы данных. В этой и следующих темах вы научитесь работать с несколькими управляемыми БД. И начнем мы с одной из самых популярных — MySQL.
  На этом уроке вы создадите и настроите кластер управляемой БД MySQL, подключитесь к нему, перенесёте данные в облако, познакомитесь с возможностями резервного копирования и мониторинга. Эти навыки пригодятся вам и в других сервисах управляемых БД, поскольку принципы работы в них очень похожи.
  Предположим, вы решили добавить в разрабатываемый вами мессенджер новую функциональность. Вы написали микросервис, который позволяет оценивать сообщения в групповых чатах и хранит оценки в БД MySQL. Давайте поместим эту БД в Yandex Cloud.
  Decision:
  Прежде всего понадобится создать кластер: набор виртуальных машин (ВМ, или хостов), на которых будет развёрнута БД. Это обязательный первый шаг при использовании любого сервиса управляемых БД.
  Войдите в консоль управления Yandex Cloud и выберите каталог для кластера. Вверху справа нажмите кнопку Создать ресурс и выберите из выпадающего списка Кластер MySQL.
  Откроется страница с основными настройками кластера. Рассмотрим их подробнее.
  Базовые параметры. Имя кластера может включать только цифры, прописные и строчные латинские буквы, дефисы.
  Поле Описание заполнять необязательно. Оно полезно, если вам нужно создать несколько кластеров для разных целей, чтобы в них было проще ориентироваться.
  О том, какое бывает Окружение кластера и чем различаются PRESTABLE и PRODUCTION, мы говорили на одном из предыдущих уроков. Поскольку микросервис только разрабатывается, выберите окружение PRESTABLE.
  Версия. В качестве сервера MySQL в Yandex Cloud используется Percona Server версии 5.7 или 8.0. У этих реализаций сервера улучшенная производительность на многоядерных машинах. Если для вас критична стабильность работы микросервиса, выбирайте проверенную временем 5.7. Для нашей задачи подойдёт 8.0: в ней много новых функций, но она ещё не полностью обкатана.
  Класс хостов. Следующий шаг — выбор класса хостов, или шаблона ВМ. Хосты кластера будут развёрнуты на базе ВМ Compute Cloud с использованием этого шаблона.
  Платформа определяет тип физического процессора (Intel Broadwell или Intel Cascade Lake), а также конфигурации числа ядер виртуального процессора (vCPU) и размера оперативной памяти.
  Если тип процессора для вас неважен, выбирайте более современную платформу Intel Cascade Lake. Она предоставляет широкий выбор конфигураций вычислительных ресурсов.
  Также на конфигурации влияет тип ВМ, на которой будет развёрнута БД.
  Standard — это обычные ВМ с 4 ГБ RAM на ядро vCPU. Это оптимальный баланс между количеством запущенных процессов, быстродействием и потребляемой оперативной памятью.
  Memory-optimized — машины с вдвое увеличенным объёмом RAM на каждое ядро. Выбирайте их для высоконагруженных сервисов с повышенными требованиями к кешу.
  Burstable — машины, для которых гарантируется использование лишь доли ядра vCPU (5, 20 или 50%) с вероятностью временного повышения вплоть до 100%. Они стоят дешевле и подходят для задач, где не нужен постоянный уровень производительности, т. е. для тестирования или разработки.
  Выберем для микросервиса следующий класс хоста: платформа — Intel Cascade Lake; тип — standard; конфигурация вычислительных ресурсов — s2.micro (два ядра vCPU, 8 ГБ RAM).
  Хранилище данных. Хранилище БД может быть сетевым или локальным. В первом случае данные находятся на виртуальных дисках в инфраструктуре Yandex Cloud. Локальное хранилище — это диски, которые физически размещаются в серверах хостов БД.
  При создании кластера можно выбирать между следующими типами хранилища:
  - Стандартное сетевое (network-hdd) — это наиболее экономичный вариант. Выбирайте его, если к скорости записи и чтения нет особых требований.
  - Быстрое сетевое (network-ssd) стоит примерно в четыре раза дороже, но при размере хранилища от 100 ГБ работает быстрее стандартного в десять и более раз (чем больше размер, тем заметнее разница в скорости).
  - Сетевое на нереплицируемых SSD-дисках (network-ssd-nonreplicated) — использует сетевые SSD-диски с повышенной производительностью, реализованной за счет устранения избыточности. Объём такого хранилища можно увеличивать только с шагом 93 ГБ.
  - Быстрое локальное (local-ssd) — самое быстрое и дорогое. Если локальный диск откажет, все сохранённые на нём данные будут потеряны. Чтобы этого избежать, при выборе локального хранилища сервис автоматически создаст отказоустойчивый кластер минимум из трёх хостов.
  При создании кластера внимательно выбирайте тип хранилища. Размер хранилища можно будет позже изменить, а тип — нет.
  Выберите для кластера стандартное сетевое хранилище network-hdd размером 50 ГБ.
  База данных. В этом разделе настроек задаются атрибуты базы: Имя БД, уникальное в рамках кластера, Имя пользователя (владельца БД) и Пароль пользователя.
  Сеть. Здесь можно выбрать облачную сеть для кластера и группы безопасности для его сетевого трафика.
  Оставьте сеть по умолчанию (default) или выберите сеть, которую создали на предыдущем курсе. Кластер будет доступен для всех ВМ, которые подключены к вашей облачной сети.
  Параметры хостов. В этом блоке можно добавить количество хостов, которые будут созданы вместе с кластером, и изменить их параметры. Дополнительные хосты могут понадобиться, например, для репликации БД или снижения нагрузки на хост-мастер.
  Для наших целей достаточно кластера из одного хоста. Нажмите значок редактирования параметров хоста и в открывшемся окне выберите опцию Публичный доступ. Это означает, что к хосту можно будет подключиться из интернета, а не только из облачной сети. Остальные параметры оставьте без изменений.
  Дополнительные настройки.Здесь можно: указать время Начала резервного копирования и Окна обслуживания. Это пригодится, если вы хотите, чтобы резервное копирование и техобслуживание хостов кластера не совпадали с периодами пиковых нагрузок на БД; разрешить Доступ из DataLens, если вы планируете анализировать в DataLens данные из базы. Подробнее о DataLens вы узнаете на одном из следующих занятий; разрешить Доступ из консоли управления, чтобы выполнять SQL-запросы к БД из консоли управления Yandex Cloud. Отметьте этот пункт: доступ из консоли понадобится нам на следующих практических работах; разрешить Доступ из Data Transfer, чтобы разрешить доступ к кластеру из сервиса Yandex Data Transfer в Serverless-режиме; разрешить Сбор статистики, чтобы воспользоваться инструментом Диагностика производительности в кластере; установить Защиту от удаления, чтобы защитить кластер от непреднамеренного удаления пользователем.
  В этом блоке также можно задать настройки БД (например используемую сервером MySQL кодировку при работе с данными и обмене информацией с клиентами). По умолчанию при создании кластера сервис выбирает оптимальные настройки. Изменяйте их, если уверены, что это необходимо.
  Настройка завершена. Осталось только нажать кнопку Создать кластер.
  Создание кластера займёт несколько минут. Когда он будет готов к работе, его статус на панели Managed Service for MySQL сменится с Creating на Running, а состояние — на Alive.
  Статус показывает, что происходит с кластером: Creating — создаётся; Running — работает; Error — не отвечает, возникла проблема; Updating — обновляется; Stopped — остановлен; Unknown — статус неизвестен (так может быть, например, когда кластер не виден из интернета).
  Состояние — это показатель доступности кластера: Alive — все хосты кластера работают; Degraded — часть хостов (один или больше) не работает; Dead — все хосты не работают.
  Task:
  Подключение к БД и добавление данных
  Доступ из консоли управления. В кластере, который вы создали, уже есть БД. Она пока пустая. Поскольку при создании кластера вы выбрали в настройках пункт Доступ из консоли управления, в консоли управления Yandex Cloud появилась вкладка с интерфейсом для выполнения SQL-запросов к БД.
  Давайте зайдём туда и создадим в БД таблицу для нашего микросервиса.
  Decision:
  На странице Managed Service for MySQL выберите строку с созданным вами кластером. В панели консоли управления перейдите на вкладку SQL. Вам будет предложено выбрать БД для SQL-запросов и имя пользователя, а также ввести пароль. Все эти атрибуты вы задавали при создании  кластера.
  Нажмите кнопку Подключиться. Откроется структура БД (сейчас там написано, что данных нет) и окно ввода для SQL-запросов.
  Теперь создадим таблицу. Введите в окне ввода следующий запрос и нажмите кнопку Выполнить.
  CREATE TABLE IF NOT EXISTS ratings (
      rating_id INT AUTO_INCREMENT PRIMARY KEY,
      user_id INT NOT NULL,
      message_id INT NOT NULL,
      rating INT NOT NULL
  ) ENGINE=INNODB; 
  Обратите внимание, что в качестве движка в сервисе управляемых БД MySQL используется только InnoDB.
  В таблицу можно добавить данные с помощью команды INSERT.
  INSERT INTO ratings (user_id,message_id,rating) VALUES (44,368,4); 
  Чтобы отобразить обновлённую структуру БД, нажмите на имя БД и выберите таблицу ratings.
  Наведите указатель на заголовок столбца, чтобы увидеть тип данных в нём.
  SQL-запросы через консоль управления Yandex Cloud — нетипичный способ работы с БД. Используйте его для небольших, разовых задач, когда быстрее и проще открыть подключение в браузере. Этот способ не очень удобен: текст запроса и результат его выполнения доступны, только пока вы не закрыли или не перезагрузили страницу в браузере. Конечно, если запрос успешно запущен, то сервис обработает его независимо от состояния консоли управления.
  В консоли выводятся только первые 1000 строк результата запроса, даже если данных больше. Чтобы увидеть строку, введите её номер в поле Номер первой строки.
  Подключение к кластеру
  В основном вы будете работать с БД из приложений или из командной строки. Однако для этого нужно подключиться к хосту, на котором развёрнута БД.
  Есть два варианта подключения. Если публичный доступ к хосту открыт, подключитесь к нему через интернет с помощью защищённого SSL-соединения. Если публичного доступа нет, подключитесь к хосту с виртуальной машины, созданной в той же виртуальной сети. SSL-соединение можно не использовать, но тогда трафик между виртуальной машиной и БД шифроваться не будет.
  Давайте подключимся к БД через интернет и создадим в ней ещё одну таблицу. Для выполнения этого задания вы можете использовать виртуальную машину с Ubuntu.
  Для создания таблицы сделаем в текстовом редакторе файл createTables.sql с командами. Например, такой:
  CREATE TABLE IF NOT EXISTS users (
      user_id INT AUTO_INCREMENT,
      nickname VARCHAR(128) NOT NULL,
      avatar VARCHAR(255),
      mail VARCHAR(255),
          PRIMARY KEY (user_id)
  ) ENGINE=INNODB; 
  Чтобы выполнить этот запрос в БД, подключимся к хосту. Для этого понадобится SSL-сертификат. Команды для его получения в Ubuntu:
  mkdir ~/.mysql
  wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" -O ~/.mysql/root.crt
  chmod 0600 ~/.mysql/root.crt 
  Чтобы получить команды для подключения к БД, в консоли управления перейдите на страницу кластера, на вкладке Обзор нажмите кнопку Подключиться. В результате их выполнения в директории /home/<домашняя_директория>/.mysql/ сохранится SSL-сертификат root.crt.
  Установите утилиту mysql-client, если на вашем компьютере или виртуальной машине её нет.
  sudo apt update
  sudo apt install -y mysql-client 
  Чтобы подключиться к БД, введите команду mysql. Для запуска нашего скрипта она выглядит следующим образом:
  mysql --host=<адрес хоста> \
          --port=3306 \
          --ssl-ca=~/.mysql/root.crt \
          --ssl-mode=VERIFY_IDENTITY \
          --user=<имя пользователя> \
          --password \
      <имя_базы_данных> < createTables.sql 
  Сервис помогает заполнить параметры в команде. Чтобы посмотреть пример команды с адресом хоста, именами пользователя и БД, в консоли управления перейдите на страницу кластера, на вкладке Обзор нажмите кнопку Подключиться.
  После запуска команды введите пароль к БД, после чего в ней будет создана таблица users.
  Если при создании кластера вы не включили публичный доступ, то к БД можно подключиться с виртуальной машины из той же облачной сети без использования шифрования. \\Следовательно, в этом случае в команде для подключения опускается параметр --ssl-ca, а --ssl-mode передаётся со значением DISABLED:
  mysql --host=адрес_хоста \
        --port=3306 \
        --ssl-mode=DISABLED \
        --user=<имя пользователя> \
        --password \
        <имя_базы_данных> < createTables.sql 
  Естественно, подключаться к БД можно не только из командной оболочки, но и из приложений. Нажмите уже знакомую вам кнопку Подключиться и посмотрите примеры кода для Python, PHP, Java, Node.js, Go, Ruby или настроек для драйвера ODBC.
  Если вы хотите перенести БД в облако, то понадобится создать дамп и восстановить его в нужном кластере. Дамп — это копия БД или её части, представляющая собой текстовый файл с командами SQL (например, CREATE TABLE или INSERT). Его создают с помощью утилиты mysqldump.
  Давайте попробуем перенести данные в кластер с помощью дампа. Для этого воспользуемся тестовой БД с данными о сотрудниках компании (имя, дата рождения, дата найма, место работы, зарплата и т. д.). Размер БД — около 167 Мб.
  Скачайте из репозитория и сохраните на компьютере файлы с расширениями .sql и .dump. В файле employees.sql содержатся SQL команды, необходимые для создания таблиц и добавления в них данных из dump-файлов. Для переноса тестовой БД в облако понадобится запустить этот файл. Но, прежде чем приступить к переносу БД, откройте этот файл и удалите или закомментируйте (допишите в начало строки --) в нём строку 110. В этой строке расположена команда FLUSH LOGS, которая закрывает и снова открывает файлы журналов, а они в этой тестовой БД отсутствуют.
  Создайте базу данных employees через консоль управления. Для этого на странице кластера перейдите на вкладку Базы данных и нажмите кнопку Добавить.
  Добавьте пользователю, например user1, разрешение на доступ к БД employees. Для этого на странице кластера перейдите на вкладку Пользователи, напротив пользователя user1 нажмите кнопку ··· и выберите Настроить. Во всплывающем окне нажмите Добавить базу данных, выберите employees, добавьте роль ALL_PRIVILEGES и нажмите Сохранить.
  Затем в командной строке перейдите в папку сохраненными файлами .sql и .dump и восстановите данные из дампа с помощью команды:
  mysql --host=<адрес хоста> \
          --port=3306 \
          --ssl-ca=~/.mysql/root.crt \
          --ssl-mode=VERIFY_IDENTITY \
          --user=<имя_пользователя> \
          --password \
      employees < ~/employees.sql 
  После того как данные скопируются, ваш кластер и БД будут готовы к работе. Подключитесь к БД в консоли управления и убедитесь, что данные перенесены.
  Decision:
  $ mkdir ~/.mysql
  $ wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" -O ~/.mysql/root.crt
  $ chmod 0600 ~/.mysql/root.crt 
  $ sudo apt update
  $ sudo apt install mysql-client 
  $ vim createTables.sql
  $ cat createTables.sql
  CREATE TABLE IF NOT EXISTS users (
      user_id INT AUTO_INCREMENT,
      nickname VARCHAR(128) NOT NULL,
      avatar VARCHAR(255),
      mail VARCHAR(255),
      PRIMARY KEY (user_id)
  ) ENGINE=INNODB;
  $ mysql --host=rc1a-642tdtv6ope6gk7u.mdb.yandexcloud.net \
        --port=3306 \
        --ssl-ca=~/.mysql/root.crt \
        --ssl-mode=VERIFY_IDENTITY \
        --user=tuser \
        --password \
        tbase < createTables.sql 
  $ wget https://github.com/datacharmer/test_db/archive/refs/heads/master.zip
  $ unzip master.zip
  $ ls test_db-master/
  Changelog                      employees.sql          load_dept_emp.dump      load_salaries1.dump  load_titles.dump  sakila            test_employees_md5.sql
  employees_partitioned_5.1.sql  images                 load_dept_manager.dump  load_salaries2.dump  objects.sql       show_elapsed.sql  test_employees_sha.sql
  employees_partitioned.sql      load_departments.dump  load_employees.dump     load_salaries3.dump  README.md         sql_test.sh       test_versions.sh
  $ cat test_db-master/employees.sql | grep flush
  flush /*!50503 binary */ logs;
  $ vim test_db-master/employees.sql
  $ cat test_db-master/employees.sql | grep flush
  $ cd test_db-master/
  $ mysql --host=rc1a-642tdtv6ope6gk7u.mdb.yandexcloud.net \
        --port=3306 \
        --ssl-ca=~/.mysql/root.crt \
        --ssl-mode=VERIFY_IDENTITY \
        --user=tuser1 \
        --password \
        employees < employees.sql 
  $ mysql --host=rc1a-642tdtv6ope6gk7u.mdb.yandexcloud.net \
        --port=3306 \
        --ssl-ca=~/.mysql/root.crt \
        --ssl-mode=VERIFY_IDENTITY \
        --user=tuser1 \
        --password \
        employees
  mysql> show tables;
  Task:
  Создание кластера базы данных PostgreSQL. 
  В этой практической работе вы создадите кластер еще одной управляемой БД, на этот раз PostgreSQL, подключитесь к ней и загрузите в нее данные. 
  Decision:
  Создание кластера управляемой базы данных PostgreSQL аналогично созданию кластера базы данных MySQL.
  Перейдите в сервис управляемых баз данных PostgreSQL и нажмите кнопку Создать кластер.
  В появившемся окне настроек задайте необходимые параметры.
  - Имя кластера и его описание. Выберите уникальное в облаке имя кластера. Описание опционально, поэтому можно оставить это поле пустым.
  - В поле Окружение выберите PRODUCTION.
  - Выберите версию PostgreSQL и класс хоста.
  - Выберите размер и тип сетевого хранилища.
  - Задайте атрибуты базы данных.
  - Выберите из списка сеть, в которой будут находиться хосты кластера (для подключения потребуются публичные хосты).
  - В блоке Хосты добавьте ещё два хоста в других зонах доступности для обеспечения отказоустойчивости кластера. База автоматически реплицируется.
  - В блоке Дополнительные настройки задайте время начала резервного копирования и включите доступ из консоли управления.
  - Нажмите кнопку Создать кластер.
  Как и в случае с MySQL, к хостам кластера Managed Service for PostgreSQL можно подключиться двумя способами.
  Через интернет. Если вы настроили публичный доступ для нужного хоста, то подключиться к нему можно с помощью SSL-соединения.
  С виртуальных машин Yandex Cloud. Они должны быть расположены в той же облачной сети. Если к хосту нет публичного доступа, для подключения с таких виртуальных машин SSL-соединение использовать необязательно. Обратите внимание, что если публичный доступ в вашем кластере настроен только для некоторых хостов, автоматическая смена мастера может привести к тому, что вы не сможете подключиться к мастеру из интернета.
  Установите клиент для подключения к БД PostgreSQL. Команда установки в Ubuntu: sudo apt update && sudo apt install -y postgresql-client 
  Скачайте сертификат для подключения к БД PostgreSQL:
  mkdir -p ~/.postgresql
  wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" -O ~/.postgresql/root.crt 
  chmod 0600 ~/.postgresql/root.crt 
  Пример команды для подключения можно посмотреть в консоли управления, нажав на кнопку Подключиться на странице кластера. Подключение с SSL происходит при помощи следующей команды:
  psql "host=<FQDN_хоста> \ 
        port=6432 \
        sslmode=verify-full \
        dbname=<имя базы данных> \
        user=<имя пользователя базы данных> \
        target_session_attrs=read-write"
  Загрузка данных в базу данных из CSV. Одним из способов добавления данных в базу является их загрузка из csv-файла.
  Предположим, вы используете БД для организации работы транспортной службы интернет-магазина. Вам нужно добавить в базу таблицу, содержащую данные о расстояниях между складом и пунктами самовывоза, а также о стандартном времени доставки товаров со склада в эти пункты. Создадим csv-файл, например DTM.csv, который содержит такие данные (100 - код склада, 101-109 - коды пунктов, Time - стандартное время доставки в минутах, Distance - расстояние в километрах):
  "depot","store","time","distance"
  "100","101",31,12
  "100","102",38,17
  "100","103",56,33
  "100","104",70,60
  "100","105",41,25
  "100","106",21,8
  "100","107",33,14
  "100","108",62,42
  "100","109",45,29 
  Важные моменты при миграции из CSV:
  - Названия колонок в файле и в таблице необязательно совпадают.
  - Файл содержит заголовок, который не нужно импортировать.
  - Первые 2 колонки конвертируем из строк (string) в целые числа (int).
  PostgreSQL позволяет импортировать данные из файла несколькими способами:
  - Командой copy.
  - Через функции pl/pgsql.
  - Средствами другого языка, например Python.
  Воспользуемся первым способом. Сначала нам понадобится создать таблицу, в которую будет осуществлена миграция данных. Подключитесь к БД согласно инструкциям выше. Выполните следующую команду:
  CREATE TABLE dtm (
      id serial PRIMARY KEY,
      depot int NOT NULL,
      store int  NOT NULL,
      time int NOT NULL,
        distance int  NOT NULL
  );
  Загрузите данные: \copy dtm(depot,store,time,distance) from '/<путь к файлу>/DTM.csv' DELIMITERS ',' CSV HEADER;
  В этой команде мы учли те моменты, о которых говорили вначале:
  - dtm (depot, store, time, distance) маппинг колонок связывает колонки в файле с колонками в таблице, их имена могут не совпадать
  - CSV HEADER показывает, что заголовок импортировать не нужно
  - Колонки в таблице уже имеют правильные типы данных, конвертация будет выполнена автоматически.
  В консоли управления на странице кластера перейдите на вкладку SQL. Введите пароль пользователя БД и нажмите кнопку Подключиться. Выберите таблицу dtm, чтобы убедиться, что добавление данных выполнено правильно.
  Decision:
  $ sudo apt update && sudo apt install postgresql-client
  $ mkdir -p ~/.postgresql
  $ wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" -O ~/.postgresql/root.crt
  $ chmod 0600 ~/.postgresql/root.crt
  $ vim DTM.csv
  $ cat DTM.csv
  "deport","store","time","distance"
  "100","101",31,12
  "100","102",38,17
  "100","103",56,33
  "100","104",70,60
  "100","105",41,25
  "100","106",21,8
  "100","107",33,14
  "100","108",62,42
  "100","109",45,29
  $ psql "host=rc1a-w3usdays081v0itf.mdb.yandexcloud.net,rc1c-qga7rd1sqe5jm8io.mdb.yandexcloud.net,rc1d-l1vzj1210qj68s8n.mdb.yandexcloud.net \
      port=6432 \
      sslmode=verify-full \
      dbname=YOUR-DB \
      user=YOUR-USERNAME \
      target_session_attrs=read-write"
  YOUR-DB=> CREATE TABLE dtm (
  YOUR-DB(>     id serial PRIMARY KEY,
  YOUR-DB(>     depot int NOT NULL,
  YOUR-DB(>     store int  NOT NULL,
  YOUR-DB(>     time int NOT NULL,
  YOUR-DB(>     distance int  NOT NULL
  YOUR-DB(> );
  YOUR-DB=> \copy dtm(depot,store,time,distance) from '/home/test/DTM.csv' DELIMITERS ',' CSV HEADER;
  YOUR-DB=> exit
  Task:
  Создание кластера MongoDB. 
  На этом уроке вы создадите кластер MongoDB, подключитесь к нему и загрузите в него данные. 
  Раньше вы работали только с реляционными БД, но использование кластера MongoDB принципиально не отличается от работы с кластером MySQL или PostgreSQL, так что многое будет вам знакомо.
  Decision:
  Выберите в консоли управления Yandex Cloud каталог для кластера БД. На дашборде каталога откройте раздел Managed Service for MongoDB. В открывшемся окне нажмите кнопку Создать кластер.
  Установите основные настройки кластера. Для этого урока создайте кластер с минимальной конфигурацией: тип хоста burstable, класс b2.nano, стандартное сетевое хранилище размером 10 ГБ. Откройте публичный доступ к хосту и задайте пароль пользователя БД. Остальные значения оставьте по умолчанию.
  В сервисе управляемых БД MongoDB к хостам можно подключаться через интернет или с виртуальных машин в той же сети. Порт для подключения — 27018.
  Для подключения через интернет хосты кластера должны находиться в публичном доступе. Подключаться можно только через зашифрованное соединение.
  Обратите внимание: если публичный доступ настроен только для некоторых хостов в кластере, то при автоматической смене основной реплики она может оказаться недоступной из интернета.
  Если к хосту нет публичного доступа и вы подключаетесь к нему с виртуальных машин Yandex Cloud, то зашифрованное соединение необязательно.
  Подключитесь к созданной БД из интернета. Используйте SSL-сертификат, который вы подготовили на одной из предыдущих практических работ, или команду (для Ubuntu):
  sudo mkdir -p /usr/local/share/ca-certificates/Yandex && \
  sudo wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" -O /usr/local/share/ca-certificates/Yandex/YandexInternalRootCA.crt 
  Если всё пройдет успешно — вы получите сообщение операционной системы о том, что сертификат сохранён.
  Установите утилиту Mongo Shell:
  sudo apt install mongodb-clients 
  Подключитесь к БД с помощью команды mongo. Чтобы получить строку подключения, на основной странице сервиса в консоли управления выберите кластер, на вкладке Обзор нажмите кнопку Подключиться.
  Сервис сформирует пример строки подключения для кластера. Там же вы можете посмотреть примеры кода на Python, PHP, Java, Node.js, Go для подключения из приложений.
  Подключитесь к кластеру из командной строки.
  mongo --norc \
          --ssl \
          --sslCAFile /usr/local/share/ca-certificates/Yandex/YandexInternalRootCA.crt \
          --host '<FQDN хоста MongoDB>:27018' \
          -u <имя пользователя БД> \
          -p <пароль пользователя БД> \
          <имя БД> 
  Создадим в БД коллекцию users. Предположим, в ней содержится информация о пользователях вашего приложения.
  db.createCollection("users") 
  Загрузим в коллекцию тестовые данные с помощью методов добавления одного документа db.insertOne(...) и сразу нескольких db.insertMany(...).
  Сначала добавим один документ (данные одного пользователя).
  db.users.insertOne({firstName: "Adam", lastName: "Smith", age: 37, email: "adam.smith@test.com"}); 
  Дополним коллекцию данными еще двух пользователей.
  db.users.insertMany( [
        {firstName: "Viktoria", lastName: "Holmes", age: 73, email: "viktoria.holmes@test.com", phone: "737772727"},
        {firstName: "Tina", lastName: "Anders", age: 29, email: "tina.anders@test.com", children: [{firstName: "Sam", lastName: "Anders"},{firstName: "Anna", lastName: "Anders"}]}
    ] ); 
  Обратите внимание, что документы в коллекции users содержат разный набор данных. С помощью MongoDB мы можем работать с данными, структура которых частично не совпадает.
  Теперь посмотрим на содержимое коллекции с помощью команды db.users.find(). Результат показывает, что все данные успешно добавлены:
  Проверим, есть ли среди пользователей те, кому больше 37 лет. Сделаем запрос к БД с помощью метода find.
  db.users.find({age: {$gt: 37}}); 
  Decision:
  $ sudo mkdir -p /usr/local/share/ca-certificates/Yandex && \
  sudo wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" -O /usr/local/share/ca-certificates/Yandex/YandexInternalRootCA.crt
  $ wget https://downloads.mongodb.com/linux/mongodb-linux-x86_64-enterprise-ubuntu2004-6.0.2.tgz
  $ tar -zxvf mongodb-linux-x86_64-enterprise-ubuntu2004-6.0.2.tgz
  $ sudo ln -s /path/to/the/mongodb-directory/bin/* /usr/local/bin/
  $ sudo apt install mongodb-clients
  $ mongo --norc \
          --ssl \
          --sslCAFile /usr/local/share/ca-certificates/Yandex/YandexInternalRootCA.crt \
          --host 'rs01/rc1b-b7xwau9lvu3hdt0w.mdb.yandexcloud.net:27018' \
          -u YOUR-USERNAME \
          -p YOUR-PASSWORD \
          YOUR-DB
  rs01:PRIMARY> db.createCollection("users")
  rs01:PRIMARY> db.users.insertOne({firstName: "Adam", lastName: "Smith", age: 37, email: "adam.smith@test.com"});
  rs01:PRIMARY> db.users.insertMany( [
        {firstName: "Viktoria", lastName: "Holmes", age: 73, email: "viktoria.holmes@test.com", phone: "737772727"},
        {firstName: "Tina", lastName: "Anders", age: 29, email: "tina.anders@test.com", children: [{firstName: "Sam", lastName: "Anders"},{firstName: "Anna", lastName: "Anders"}]}
    ] );
  rs01:PRIMARY> db.users.find({age: {$gt: 37}});
  Task:
  Создание кластера ClickHouse и подключение к нему.
  В этой практической работе вы создадите кластер ClickHouse. Вы уже знаете, как создавать кластеры и выставлять их основные настройки в сервисах платформы данных. Но у БД ClickHouse есть свои особенности.
  Когда вы создадите кластер из двух или более хостов, сервис дополнительно создаст ещё один кластер из трёх хостов, где развернёт Apache ZooKeeper. Это служба для распределенных систем, которая управляет конфигурацией, репликацией и распределением запросов по хостам БД. Без неё кластер ClickHouse работать не будет. К ZooKeeper у пользователей доступа нет, однако его хосты учитываются при расчёте квоты ресурсов облака и стоимости сервиса.
  ZooKeeper синхронизирует шарды (т. е. хосты) ClickHouse. В отличие от классических реляционных БД, у ClickHouse нет главного узла (мастера), через который добавляются данные. В ClickHouse данные можно и записывать, и читать с любого узла.
  Decision:
  Перейдите в каталог, где нужно создать кластер БД, выберите Managed Service for ClickHouse и нажмите кнопку Создать кластер.
  Для практической работы нам понадобится кластер с минимальной конфигурацией: тип хоста burstable, класс b2.nano и стандартное сетевое хранилище размером 10 ГБ.
  Задайте настройки: введите имена для кластера и БД, а также имя и пароль пользователя. Откройте публичный доступ к хосту.
  Обратите внимание: в отличие от сервисов, которые мы уже рассматривали, здесь в разделе База данных можно включить опции управления пользователями и БД с помощью SQL-запросов.
  Кроме того, в дополнительных настройках можно включить доступ к БД из консоли управления, сервисов DataLens, Яндекс Метрики и AppMetrica, а также возможность использовать бессерверные вычисления (подробно о них мы расскажем на курсе «Serverless»). С помощью DataLens, например, вы визуализируете результаты поисковых запросов в виде графиков, диаграмм и дашбордов, а подключение AppMetrica позволит импортировать данные из этого сервиса в кластер.
  Отметьте пункт Доступ из DataLens: он понадобится вам на одном из следующих уроков. Нажмите кнопку Создать кластер.
  К хостам кластера ClickHouse можно подключаться через интернет или с виртуальных машин в той же виртуальной сети. Если к хостам БД открыт публичный доступ, то для подключения к ним используется шифрованное соединение.
  Подключайтесь к кластеру с помощью HTTP-протокола или более низкоуровневого Native TCP-протокола. В большинстве случаев рекомендуется взаимодействовать с ClickHouse не напрямую, а с помощью инструмента или библиотеки. Официально поддерживаются консольный клиент, драйверы JDBC и ODBC, клиентская библиотека для C++. Также можно использовать библиотеки сторонних разработчиков для Python, PHP, Go, Ruby и т. д.
  Примеры строк подключения приводятся в документации и консоли управления на вкладке Обзор страницы кластера.
  С БД удобно работать в приложении с графическим интерфейсом. Один из вариантов — универсальный клиент DBeaver. Другие варианты вы найдёте в полном списке клиентов.
  Подробная информация о настройке подключения приведена в документации. Чтобы создать подключение к ClickHouse в DBeaver, помимо обычных параметров (адреса хоста, порта, имени БД, логина и пароля) задайте на вкладке Свойства драйвера настройки свойств драйвера JDBC. Укажите следующие параметры: ssl = true; sslmode = strict; sslrootcert = <путь к SSL-сертификату>. Как получить SSL-сертификат, вы уже узнали на предыдущих уроках.
  При подключении DBeaver покажет номер версии ClickHouse и пинг до хоста.
  В двух следующих практических работах мы используем кластер для аналитической работы с датасетами и для создания БД ClickHouse.
  Decision:
  $ wget https://dbeaver.io/files/dbeaver-ce_latest_amd64.deb
  $ sudo dpkg -i dbeaver-ce_latest_amd64.deb
  $ dbeaver-ce &
  $ mkdir -p ~/.clickhouse-client
  $ sudo wget "https://storage.yandexcloud.net/mdb/clickhouse-client.conf.example" -O ~/.clickhouse-client/config.xml
  $ sudo apt-get install -y apt-transport-https ca-certificates dirmngr
  $ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 8919F6BD2B48D754
  $ echo "deb https://packages.clickhouse.com/deb stable main" | sudo tee \
      /etc/apt/sources.list.d/clickhouse.list
  $ sudo apt-get update
  $ sudo apt-get install -y clickhouse-server clickhouse-client
  $ clickhouse-client --host rc1a-mg8yquor7pspcwkc.mdb.yandexcloud.net \
                    --secure \
                    --user YOUR-USERNAME \
                    --database YOUR-DB \
                    --port 9440 \
                    --ask-password
  Task:
  Работа с данными из объектного хранилища.
  В интернете выложено множество датасетов —  структурированных наборов данных, связанных общей темой. Например в репозитории проекта Our World in Data находится около тысячи разнообразных датасетов: от численности населения государств до сведений об употреблении алкоголя в США с 1850 года.
  Датасеты часто выкладывают в виде CSV- или TSV-файлов. В них значения разделены запятой (comma separated values, CSV) или табуляцией (tab separated values, TSV).
  Сохраняйте датасеты в объектное хранилище и анализируйте данные с помощью ClickHouse. При этом не требуется создавать БД и копировать в неё данные из датасета. Отправляйте запросы к ClickHouse — а ClickHouse сходит за данными напрямую в объектное хранилище.
  Decision:
  В качестве примера возьмем датасет с историей метеонаблюдений за 10 лет и попробуем развеять мифы о разнице погоды в Москве и Санкт-Петербурге. Датасет содержит примерно 50 тысяч записей, он выложен в объектном хранилище Yandex Cloud и доступен всем.
  Воспользуемся кластером БД, который мы создали на предыдущем уроке. Откройте его в консоли управления. Запросы к датасету будем делать через SQL-консоль. На панели слева выберите вкладку SQL и введите пароль пользователя. В правом поле открывшейся консоли мы и станем вводить SQL-запросы.
  Как вы думаете, где зарегистрирована самая низкая температура? Наверняка в Санкт-Петербурге! Давайте проверим.
  Выполните запрос:
  SELECT
      City,
      LocalDate,
      TempC
  FROM s3(
          'https://storage.yandexcloud.net/arhipov/weather_data.tsv',
          'TSV',
          'LocalDateTime DateTime, LocalDate Date, Month Int8, Day Int8, TempC Float32,Pressure Float32, RelHumidity Int32, WindSpeed10MinAvg Int32, VisibilityKm Float32, City String')
  ORDER BY TempC ASC
  LIMIT 1 
  Всё-таки наши интуитивные представления не всегда верны и могут опровергаться данными.
  А что насчет самой высокой температуры, скорости ветра и влажности? Проверьте сами, изменив поля в запросе (средняя скорость ветра за 10 минут — WindSpeed10MinAvg, относительная влажность — RelHumidity; сортировка по возрастанию — ASC, по убыванию — DESC). Увеличив количество выводимых данных, вы получите более точное представление (измените параметр LIMIT c 1 до 10).
  Но это были крайние значения. Давайте проверим, насколько в этих городах отличается климат в целом. Узнаем, например, разницу среднегодовых температур.
  SELECT
      Year,
      msk.t - spb.t
  FROM
  (
      SELECT
          toYear(LocalDate) AS Year,
          avg(TempC) AS t
      FROM s3(
          'https://storage.yandexcloud.net/arhipov/weather_data.tsv',
          'TSV',
          'LocalDateTime DateTime, LocalDate Date, Month Int8, Day Int8, TempC Float32,Pressure Float32, RelHumidity Int32, WindSpeed10MinAvg Int32, VisibilityKm Float32, City String')
      WHERE City = 'Moscow'
      GROUP BY Year
      ORDER BY Year ASC
  ) AS msk
  INNER JOIN
  (
      SELECT
          toYear(LocalDate) AS Year,
          avg(TempC) AS t
      FROM s3(
          'https://storage.yandexcloud.net/arhipov/weather_data.tsv',
          'TSV',
          'LocalDateTime DateTime, LocalDate Date, Month Int8, Day Int8, TempC Float32,Pressure Float32, RelHumidity Int32, WindSpeed10MinAvg Int32, VisibilityKm Float32, City String')
      WHERE City = 'Saint-Petersburg'
      GROUP BY Year
      ORDER BY Year ASC
  ) AS spb ON msk.Year = spb.Year 
  Измените поля в запросе, чтобы проверить разницу относительной влажности.
  Давайте теперь рассчитаем, где раньше начинается лето. Будем считать началом лета день, начиная с которого температура поднималась выше +15 °С хотя бы пять раз в течение 10-дневного периода (864 тысячи секунд).
  SELECT
      City,
      toYear(LocalDate) AS year,
      MIN(LocalDate)
  FROM
  (
      SELECT
          City,
          LocalDate,
          windowFunnel(864000)(LocalDateTime, TempC >= 15, TempC >= 15, TempC >= 15, TempC >= 15, TempC >= 15) AS warmdays
      FROM s3(
          'https://storage.yandexcloud.net/arhipov/weather_data.tsv',
          'TSV',
          'LocalDateTime DateTime, LocalDate Date, Month Int8, Day Int8, TempC Float32,Pressure Float32, RelHumidity Int32, WindSpeed10MinAvg Int32, VisibilityKm Float32, City String')
      GROUP BY
          City,
          LocalDate
  )
  WHERE warmdays = 5
  GROUP BY
      year,
      City
  ORDER BY
      year ASC,
      City ASC 
  Task:
  Добавление данных. 
  Предположим, вы работаете в метеорологической службе и постоянно изучаете датасеты с погодными данными. 
  Сбор данных о погоде автоматизирован: на территории области расположены несколько десятков пунктов наблюдения с датчиками. 
  Информация о температуре, давлении, влажности и скорости ветра раз в полчаса передаётся с датчиков на центральный сервер. 
  Приложение на сервере обрабатывает данные, переводит их в нужный формат и записывает в файл. Каждый файл содержит данные за три часа наблюдений. 
  Для прогноза нужно учитывать всю историю наблюдений за последние несколько лет, то есть все файлы потребуется собрать в одну БД.
  Давайте потренируемся добавлять данные из файлов в БД ClickHouse.
  На предыдущих уроках мы создали кластер, на котором развёрнута БД, и научились подключаться к нему. 
  Продолжим работать с этой БД, а в качестве добавляемого файла возьмем уже известный вам датасет с данными о погоде в Москве и Санкт-Петербурге.
  Decision:
  Сохраните файл на компьютере.
  Прежде чем добавлять файл в БД, создадим в ней таблицу, куда будут вставляться данные. Перейдите в SQL-консоль кластера и выполните команду:
  CREATE TABLE <имя вашей БД>.Weather
  (  LocalDateTime DateTime,
    LocalDate Date,
    Month Int8,
    Day Int8,
    TempC Float32,
    Pressure Float32,
    RelHumidity Int32,
    WindSpeed10MinAvg Int32,
    VisibilityKm Float32,
    City String
  ) ENGINE=MergeTree
  ORDER BY LocalDateTime; 
  В результате будет создана пустая таблица с полями и типами данных, соответствующими полям и типам данных в нашем файле (датасете).
  Вставим данные в таблицу с помощью клиента командной строки clickhouse-client. Команды для его установки (для Ubuntu):
  sudo apt update && sudo apt install --yes apt-transport-https ca-certificates dirmngr && \
  sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv E0C56BD4 && \
  echo "deb https://repo.clickhouse.com/deb/stable/ main/" | sudo tee \
  /etc/apt/sources.list.d/clickhouse.list
  sudo apt update && sudo apt install --yes clickhouse-client
  mkdir --parents ~/.clickhouse-client && \
  wget "https://storage.yandexcloud.net/mdb/clickhouse-client.conf.example" \
  --output-document ~/.clickhouse-client/config.xml 
  Подробности о том, как установить клиент и работать с ним, вы найдёте в документации ClickHouse.
  Подключитесь к кластеру. Пример строки подключения посмотрите в консоли управления.
  Добавим файл с данными в БД с помощью команды:
  cat weather_data.tsv | clickhouse-client \
  --host <адрес вашей БД> \
  --secure \
  --user user1 \
  --database db1 \
  --port 9440 \
  -q "INSERT INTO db1.Weather FORMAT TabSeparated" \
  --ask-password 
  Переключившись в SQL-консоль, вы увидите, что данные появились в таблице.
  Данные в БД можно загружать и другими способами: из приложений или клиентов с графическим интерфейсом (например DBeaver). В этом случае подключение к БД и передача данных будут идти по HTTP-протоколу через порт 8443.
  Теперь вы можете анализировать 10-летний срез данных о погоде в Москве и Санкт-Петербурге непосредственно в ClickHouse, без обращений к внешним источникам. Попробуйте, например, выяснить, какой день был самым ветреным в этих городах.
  После практической работы остановите кластер, но не удаляйте его. Кластер ещё понадобится, когда мы будем рассматривать сервис визуализации и анализа данных Yandex DataLens.
  Decision:
  $ wget https://storage.yandexcloud.net/arhipov/weather_data.tsv
  $ cat weather_data.tsv | clickhouse-client \
  --host YOUR-IP \
  --secure \
  --user YOUR-USERNAME \
  --database YOUR-DB \
  --port 9440 \
  -q "INSERT INTO YOUR-DB.Weather FORMAT TabSeparated" \
  --ask-password
  Task:
  Создание базы данных. 
  В этой практической работе вы создадите БД YDB в dedicated режиме, научитесь подключаться к ней и добавлять данные из тестового приложения.
  Также вы подключитесь к БД и запустите тестовое приложение, чтобы создать в ней несколько таблиц с данными о популярных сериалах.
  Decision:
  На стартовой странице консоли управления перейдите в каталог, в котором будете создавать БД, выберите в списке сервисов База данных YDB и нажмите кнопку Создать ресурс.
  В открывшемся окне выберите тип БД dedicated. Появившийся интерфейс создания новой БД практически идентичен уже знакомым вам интерфейсам создания кластеров управляемых БД.
  Выберите для вашей БД имя, назначьте необходимые вычислительные ресурсы (для этой и следующих практических работ достаточно одного хоста конфигурации medium), тип и количество групп хранения (достаточно одной группы).
  Группа хранения – это массив независимых дисковых накопителей, объединённых по сети в единый логический элемент. В YDB такой массив состоит из 9 дисков, расположенных по три в каждой из трёх зон доступности. Такая конфигурация обеспечивает устойчивость при одновременном отказе одной из зон и отказе диска в другой зоне. Стандартный размер группы хранения — 100 ГБ.
  Выберите облачную сеть и подсети для работы с БД. Вы можете оставить сеть по умолчанию или выбрать ту, которую создали на предыдущем курсе. БД будет доступна для всех виртуальных машин, которые подключены к той же облачной сети.
  Также выберите опцию присвоения публичного IP-адреса, чтобы иметь возможность подключаться к БД из интернета.Нажмите кнопку Создать базу данных. Создание БД занимает несколько минут. Когда статус БД изменится с Provisioning на Running, она готова к работе. Кликнув на созданную БД в консоли управления, вы перейдёте на вкладку Обзор.
  В разделе Соединение на этой странице приведена информация, которая вам понадобится для подключения к БД:
  - Эндпоинт — точка подключения с указанием протокола, представляющая собой в данном случае адрес, на который посылаются сообщения;
  - Размещение базы данных — полный путь к БД.
  - Примеры подключений из командной строки и приложений вы можете посмотреть, нажав на кнопку Подключиться.
  Подключение к базе данных и запуск тестового приложения. 
  Для того, чтобы выполнить эту задачу, вам понадобится сервисный аккаунт с ролями viewer и editor. Перейдите в дашборд каталога и выберите вкладку Сервисные аккаунты. Создайте сервисный аккаунт, назначив для него указанные роли. Сохраните идентификатор этого аккаунта.
  Вы можете запускать тестовое приложение со своего компьютера или с виртуальной машины в Yandex Cloud. В данном примере используется OC Ubuntu и приложение на Python.
  Если при создании БД вы не присвоили ей публичный IP-адрес, то подключиться к ней вы сможете только с виртуальной машины, расположенной в той же облачной сети.
  Для запуска приложения нужно склонировать на свою машину репозиторий YDB Python SDK, из которого оно будет вызываться, а также установить библиотеки ydb, iso8601 и yandexcloud. Воспользуйтесь для этого следующими командами:
  git clone https://github.com/yandex-cloud/ydb-python-sdk.git
  sudo pip3 install iso8601 ydb yandexcloud 
  Создайте авторизованный ключ для вашего сервисного аккаунта и сохраните его в файл с помощью интерфейса командной строки Yandex Cloud.
  mkdir ~/.ydb
  yc iam key create \
    --folder-id <идентификатор каталога> \
    --service-account-name <имя сервисного аккаунта> \
    --output ~/.ydb/sa_name.json 
      Получите SSL-сертификат:
  wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" \
    -O ~/.ydb/CA.pem 
  Установите переменную окружения YDB_SERVICE_ACCOUNT_KEY_FILE_CREDENTIALS и переменную окружения с SSL-сертификатом.
  export YDB_SERVICE_ACCOUNT_KEY_FILE_CREDENTIALS=~/.ydb/sa_name.json
  export YDB_SSL_ROOT_CERTIFICATES_FILE=~/.ydb/CA.pem 
      Запустите тестовое приложение basic_example_v1 из репозитория ydb-python-sdk, указав в качестве параметров подключения значения протокола, эндпоинта и полного пути к БД.
  cd ./ydb-python-sdk/examples/basic_example_v1
  python3 __main__.py \
  -e <Эндпоинт> \
  -d <Размещение базы данных> 
  Результат выполнения приложения должен выглядеть так:
  > describe table: series
  column, name: series_id , Uint64
  column, name: title , Utf8
  column, name: series_info , Utf8
  column, name: release_date , Uint64
  > select_simple_transaction:
  series, id:  1 , title:  IT Crowd , release date:  b'2006-02-03'
  > bulk upsert: episodes
  > select_prepared_transaction:
  episode title: To Build a Better Beta , air date: b'2016-06-05'
  > select_prepared_transaction:
  episode title: Bachman's Earnings Over-Ride , air date: b'2016-06-12'
  > explicit TCL call
  > select_prepared_transaction:
  episode title: TBD , air date: b'2022-08-24' 
  Вернитесь в консоль управления Yandex Cloud, чтобы посмотреть на результаты работы приложения. Переключитесь на вкладку Навигация.
  В вашей БД созданы три таблицы: episodes, seasons и series с информацией о двух популярных сериалах IT Crowd и Silicon Valley. Кликнув по названию таблицы, вы увидите содержащиеся в ней данные. А если подвести к названию таблицы курсор и кликнуть на значок «информация» справа, то внизу появится дополнительное окно с вкладками Обзор, Схема и Партиции.
  Кнопка Создать на панели Навигация служит для создания директорий и таблиц. С её помощью можно создать новую таблицу, не прибегая к командам YQL.
  Decision:
  $ git clone https://github.com/yandex-cloud/ydb-python-sdk.git
  $ mkdir ~/.ydb
  $ yc iam key create \
  --service-account-name sa-ydb \
  --output ~/.ydb/authorized_key.json 
  $ export YDB_SERVICE_ACCOUNT_KEY_FILE_CREDENTIALS=~/.ydb/authorized_key.json
  $ cd ydb-python-sdk/examples/basic_example_v1
  $ python3 __main__.py \
  -e <Эндпоинт> \
  -d <Размещение базы данных> 
  Task:
  YQL и работа с данными
  В этом уроке вы освоите базовый набор операций для работы с данными с использованием YQL и консоли управления Yandex.Cloud. 
  Decision:
  Чтобы начать, войдите в раздел Навигация консоли управления и откройте редактор SQL, нажав на кнопку SQL-запрос.
  На прошлом уроке мы уже создали в нашей БД три таблицы, содержащие информацию о сериалах IT Crowd и Silicon Valley.
  Добавим в БД еще одну таблицу с рейтингами эпизодов сериала IT Crowd на IMDb.com.
  YQL является диалектом SQL, поэтому многие инструкции в этих языках идентичны.
  Для создания таблицы вам понадобится сделать запрос к БД, содержащий инструкцию CREATE TABLE. Например, если бы мы хотели создать таблицу seasons (она уже есть в вашей БД), то SQL запрос выглядел бы следующим образом:
  CREATE TABLE seasons
  (
      series_id Uint64, 
      season_id Uint64, 
      first_aired Date, 
      last_aired Date, 
      title Utf8, 
          PRIMARY KEY (series_id, season_id)
  ); 
  Обратите внимание, что в пределах директории YDB имена таблиц должны быть уникальны. Первичный ключ (PRIMARY KEY) — это столбец или комбинация столбцов, однозначно идентифицирующих каждую строку в таблице. Он может содержать только неповторяющиеся значения. Для таблицы YDB указание первичного ключа обязательно, при этом он может быть только один.
  Первичный ключ по сути является первичным индексом, который помогает СУБД быстрее обнаруживать отдельные записи в таблице и сокращает время выполнения запросов. Также в таблицу можно добавить один или несколько вторичных индексов. Они служат той же цели, но в отличие от первичного индекса могут содержать повторяющиеся значения. Добавить вторичные индексы можно в любой момент, когда возникнет необходимость, и это не вызовет деградацию производительности БД. Чтобы при создании таблицы добавить в нее вторичный индекс, используется такая конструкция:
  INDEX <имя индекса> GLOBAL ON (<имя столбца1>, <имя столбца2>, ...) 
  Вторичный индекс можно добавить и в уже существующую таблицу. Работа БД при этом не прерывается. В отличие от предыдущего случая в существующую таблицу можно добавлять только один вторичный индекс за раз. Делается это с помощью следующей команды:
  ALTER TABLE <имя таблицы> ADD INDEX <имя индекса> GLOBAL ON (<имя столбца>); 
  Task:
  создайте таблицу ratings, в которой будут содержаться рейтинги всех эпизодов сериала IT Crowd, со столбцами season_id (Uint64), episodes_id (Uint64), title (Utf8), air_date (Date) и imdb_rating (Uint64) и вторичным индексом rating_index по полю imdb_rating.
  Decision:
  CREATE TABLE ratings (
      season_id Uint64, 
      episodes_id Uint64, 
      title Utf8, 
      air_date Date, 
      imdb_rating Uint64, 
          PRIMARY KEY (season_id, episodes_id), 
          INDEX rating_index GLOBAL ON (imdb_rating)
  ); 
  Decision:
  Добавим в эту таблицу данные. Для вставки данных в YDB помимо обычной SQL инструкции INSERT также используются инструкции REPLACE и UPSERT.
  При выполнении INSERT перед операцией записи выполняется операция чтения данных. Это позволяет убедиться, что уникальность первичного ключа будет соблюдена. При выполнении инструкций REPLACE и UPSERT осуществляется слепая запись.
  Инструкции REPLACE и UPSERT используются для добавления новой или изменения существующей строки по заданному значению первичного ключа. При операциях записи и изменения данных использование этих инструкций эффективнее.
  Если при выполнении этих инструкций строка с указанным значением первичного ключа не существует, то она будет создана. Если же такая строка существует, то значения ее столбцов будут заменены на новые. Отличие между REPLACE и UPSERT заключается в том, что первая из этих инструкций устанавливает значения столбцов, не участвующих в операции, в значения по умолчанию, а вторая такие значения не меняет.
  Одним запросом REPLACE, UPSERT или INSERT можно вставить в таблицу несколько строк.
  Например, если бы мы хотели добавить в таблицу series те данные, которые в ней сейчас содержатся, то SQL запрос выглядел бы так:
  REPLACE INTO series (series_id, title, release_date, series_info) 
  VALUES 
      ( 
          1, 
          "IT Crowd", 
          Date("2006-02-03"), 
          "The IT Crowd is a British sitcom produced by Channel 4, written by Graham Linehan, produced by Ash Atalla and starring Chris O'Dowd, Richard Ayoade, Katherine Parkinson, and Matt Berry."), 
      ( 
          2, 
          "Silicon Valley", 
          Date("2014-04-06"), 
          "Silicon Valley is an American comedy television series created by Mike Judge, John Altschuler and Dave Krinsky. The series focuses on five young men who founded a startup company in Silicon Valley." 
      ); 
  Task:
  добавьте в таблицу ratings данные из этого файла.
  Decision:
  REPLACE INTO ratings (season_id, episodes_id, title, air_date, imdb_rating) VALUES 
      (1, 1, "Yesterday's Jam", Date("2006-02-03"), 76),
      (1, 2, "Calamity Jen", Date("2006-02-03"), 82),
      (1, 3, "Fifty-Fifty", Date("2006-02-10"), 79),
      (1, 4, "The Red Door", Date("2006-02-17"), 80),
      (1, 5, "The Haunting of Bill Crouse", Date("2006-02-24"), 85),
      (1, 6, "Aunt Irma Visits", Date("2006-03-03"), 81),
      (2, 1, "The Work Outing", Date("2006-08-24"), 95),
      (2, 2, "Return of the Golden Child", Date("2007-08-31"), 82),
      (2, 3, "Moss and the German", Date("2007-09-07"), 82),
      (2, 4, "The Dinner Party", Date("2007-09-14"), 87),
      (2, 5, "Smoke and Mirrors", Date("2007-09-21"), 78),
      (2, 6, "Men Without Women", Date("2007-09-28"), 76),
      (3, 1, "From Hell", Date("2008-11-21"), 78),
      (3, 2, "Are We Not Men?", Date("2008-11-28"), 85),
      (3, 3, "Tramps Like Us", Date("2008-12-05"), 82),
      (3, 4, "The Speech", Date("2008-12-12"), 90),
      (3, 5, "Friendface", Date("2008-12-19"), 85),
      (3, 6, "Calendar Geeks", Date("2008-12-26"), 78),
      (4, 1, "Jen The Fredo", Date("2010-06-25"), 80),
      (4, 2, "The Final Countdown", Date("2010-07-02"), 84),
      (4, 3, "Something Happened", Date("2010-07-09"), 75),
      (4, 4, "Italian For Beginners", Date("2010-07-16"), 82),
      (4, 5, "Bad Boys", Date("2010-07-23"), 84),
      (4, 6, "Reynholm vs Reynholm", Date("2010-07-30"), 76); 
  Decision:
  C помощью SQL запросов можно добавлять и удалять не только строки таблицы, но и столбцы. Для этого используется команда ALTER TABLE и фразы ADD COLUMN и DROP COLUMN.
  Например, если вы хотите добавить в таблицу ratings столбец viewed с данными о том, какие эпизоды сериала вы уже посмотрели, то это можно сделать с помощью следующей команды.
  ALTER TABLE ratings ADD COLUMN viewed Bool;  
  Task:
  Вы решили, что столбец с датой выхода эпизодов в таблице ratings не нужен, поскольку эта информация уже содержится в другой таблице. Удалите столбец air_date из таблицы ratings.
  Decision:
  ALTER TABLE ratings DROP COLUMN air_date; 
  Decision: 
  Теперь потренируемся извлекать данные из БД. Для этого используется команда SELECT. В простейшем случае ее синтаксис выглядит так:
  SELECT <имя столбца1>, <имя столбца2>, ...
  FROM <имя таблицы>; 
  Например, чтобы выбрать всю информацию из таблицы seasons, нужно сделать следующий запрос к БД.
  SELECT * FROM seasons; 
  Если нужно выбрать из таблицы только те строки, которые удовлетворяют определенному условию, в запросе используют секцию WHERE. В этой секции должно находиться выражение, возвращающее логический результат. Обычно оно состоит из логических операций and, or, not и операций сравнения.
  Например, выбрать из таблицы episodes только первые эпизоды всех сезонов можно так:
  SELECT * FROM episodes
  WHERE episode_id = 1;  
  Запрос SELECT извлекает строки без определенного порядка. Чтобы отсортировать полученные данные нужным образом, в этот запрос включают секцию ORDER BY. В ней указывается список столбцов, которые будут определять порядок сортировки результатов запроса.
  Task: 
  получите список самых популярных (с рейтингом не менее 85) эпизодов сериала IT Crowd. При поиске используйте созданный ранее вторичный индекс rating_index. Чтобы упорядочить результаты по убыванию рейтинга используйте конструкцию ORDER BY … DESC.
  Decision:
  SELECT 
      season_id, 
      episodes_id, 
      title, 
      imdb_rating
  FROM ratings VIEW rating_index 
  WHERE 
      imdb_rating >= 85 
  ORDER BY 
      imdb_rating DESC;  
  Decision:
  Для получения обобщённых сведений о содержащихся в таблице данных — например, о числе строк в таблице или среднем значении какого-либо выражения — в запрос SELECT включают агрегатные функции и секцию GROUP BY. Эта секция используется для агрегации внутри каждого ключа. Ключом является значение одной или более колонок, указанных в GROUP BY.
  Примеры агрегатных функций:
  COUNT(*) — вычисляет число строк в таблице.
  MAX(expr) — находит максимум выражения expr по всем строкам.
  SUM(expr) — суммирует выражение expr по всем строкам. Тип выражения должен быть числовым.
  AVG(expr) — находит среднее значение выражения expr по всем строкам. Тип выражения должен быть числовым или интервалом.
  SOME(expr) — возвращает одно произвольное значение выражения по всем строкам.
  Результаты выполнения агрегатной функции выводятся в отдельном столбце. Чтобы задать этому столбцу имя, используют оператор AS. Конструкция может выглядеть, например, так:
  SELECT 
      <имя столбца1>, 
      MAX(<имя столбца2>) AS max_value
  ...; 
  Task:
  Напишите SQL запрос к таблице episodes, который выводит данные о числе эпизодов каждого сериала.
  Вам понадобится вычислить число строк для каждого значения столбца series_id и сгруппировать результаты по series_id.
  Decision:
  SELECT 
      series_id, 
      COUNT(*) AS total_episodes 
  FROM episodes 
  GROUP BY 
      series_id 
  ORDER BY 
      series_id; 
  Task: 
  Напишите SQL запрос, с помощью которого можно сравнить популярность сезонов сериала IT Crowd. 
  Вам понадобится вычислить средний рейтинг эпизодов для каждого сезона и сгруппировать результаты по столбцу season_id.
  Decision:
  SELECT 
      season_id, 
      AVG (imdb_rating) AS avg_rating
  FROM ratings 
  GROUP BY season_id
  ORDER BY avg_rating DESC; 
  Decision:
  В реляционной БД таблицы логически связаны друг с другом. С помощью объединений (JOIN) можно получить данные из нескольких связанных друг с другом таблиц и представить их в виде одной результирующей таблицы.
  Столбцы, по которым выполняется объединение, можно указать одним из двух способов.
  - После ключевого слова USING, например table1 AS a JOIN table2 AS b USING (foo). Это более короткий способ записи, удобный для простых случаев. Имена столбцов, по которым происходит объединение таблиц, должны быть одинаковы.
  - После ключевого слова ON (например, a JOIN b ON a.foo = b.bar). Этот способ позволяет использовать разные имена столбцов и указывать дополнительные условия по аналогии с WHERE.
  Поскольку такие запросы затрагивают столбцы разных таблиц, имена столбцов должны содержать и имя таблицы (то есть, например, не просто series_id, а seasons.series_id).
  В YDB доступны следующие логические типы объединений:
  INNER (используется по умолчанию) — строки попадают в результат, только если значение ключевых колонок присутствует в обеих таблицах;
  FULL, LEFT и RIGHT — при отсутствии значения в обеих или в одной из таблиц включает строку в результат, но оставляет пустыми (NULL) колонки, соответствующие противоположной таблице.
  LEFT/RIGHT SEMI — одна сторона выступает как белый список (whitelist) ключей, её значения недоступны. В результат включаются столбцы только из одной таблицы, декартового произведения не возникает;
  LEFT/RIGHT ONLY — вычитание множеств по ключам (blacklist). Практически эквивалентно добавлению условия IS NULL на ключ противоположной стороны в обычном LEFT/RIGHT, но, как и в SEMI, нет доступа к значениям;
  CROSS — декартово произведение двух таблиц целиком без указания ключевых колонок, секция с ON/USING явно не пишется;
  EXCLUSION — обе стороны минус пересечение.
  Простой пример запроса с объединением таблиц приведен ниже.
  SELECT
      sa.title AS season_title,
      sr.title AS series_title,
      sr.series_id, sa.season_id 
  FROM seasons AS sa
  INNER JOIN series AS sr ON sa.series_id = sr.series_id 
  WHERE sa.season_id = 1
  ORDER BY sr.series_id; 
  Этот запрос извлекает из таблиц series и seasons сведения о первых сезонах всех сериалов и выводит объединённые данные в результирующей таблице.
  Task: 
  напишите запрос, который выводит таблицу, содержащую название сериала IT Crowd и названия всех его эпизодов (то есть, каждая строка итоговой таблице должна содержать название сериала и название отдельного эпизода).
  Decision:
  SELECT 
      sr.title AS series_title, 
      ep.title AS episode_title, 
      ep.season_id,     
      ep.episode_id 
  FROM 
      series AS sr 
  INNER JOIN 
      episodes AS ep 
  ON sr.series_id = ep.series_id 
  WHERE sr.series_id = 1 
  ORDER BY 
      ep.season_id,     
      ep.episode_id;  
  Task:
  Создание кластера Hadoop.
  На этом уроке вы создадите и настроите кластер Hadoop с помощью сервиса Yandex Data Proc. 
  Hadoop предназначается для работы с большими данными, поэтому создание кластера потребует от вас больше усилий, чем на предыдущих практических работах (но гораздо меньше, чем если бы вы делали это самостоятельно).
  Decision:
  Для хранения зависимостей заданий нашего кластера и результатов их выполнения нужно предварительно создать бакет в объектном хранилище. О том, как это сделать, мы рассказывали на одном из предыдущих занятий.
  Также создайте сервисный аккаунт для доступа к кластеру. Обратите внимание: можно использовать только аккаунт с ролью mdb.dataproc.agent. Для автоматического масштабирования кластера сервисному аккаунту также понадобятся роли editor и dataproc.agent.
  Откройте каталог, где будете создавать кластер, и выберите сервис Data Proc.
  В открывшемся окне нажмите кнопку Создать кластер.
  Задайте для кластера имя и выберите версию образа — 1.4. В образ включена одна из версий Hadoop и дополнительные компоненты. Некоторые вы можете устанавливать по выбору. Кроме того, в каждую версию образа входит Conda (менеджер окружений для Python) и набор инструментов машинного обучения (scikit-learn, TensorFlow, CatBoost, LightGBM и XGBoost).
  Обратите внимание на то, что некоторые из сервисов обязательны, чтобы использовать другие. На следующем уроке нам понадобится сервис HIVE. Выберите его, и рядом с MAPREDUCE и YARN вы увидите напоминания о том, что они нужны для HIVE.
  Вставьте в поле публичный ключ публичную часть SSH-ключа. Как сгенерировать и использовать SSH-ключи, мы рассказывали в одной из практических работ о виртуальных машинах.
  Выберите созданный сервисный аккаунт для доступа к кластеру.
  Выберите зону доступности для кластера. Все подкластеры будут находиться в этой 
  Если нужно, задайте свойства Hadoop и его компонентов. Доступные свойства перечислены в документации.
  Выберите бакет в объектном хранилище, где будут храниться зависимости заданий и результаты их выполнения.
  Выберите или создайте сеть для кластера. Включите опцию NAT в интернет для подсетей, в которых размещается кластер.
  Если нужно, создайте группу безопасности. Правила для неё вы добавите позже в сервисе Virtual Private Cloud.
  Включите опцию UI Proxy, чтобы получить доступ к веб-интерфейсам компонентов Data Proc. У некоторых компонентов (например Hadoop, Spark, YARN и Zeppelin) есть пользовательские веб-интерфейсы, доступные на мастер-узле кластера. С помощью этих интерфейсов вы можете:
  - отслеживать ресурсы кластера и управлять ими (YARN Resource Manager, HDFS NameNode);
  - просматривать статус и отлаживать задания (Spark History, JobHistory);
  - проводить эксперименты, совместно работать или выполнять отдельные операции (Zeppelin).
  Настройка подкластеров. В состав кластера входит один главный подкластер (Мастер) с управляющим хостом, а также подкластеры для хранения данных (Data) или вычислений (Compute).
  В подкластерах Data можно разворачивать компоненты для хранения данных, а в подкластерах Compute — для обработки данных. Хранилище в подкластере Compute предназначено только для временного хранения обрабатываемых файлов.
  Для каждого подкластера можно задать число и класс хостов, размер и тип хранилища, а также подсеть той сети, в которой расположен кластер. Кроме того, для подкластеров Compute можно настроить автоматическое масштабирование. Это позволит выполнять задания на обработку данных быстрее без дополнительных усилий с вашей стороны.
  Создадим подкластер Compute с одним хостом.
  В блоке Добавить подкластер нажмите кнопку Добавить.
  В поле Роли выберите COMPUTENODE. В блоке Масштабирование включите опцию Автоматическое масштабирование.
  Все открывшиеся настройки знакомы вам из практических работ по созданию виртуальных машин.
  Автоматическое масштабирование подкластеров обработки данных поддерживается в кластерах Yandex Data Proc версии 1.2 и выше. Чтобы оно работало, в кластере с установленным Spark или Hive должен быть также установлен сервис YARN.
  Yandex Data Proc автоматически масштабирует кластер, используя для этого системные метрики нагрузки на кластер. Когда их значение выходит из установленного диапазона, запускается масштабирование. Если значение метрики превысит порог, в подкластер добавятся хосты. Если опустится ниже порога, начнётся декомиссия (высвобождение ненужных ресурсов), а избыточные хосты удалятся.
  По умолчанию для масштабирования используется внутренняя метрика YARN (yarn.cluster.containersPending). Она показывает, сколько единиц ресурсов нужно заданиям в очереди. Выбирайте эту опцию Масштабирование по умолчанию, если в кластере выполняется много относительно небольших заданий.
  Другой вариант — масштабирование на основе метрики загрузки процессора (vCPU). Чтобы использовать его, отключите опцию Масштабирование по умолчанию и укажите целевой уровень загрузки vCPU.
  Настроив подкластеры, нажмите кнопку Создать кластер.
  Сервис запустит создание кластера. После того как статус кластера изменится на Running, вы сможете подключиться к любому активному подкластеру с помощью указанного в настройках SSH-ключа.
  Task:
  Подключение к кластеру и работа с Hive.
  На этом уроке вы научитесь подключаться к кластеру Hadoop и работать с ним на примере выполнения запросов с помощью Hive.
  Decision:
  Подключимся к управляющему хосту главного подкластера. Поскольку хостам кластера Hadoop не назначается публичный IP-адрес, для подключения к ним нужна виртуальная машина, расположенная в той же сети Yandex Cloud.
  Выберите машину, которую создавали раньше, или создайте новую. Подключитесь к ней по SSH. Вы уже делали это, когда изучали виртуальные машины.
  Подключитесь с этой машины к хосту главного подкластера также с помощью SSH. Для этого на машине должна быть закрытая часть SSH-ключа, открытую часть которого вы указали при создании кластера Data Proc. Вы можете скопировать ключ на виртуальную машину или подключаться к ней с запущенным SSH-агентом.
  Скопировать ключ можно с помощью утилиты nano. На виртуальной машине выполните команду:
  sudo nano ~/.ssh/<имя ключа> 
  В открывшийся редактор скопируйте содержимое закрытой части SSH-ключа с вашей локальной машины.
  Запустите SSH-агент:
  eval `ssh-agent -s` 
  Добавьте ключ в список доступных агенту:
  ssh-add ~/.ssh/<имя ключа> 
  Узнайте внутренний FQDN хоста главного подкластера. Для этого в консоли управления на странице кластера перейдите на вкладку Хосты и выберите хост с ролью MASTERNODE.
  Откройте SSH-соединение с хостом Data Proc для пользователя root, например:
  ssh root@<FQDN хоста> 
  Пошаговые инструкции по различным способам подключения к кластеру Data Proc приведены в документации.
  Проверим, что команды Hadoop выполняются, например:
  hadoop version 
  Результат выполнения этой команды выглядит так:
  Запуск заданий Apache Hive
  Как мы уже говорили ранее, Hive — это платформа для хранения данных и управления ими в экосистеме Hadoop. Она используется для доступа к большим датасетам, сохранённым в распределённом хранилище.
  Hive позволяет работать с данными различного формата (csv, tsv, Parquet, ORC, Avro и другими), подключаться к БД и взаимодействовать с ней с помощью SQL-подобного языка запросов. Hive используется преимущественно для работы с данными в HDFS, HBase, S3-совместимых хранилищах и реляционных СУБД.
  Запрос на действия с данными в Hive называется заданием. Задания можно запускать на управляющем хосте с помощью командной оболочки CLI Hive, а также с помощью CLI Yandex Cloud.
  Для запуска Hive CLI выполните команду hive на управляющем хосте.
  Проверьте, всё ли работает: выполните, например, команду select 1; — корректный результат выглядит так:
  Теперь создайте внешнюю таблицу (external table) в формате Parquet, содержащую открытые данные о списке перелётов между городами США в 2018 году. Для этого с помощью Hive CLI выполните запрос:
  hive> CREATE EXTERNAL TABLE flights (Year bigint, Month bigint, FlightDate string, Flight_Number_Reporting_Airline bigint, OriginAirportID bigint, DestAirportID bigint) STORED AS PARQUET LOCATION 's3a://yc-mdb-examples/dataproc/example01/set01'; 
  Проверим список таблиц, выполнив команду show tables. Результат должен выглядеть так:
  Запросим число перелётов с разбивкой по месяцам:
  hive> SELECT Month, COUNT(*) FROM flights GROUP BY Month; 
  Пример результата такого запроса:
  Безусловно, на одном примере сложно показать возможности сервиса Data Proc. Если вас интересует работа с большими данными в облаке, посмотрите доклады сотрудников Yandex Cloud об управлении кластерами Hadoop и заданиями в Data Proc на YouTube-канале Yandex Cloud.
  Decision:
  $ cat YOUR-KEY
  $ eval `ssh-agent -s`
  $ ssh-add YOUR-KEY
  $ ssh root@<FQDN хоста> 
  $ hadoop version
  $ hive
  hive> select 1;
  hive> CREATE EXTERNAL TABLE flights (Year bigint, Month bigint, FlightDate string, Flight_Number_Reporting_Airline bigint, OriginAirportID bigint, DestAirportID bigint) STORED AS PARQUET LOCATION 's3a://yc-mdb-examples/dataproc/example01/set01';
  hive> show tables;
  hive> SELECT Month, COUNT(*) FROM flights GROUP BY Month;
  Task:
  Создание датасетов, чартов и дашбордов.
  На этом уроке вы научитесь создавать чарты и дашборды. Мы пройдём по всей цепочке сущностей DataLens начиная с источника данных.
  Изучая ClickHouse, мы анализировали данные о погоде с помощью SQL-запросов. 
  Давайте посмотрим на примере того же самого набора данных, как с помощью DataLens быстро и наглядно показать отличия климата в Москве и Санкт-Петербурге.
  Decision:
  Источник данных.ClickHouse и DataLens интегрированы друг с другом, поэтому подключение DataLens к ClickHouse можно настроить всего за пару кликов.
  В консоли управления запустите кластер ClickHouse, в котором развёрнута БД с таблицей Weather, созданной вами ранее. Перейдите на страницу кластера, на панели слева выберите DataLens.
  Подключение. Нажмите кнопку Создать подключение. В открывшемся диалоговом окне вы увидите, что кластер ClickHouse, из которого мы возьмём данные для анализа, имя хоста и имя пользователя БД уже указаны.
  Вам осталось только дать имя подключению в пустом поле вверху, ввести пароль к БД, нажать кнопку Проверить подключение и убедиться, что всё в порядке, а потом — кнопку Создать.
  Датасет. После того как подключение будет создано, DataLens выведет на панели слева таблицы из БД и предложит создать датасет. Наш датасет будет состоять из одной таблицы: db1.Weather. Перетащите её на центральную панель, и внизу откроется предпросмотр данных.
  Нажмите кнопку Сохранить и задайте имя датасета.
  Подготовим данные. Это важная часть аналитической работы, и её не стоит пропускать. Прежде всего укажем имена полей на русском языке. Перейдите на вкладку Поля и переименуйте их:
  - LocalDateTime - Дата и время
  - LocalDate - Дата
  - Month - Месяц
  - Day - День
  - TempC - Температура
  - Pressure - Давление
  - RelHumidity - Влажность
  - Тип WindSpeed10MinAvg - Скорость ветра
  - VisibilityKm - Видимость
  - City - Город
  Поля Дата и время, Дата, Месяц, День, Город будут полями-измерениями, а Температура, Давление, Влажность, Скорость ветра, Видимость — полями-показателями. Зададим для показателей тип агрегации Среднее.
  Чарты. Приступим к созданию первого чарта. Нажмите кнопку Создать чарт. Выберите тип чарта Линейная диаграмма и перетащите Дата в раздел X , а Температура — в раздел Y.
  На этом примере видно, что средства визуализации иногда помогают быстро проверить качество датасета: есть ли в нём пропущенные или странные, выбивающиеся из общей тенденции данные.
  В нашем случае можно сделать вывод о том, что в датасете не хватает данных примерно с середины 2015-го по середину 2016-го.
  Разделим показатели температуры для двух городов. Для этого перетащим Город в раздел Цвета. Кроме того, округлим значения поля Дата до месяцев, чтобы лучше увидеть, как различаются данные для Москвы и Санкт-Петербурга. Для этого слева от поля Дата нажмите зелёный значок календаря и в разделе Группировка выберите округление по месяцам.
  Из этого графика уже можно делать выводы. В целом температура в Москве выше, чем в Санкт-Петербурге. Летом примерно на 5 градусов, зимой — на 1−2 градуса.
  Сохраните чарт, чтобы затем использовать его для дашборда.
  Чтобы окончательно разобраться с температурой, построим ещё один чарт — Столбчатую диаграмму — и сравним среднегодовую температуру. Выберите тип диаграммы. Добавьте поле Город в раздел X, чтобы разделить отображение значений температуры. Также для поля Дата выберите группировку по годам.
  Кроме того, для чарта понадобится задать фильтр по датам. Поскольку мы сравниваем среднегодовые значения, неполные данные за 2009 и 2019 годы отбросим. В разделе Фильтры нажмите + и выберите поле Дата.
  Для этого чарта мы возьмём только данные из диапазона с начала 2010-го по конец 2018-го. Нажмите кнопку Применить фильтр и сохраните чарт.
  Сделайте сами два таких же чарта с данными о скорости ветра: линейную диаграмму со среднемесячными значениями скорости ветра в городах и столбчатую диаграмму со среднегодовыми значениями.
  Теперь у нас достаточно чартов для информативного дашборда.
  Дашборд. На панели слева выберите Дашборды и нажмите кнопку Создать дашборд. Введите название дашборда и нажмите Создать.
  Если в каталоге это первый дашборд — он откроется сразу после создания. Если в каталоге есть другие дашборды, вы увидите список. В этом случае выберите из списка только что созданный дашборд.
  Теперь добавим созданные нами чарты на дашборд. Нажмите Добавить и в выпадающем списке выберите Чарт. Поочерёдно выбирайте из списка и добавляйте чарты.
  В результате на дашборде появятся четыре виджета с чартами. Меняйте размеры и положение виджетов для лучшей визуализации.
  Осталось лишь несколько последних штрихов. В том же пункте меню Добавить создадим пару заголовков и селектор по датам. В правом верхнем углу каждого виджета нажмите значок шестерёнки, чтобы изменить названия. Сохраним дашборд. 
  То, какие чарты сделать и как их разместить на дашборде, бывает понятно не сразу. Рассмотрите несколько вариантов, когда строите дашборд, чтобы разобраться, какая именно визуализация лучше помогает ответить на вопросы.
  В маркетплейсе DataLens вы найдёте ещё один дашборд с погодой. Он хорошо демонстрирует возможности визуализации данных этого сервиса.
  Чтобы открыть публичный доступ к дашборду, справа от его названия нажмите ··· и выберите Публичный доступ. Скопируйте ссылку. По ней дашборд будет доступен всем, с любых устройств и без аутентификации.

Devops и автоматизация в Yandex Cloud
  Task:
  Для защиты практической работы по теме "Devops и автоматизация" поднял кластер Kubernetes в Yandex Cloud, благодаря которому развернул приложение веб-сервер NGINX c Балансировкой нагрузки и Автомасштабированием в Yandex Managed Kubernetes и проверил на отказоустойчивость по основным сценариям сбоев
  Task:
  Начало работы в CLI. На этой практической работе мы установим утилиту yc, познакомимся с режимом подсказок --help и выполним несколько простых команд.
  Decision:
  Первым делом скачайте и установите CLI (если вы ещё не сделали этого в предыдущих курсах).
  Теперь настройте программу для работы с вашим аккаунтом и облаком. Для этого запустите командную строку и введите команду:
  yc init
  В консоли появится предложение перейти по ссылке, чтобы программа получила доступ к аккаунту и облаку. Перейдите по ссылке, согласитесь с условиями, затем скопируйте токен и вставьте его в окно командной строки.
  Срок жизни токена — один год. Через год необходимо получить новый токен и повторить аутентификацию.
  Если кто-то узнал ваш токен, отзовите его и запросите новый.
  Продолжайте установку: выберите облако, каталог и зону доступности по умолчанию. Следуйте указаниям системы и завершите настройку.
  При установке автоматически создается профиль default, в него записываются выбранные настройки.
  Список профилей можно посмотреть командой:
  yc config profile list
  Флаг --help
  На прошлом уроке вы узнали о флагах, которые используются при запуске команд. Пожалуй, самый важный и частый флаг — --help (сокращенно -h), поэтому поговорим о нём подробнее.
  Команд для управления ресурсами облака много. Вы запомните некоторые, но помнить всё невозможно. Пользуйтесь флагом --help, когда пишете команду. В этом случае она не выполняется, а в консоль выводится информация о ресурсах, которыми управляет команда, и параметрах запуска.
  Помните, на предыдущем уроке мы говорили о типичной структуре большинства команд CLI (сервис — ресурс — действие — флаги)?
  Важная особенность флага --help заключается в том, что его можно применять для каждого уровня этой структуры и писать команду постепенно.
  В этой практической работе нам понадобится ВМ в облаке. Если у вас нет ВМ — создайте её в консоли управления, как мы это делали в предыдущих курсах.
  Допустим, вы хотите перезапустить эту ВМ, но не помните полный синтаксис команды.
  Сначала вызовите описание сервиса Yandex Compute Cloud:
  yc compute --help
  Вы увидите синтаксис команды (раздел Usage), список ресурсов, которыми можно управлять (Groups), а также действий (Commands) и флагов
  Вам нужна ВМ — ресурс instance. Теперь узнайте, как получить список активных ВМ и какая команда отвечает за перезапуск:
  yc compute instance --help
  Сначала получите список ВМ, это команда list:
  yc compute instance list
  Результатом выполнения команды будет список машин в том каталоге, где вы работаете, с именами и идентификаторами:
  +----------------------+-------------+---------------+---------+----------------+-------------+
  |          ID          |    NAME     |    ZONE ID    | STATUS  |  EXTERNAL IP   | INTERNAL IP |
  +----------------------+-------------+---------------+---------+----------------+-------------+
  | fhm2p20bifmg3k3voda7 | my-instance | ru-central1-a | RUNNING | ХХХ.ХХХ.ХХХ.ХХ | ХХ.ХХХ.Х.ХХ |
  +----------------------+-------------+---------------+---------+----------------+-------------+
  На шаге 2 вы нашли команду перезапуска — restart, теперь можно посмотреть её синтаксис:
  yc compute instance restart --help
  Наконец, вы получили полный синтаксис команды перезапуска. Примените её к ВМ:
  yc compute instance restart --name <имя_ВМ>
  По такому принципу вы можете сформировать в консоли любую команду.
  Итак, подведём итоги. Чтобы уточнить синтаксис или порядок действий при работе с CLI, выберите один из трёх путей: найдите нужное действие на странице сервиса Yandex Cloud в документации и там переключитесь на вкладку CLI; найти команду в справочнике о yc; воспользуйтесь флагом --help, шаг за шагом уточняя возможности и синтаксис команды.
  Синхронный и асинхронный режимы работы. Некоторые команды выполняются очень быстро. Например, создание каталога или просмотр настроек профиля. Такие команды можно выполнять подряд, одну за другой — без задержек.
  Если при выполнении команды ресурс изменяет состояние, создается операция. Примеры операций — перезапуск ВМ после обновления или резервное копирование базы данных.
  Если каждая следующая команда ждёт завершения предыдущей операции, такой режим выполнения называется синхронным. Когда какая-то операция в синхронном режиме выполняется долго, CLI выводит в консоли точки и время с начала операции, чтобы показать, что процесс не завис:
  ...1s...6s...12s...17s
  Операция может выполняться довольно долго, а ожидание — затормозить процесс. В таких случаях важно оценить, нужен ли результат операции для выполнения следующих команд. Если нет — можно не ждать её завершения и сразу же переходить к следующей команде. Этот режим называется асинхронным.
  Чтобы выполнить команду асинхронно, используйте флаг --async.
  Перезапустите одну из ранее созданных ВМ в асинхронном режиме (в команде укажите имя этой ВМ):
  yc compute instance restart --name <имя_ВМ> --async
  В ответ на асинхронный вызов CLI выводит идентификатор операции (в поле id) и информацию о ней:
  id: fhm5k7iq03rm2s7enhdk
  description: Restart instance
  created_at: "2021-03-27T08:32:47.562595036Z"
  created_by: aje9cb7k03512mrugcee
  modified_at: "2021-03-27T08:32:47.562595036Z"
  metadata:
    '@type': type.googleapis.com/yandex.cloud.compute.v1.RestartInstanceMetadata
    instance_id: fhm2p20bifmg3k3voda7
  С помощью идентификатора операции вы можете проверить результаты её выполнения:
  yc operation get <идентификатор_операции>
  Когда операция завершится, вы увидите результат:
  id: fhm5k7iq03rm2s7enhdk
  ...
  done: true
  ...
  Decision:
  $ yc init
  $ yc config profile list
  $ yc compute --help
  $ yc compute instance --help
  $ yc compute instance list
  $ yc compute instance restart --help
  $ yc compute instance restart --name ubuntu-test
  $ yc compute instance restart --name ubuntu-test --async
  $ yc operation get fhmq5a4aren4lmigbt03
  Task:
  Создание виртуальных машин с помощью CLI. Создание ВМ — одна из самых сложных команд CLI, потому что в ней очень много параметров.
  Давайте потренируемся работать с ней.  Но сначала подготовим окружение.
  Мы будем работать в каталоге по умолчанию. Создадим в нём сеть, в ней — три подсети, а затем по ВМ в каждой подсети.
  Decision:
  Создайте сеть my-network в Virtual Private Cloud. Эта команда относится к группе vpc.
  yc vpc network create --name my-network
  Выполнение операции займёт какое-то время. В итоге сеть появится в каталоге, который вы выбрали в предыдущей практической работе.
  Теперь создадим три подсети в разных зонах доступности (ru-central1-a, ru-central1-b, ru-central1-c). Чтобы проще было выполнять задания дальше, назовите их my-subnet-1, my-subnet-2 и my-subnet-3. А пространства IP-адресов для подсетей укажите, соответственно, как 192.168.1.0/24, 192.168.2.0/24 и 192.168.3.0/24.
  Не забудьте указать, что вы создаёте подсети в новой сети, созданной на предыдущем шаге. Иначе они появятся в сети по умолчанию, которая есть в каждом каталоге.
  Вот так выглядит команда создания подсети в зоне доступности ru-central1-a:
  yc vpc subnet create \
    --name my-subnet-1 \
    --zone ru-central1-a \
    --range 192.168.1.0/24 \
    --network-name my-network
  Где: name — имя подсети. zone — зона доступности. range — адресное пространство подсети. network-name — имя сети, в которой создаётся подсеть.
  Создайте две другие подсети сами.
  yc vpc subnet create \
    --name my-subnet-2 \
    --zone ru-central1-b \
    --range 192.168.2.0/24 \
    --network-name my-network
  yc vpc subnet create \
    --name my-subnet-3 \
    --zone ru-central1-c \
    --range 192.168.3.0/24 \
    --network-name my-network
  Осталось создать три ВМ в нужных зонах доступности и привязать к ним подсети.
  Пусть машины работают под управлением ОС Ubuntu 20.04 LTS, имеют диски объёмом 30 Гб, 4 Гб оперативной памяти и два виртуальных процессорных ядра.
  ВМ могут создаваться долго, поэтому запускайте команды в асинхронном режиме.
  Пример команды для первой ВМ:
  yc compute instance create \
    --name my-instance-1 \
    --hostname my-instance-1 \
    --zone ru-central1-a \
    --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \
    --image-folder-id standard-images \
    --memory 4 --cores 2 --core-fraction 100 \
    --network-interface subnet-name=my-subnet-1,nat-ip-version=ipv4 \
    --async
  Посмотрите внимательно на все параметры. Подумайте, какой из них за что отвечает.
  Создайте две другие машины сами.
  yc compute instance create \
    --name my-instance-2 \
    --hostname my-instance-2 \
    --zone ru-central1-b \
    --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \
    --image-folder-id standard-images \
    --memory 4 --cores 2 --core-fraction 100 \
    --network-interface subnet-name=my-subnet-2,nat-ip-version=ipv4 \
    --async
  yc compute instance create \
    --name my-instance-3 \
    --hostname my-instance-3 \
    --zone ru-central1-c \
    --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \
    --image-folder-id standard-images \
    --memory 4 --cores 2 --core-fraction 100 \
    --network-interface subnet-name=my-subnet-3,nat-ip-version=ipv4 \
    --async
  После выполнения каждой команды благодаря флагу --async вы получите идентификатор операции и ее описание в виде:
  id: c9q9v4bsn1hs9api4b13
  description: Create instance
  created_at: "2021-03-01T03:23:00.079888Z"
  created_by: aje8s4vd4pp7cduq2o4k
  modified_at: "2022-07-29T09:14:53.567744154Z"
  metadata:
    '@type': type.googleapis.com/yandex.cloud.compute.v1.CreateInstanceMetadata
    instance_id: fhmepiaciq5l9slqid3k
  Проследите за статусом одной из операций, используя ее идентификатор.
  Проверьте синтаксис команды.
  yc operation get <идентификатор_операции>
  Дождитесь, пока операция завершится. Используйте для этого команду wait.
  Проверьте синтаксис команды
  yc operation wait <идентификатор_операции>
  Убедитесь, что ВМ созданы. Для этого выведите их список.
  yc compute instance list
  По умолчанию список выдается в виде таблицы:
  +----------------------+---------------+---------------+---------+---------------+--------------+
  |          ID          |     NAME      |    ZONE ID    | STATUS  |  EXTERNAL IP  | INTERNAL IP  |
  +----------------------+---------------+---------------+---------+---------------+--------------+
  | ef34r4fs8dsva3qtsivs | my-instance-3 | ru-central1-c | RUNNING | 51.250.44.130 | 192.168.3.34 |
  | epdj7u79isrolup3vfo8 | my-instance-2 | ru-central1-b | RUNNING | 158.160.6.249 | 192.168.2.28 |
  | fhmepiaciq5l9slqid3k | my-instance-1 | ru-central1-a | RUNNING | 62.84.126.39  | 192.168.1.30 |
  +----------------------+---------------+---------------+---------+---------------+--------------+
  Список можно вывести в формате YAML или JSON (эта возможность пригодится вам на следующих уроках):
  yc compute instance list --format json
  Список в формате JSON содержит больше информации, чем таблица.
  Посмотрите результат
  [
    {
      "id": "ef3rutmaas72bsujcja7",
      "folder_id": "b1gfdbij3ijgopgqv9m9",
      "created_at": "2021-06-21T12:41:10Z",
      "name": "my-instance-3",
      "zone_id": "ru-central1-c",
      "platform_id": "standard-v2",
      "resources": {
        "memory": "4294967296",
        "cores": "2",
        "core_fraction": "100"
      },
      "status": "RUNNING",
      "metadata_options": {
        "gce_http_endpoint": "ENABLED",
        "aws_v1_http_endpoint": "ENABLED",
        "gce_http_token": "ENABLED",
        "aws_v1_http_token": "ENABLED"
      },
      "boot_disk": {
        "mode": "READ_WRITE",
        "device_name": "ef3v2lor1u4pfn3ce1al",
        "auto_delete": true,
        "disk_id": "ef3v2lor1u4pfn3ce1al"
      },
      "network_interfaces": [
        {
          "index": "0",
          "mac_address": "d0:0d:1b:f7:6c:a5",
          "subnet_id": "b0c4h992tbuodl5hudpu",
          "primary_v4_address": {
            "address": "10.128.0.32",
            "one_to_one_nat": {
              "address": "178.154.212.5",
              "ip_version": "IPV4"
            }
          }
        }
      ],
      "fqdn": "my-instance-3.ru-central1.internal",
      "scheduling_policy": {},
      "network_settings": {
        "type": "STANDARD"
      },
      "placement_policy": {}
    },
    {
      "id": "epd928ffks7m8ssc4i3k",
      "folder_id": "b1gfdbij3ijgopgqv9m9",
      "created_at": "2021-06-21T12:40:35Z",
      "name": "my-instance-2",
      "zone_id": "ru-central1-b",
      "platform_id": "standard-v2",
      "resources": {
        "memory": "4294967296",
        "cores": "2",
        "core_fraction": "100"
      },
      "status": "RUNNING",
      "metadata_options": {
        "gce_http_endpoint": "ENABLED",
        "aws_v1_http_endpoint": "ENABLED",
        "gce_http_token": "ENABLED",
        "aws_v1_http_token": "ENABLED"
      },
      "boot_disk": {
        "mode": "READ_WRITE",
        "device_name": "epddf7t0ljn9i1jp2pbs",
        "auto_delete": true,
        "disk_id": "epddf7t0ljn9i1jp2pbs"
      },
      "network_interfaces": [
        {
          "index": "0",
          "mac_address": "d0:0d:91:21:ef:a7",
          "subnet_id": "e2l1fgq2fbhnp6b929t7",
          "primary_v4_address": {
            "address": "10.129.0.9",
            "one_to_one_nat": {
              "address": "84.201.176.134",
              "ip_version": "IPV4"
            }
          }
        }
      ],
      "fqdn": "my-instance-2.ru-central1.internal",
      "scheduling_policy": {},
      "network_settings": {
        "type": "STANDARD"
      },
      "placement_policy": {}
    },
    {
      "id": "fhm1op9id0dc6bubfags",
      "folder_id": "b1gfdbij3ijgopgqv9m9",
      "created_at": "2021-06-21T12:39:43Z",
      "name": "my-instance-1",
      "zone_id": "ru-central1-a",
      "platform_id": "standard-v2",
      "resources": {
        "memory": "4294967296",
        "cores": "2",
        "core_fraction": "100"
      },
      "status": "RUNNING",
      "metadata_options": {
        "gce_http_endpoint": "ENABLED",
        "aws_v1_http_endpoint": "ENABLED",
        "gce_http_token": "ENABLED",
        "aws_v1_http_token": "ENABLED"
      },
      "boot_disk": {
        "mode": "READ_WRITE",
        "device_name": "fhmms7r7ia4uteikv1to",
        "auto_delete": true,
        "disk_id": "fhmms7r7ia4uteikv1to"
      },
      "network_interfaces": [
        {
          "index": "0",
          "mac_address": "d0:0d:1c:65:32:68",
          "subnet_id": "e9bcvlanhbum9ggdvkh2",
          "primary_v4_address": {
            "address": "10.130.0.6",
            "one_to_one_nat": {
              "address": "178.154.225.167",
              "ip_version": "IPV4"
            }
          }
        }
      ],
      "fqdn": "my-instance-1.ru-central1.internal",
      "scheduling_policy": {},
      "network_settings": {
        "type": "STANDARD"
      },
      "placement_policy": {}
    }
  ]
  Чтобы избежать ненужных расходов, удалите три созданные ВМ (в следующих практических работах они не понадобятся).
  yc compute instance delete my-instance-1 my-instance-2 my-instance-3
  Decision:
  $ yc vpc network create --name my-network
  $ yc vpc subnet create \
    --name my-subnet-1 \
    --zone ru-central1-a \
    --range 192.168.1.0/24 \
    --network-name my-network
  $ yc vpc subnet create \
    --name my-subnet-2 \
    --zone ru-central1-b \
    --range 192.168.2.0/24 \
    --network-name my-network
  $ yc vpc subnet create \
    --name my-subnet-3 \
    --zone ru-central1-c \
    --range 192.168.3.0/24 \
    --network-name my-network
  $ yc compute instance create \
    --name my-instance-1 \
    --hostname my-instance-1 \
    --zone ru-central1-a \
    --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \
    --image-folder-id standard-images \
    --memory 4 --cores 2 --core-fraction 100 \
    --network-interface subnet-name=my-subnet-1,nat-ip-version=ipv4 \
    --async
  $ yc compute instance create \
    --name my-instance-2 \
    --hostname my-instance-2 \
    --zone ru-central1-b \
    --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \
    --image-folder-id standard-images \
    --memory 4 --cores 2 --core-fraction 100 \
    --network-interface subnet-name=my-subnet-2,nat-ip-version=ipv4 \
    --async
  $ yc compute instance create \
    --name my-instance-3 \
    --hostname my-instance-3 \
    --zone ru-central1-c \
    --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \
    --image-folder-id standard-images \
    --memory 4 --cores 2 --core-fraction 100 \
    --network-interface subnet-name=my-subnet-3,nat-ip-version=ipv4 \
    --async
  $ yc operation get ef3vcn9383adr1anudcg
  $ yc operation get epdk80vb9jjjc3190s6a
  $ yc operation get fhm0u5fo2bq3cl2gagpr
  $ yc operation wait fhm0u5fo2bq3cl2gagpr
  $ yc compute instance list
  $ yc compute instance list --format json
  $ yc compute instance delete my-instance-1 my-instance-2 my-instance-3
  Task:
  Использование файлов спецификаций
  В этой практической работе вы создадите, обновите и удалите группу ВМ.
  Вы уже убедились, что создать даже одну ВМ через yc непросто: нужно установить много разных параметров. Создание группы ВМ требует ещё больше параметров.
  Чтобы не указывать их все в командной строке, конфигурацию описывают в файле, который используют при создании группы. Такой файл называется спецификацией.
  Использование спецификаций — это первый шаг в освоении подхода Infrastructure as Code (IaC), который мы будем применять на следующих уроках.
  Спецификации пишутся в разных форматах. Для группы ВМ используется язык YAML. Если вы не знакомы с ним — ничего страшного.
  Decision:
  Часть 1. Создание Instance Group. Для разворачивания группы ВМ потребуется сеть. Если сети ещё нет, создайте её. Посмотрите информацию об имеющихся сетях.
  yc vpc network list
  Сохраните идентификатор сети, он понадобиться нам в дальнейшем.
  По умолчанию все операции в Instance Groups выполняются от имени сервисного аккаунта c ролью editor на каталог. Если сервисного аккаунта нет, то тоже создайте его и назначьте эту роль.
  Посмотрите информацию об имеющихся сервисных аккаунтах.
  yc iam service-account list
  Сохраните идентификатор сервисного аккаунта, он понадобится нам в дальнейшем.
  Для создания группы необходимо подготовить её спецификацию. Создайте в любом текстовом редакторе файл с расширением yaml, например specification.yaml.
  Обратите внимание: в формате YAML важны отступы слева. Даже если текст правильный, но отступы не соблюдены, при выполнении спецификации возникнут ошибки.
  Сначала внесите информацию о группе. Пусть группа называется my-group. Укажите идентификатор сервисного аккаунта, от имени которого будете работать (см. шаг 2).
  Идентификаторы ресурсов уникальны. Копируя команды из текста урока, не забывайте подставлять свои идентификаторы.
  name: my-group
  service_account_id: <идентификатор_сервисного_аккаунта>
  Наша группа будет содержать три одинаковые ВМ. Машины создадим из публичного образа Ubuntu 18.04 LTS (возьмём не последнюю версию, чтобы потренироваться обновлять ВМ). Узнайте идентификатор образа с помощью команды:
  yc compute image list --folder-id standard-images
  В столбце FAMILY найдите ubuntu-1804-lts, в столбце ID будет указан нужный идентификатор.
  Опишите в спецификации ВМ. Это раздел instance_template.
  Пусть каждая машина использует платформу Intel Broadwell (посмотрите поддерживаемые платформы в документации Yandex Compute Cloud),  имеет 2 Гб оперативной памяти и два процессорных ядра.
  instance_template:
      platform_id: standard-v1
      resources_spec:
          memory: 2g
          cores: 2
  Добавьте описание загрузочного диска. Он будет использоваться на чтение и запись (режим READ_WRITE). Укажите идентификатор образа, который получили на шаге 5. Выделите сетевой HDD объёмом 32 Гб.
      boot_disk_spec:
          mode: READ_WRITE
          disk_spec:
              image_id: <идентификатор_образа>
              type_id: network-hdd
              size: 32g
  Теперь опишите сеть: идентификатор сети из каталога по умолчанию (см. шаг 1). Задайте публичный IP-адрес, чтобы к ВМ можно было обращаться извне.
      network_interface_specs:
          - network_id: <идентификатор_сети>
            primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }}
  В политике планирования укажите, что машина не прерываемая.
      scheduling_policy:
          preemptible: false
  В политике развертывания (раздел deploy policy) укажите, что в каждый момент времени может быть неработоспособной только одна машина, не больше. Запретите увеличивать число ВМ, т. е. создавать больше трех машин одновременно. Мы чуть подробнее разберём эти настройки, когда будем обновлять ВМ в группе.
      deploy_policy:
          max_unavailable: 1
          max_expansion: 0
  Мы создаем группу фиксированного размера из трёх ВМ. Укажите это в политике масштабирования (раздел scale_policy):
      scale_policy:
          fixed_scale:
          size: 3
  Наконец, в политике распределения машин по зонам (раздел allocation_policy) укажите, что будет использоваться зона ru-central1-a. Мы делаем это для простоты. Лучше распределять ВМ группы по разным зонам доступности — это позволит пережить краткие сбои или выход зоны из строя.
      allocation_policy:
          zones:
              - zone_id: ru-central1-a
  Для балансировщика нагрузки (раздел load_balancer_spec) укажите целевую группу, к которой он будет привязан (это мы рассмотрим чуть ниже).
      load_balancer_spec:
          target_group_spec:
          name: my-target-group
  Нашей спецификации уже достаточно, чтобы создать группу ВМ. Но на эти машины не будет установлено никакого ПО, только операционная система из публичного образа. Если не менять конфигурацию, то после создания ВМ вам придётся устанавливать программы вручную.
  Чтобы сэкономить время и сократить число ошибок, давайте максимально автоматизируем создание ВМ, включая установку ПО. Для этого добавим в конфигурацию машины секцию, где будут вызываться команды установки программ. В этой же секции можно описать создание пользователей, но мы этого делать не будем, так как заходить на ВМ не планируем.
  Установим на машины веб-сервер NGINX и на веб-странице index.nginx-debian.html, которая создается по умолчанию и выводит приветственное сообщение «Welcome to nginx», заменим слово nginx идентификатором активной ВМ и версией ОС. Поскольку мы подключим балансировщик нагрузки, идентификатор активной ВМ будет различаться для разных пользователей. Это и позволит нам убедиться в том, что балансировщик работает.
  Для установки ПО используйте cloud-init — пакет, выполняющий команды на ВМ при первом запуске. Вы узнали о нём из курса о ВМ. Команды опишите в блоке конфигурации #cloud-config. Примеры команд смотрите в документации cloud-init.
  Содержимое #cloud-config описывается в разделе instance_template в секции metadata:
      metadata:
        user-data: |-
          #cloud-config
            package_update: true
            runcmd:
              - [apt-get, install, -y, nginx ]
              - [/bin/bash, -c, 'source /etc/lsb-release; sed -i "s/Welcome to nginx/It is $(hostname) on $DISTRIB_DESCRIPTION/" /var/www/html/index.nginx-debian.html']
  Спецификация готова. Вот ее полный текст. Помните, что в формате YAML важно соблюдать отступы слева.
  name: my-group
  service_account_id: ajeu495h1s9tn1rorulb
  instance_template:
      platform_id: standard-v1
      resources_spec:
          memory: 2g
          cores: 2
      boot_disk_spec:
          mode: READ_WRITE
          disk_spec:
              image_id: fd8fosbegvnhj5haiuoq
              type_id: network-hdd
              size: 32g
      network_interface_specs:
          - network_id: enpnr4onfs6ihtoao32u
            primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }}
      scheduling_policy:
          preemptible: false
      metadata:
        user-data: |-
          #cloud-config
            package_update: true
            runcmd:
              - [ apt-get, install, -y, nginx ]
              - [/bin/bash, -c, 'source /etc/lsb-release; sed -i "s/Welcome to nginx/It is $(hostname) on $DISTRIB_DESCRIPTION/" /var/www/html/index.nginx-debian.html']
  deploy_policy:
      max_unavailable: 1
      max_expansion: 0
  scale_policy:
      fixed_scale:
          size: 3
  allocation_policy:
      zones:
          - zone_id: ru-central1-a
  load_balancer_spec:
      target_group_spec:
          name: my-target-group
  Теперь создайте группу ВМ по подготовленной спецификации. Уточните синтаксис команды сами:
  yc compute instance-group --help
  Проверить синтаксис команды
  yc compute instance-group create --file <путь_к_файлу_specification.yaml>  
  Для тренировки можете вызвать эту команду в асинхронном режиме, а затем проверить её статус и дождаться завершения.
  Убедитесь, что группа создана, в веб-консоли или выведя список групп с помощью yc.
  yc compute instance-group list
  В списке вы должны увидеть свою группу машин my-group:
  +----------------------+------------+------+
  |          ID          |    NAME    | SIZE |
  +----------------------+------------+------+
  | amc65sbgfqeqf00m02sc | my-group   |    3 |
  +----------------------+------------+------+
  Часть 2. Балансировщик
  Создайте балансировщик my-load-balancer. Посмотрите, какие параметры должны быть у соответствующей команды:
  yc load-balancer network-load-balancer create --help
  В выводе справки обратите внимание, что при создании балансировщика можно сразу создать и обработчик входящего трафика (параметр --listener).
  Формат параметра --listener достаточно хитрый: в нём можно указать сразу несколько подпараметров через запятую:
  ...
  --listener name=my-listener,external-ip-version=ipv4,port=80
  ...
  Помимо имени обработчика, здесь указывается версия IP-протокола и порт, на котором балансировщик будет принимать трафик.
  Проверить синтаксис команды
  yc load-balancer network-load-balancer create \
    --region-id ru-central1 \
    --name my-load-balancer \
    --listener name=my-listener,external-ip-version=ipv4,port=80
  Затем подключите к балансировщику целевую группу (команда attach-target-group). Вам понадобится идентификатор целевой группы. Чтобы узнать его, запросите с помощью yc список доступных целевых групп и выберите ту, которую вы указали в спецификации specification.yaml.
  Проверить синтаксис команды
  yc load-balancer target-group list
  Целевая группа также подключается с помощью нескольких подпараметров, которые соответствуют настройкам в консоли управления (их вы изучали на первом курсе). Для целевой группы укажите такие параметры:
      target-group-id — идентификатор группы;
      healthcheck-name, healthcheck-interval, healthcheck-timeout, healthcheck-unhealthythreshold, healthcheck-healthythreshold, healthcheck-http-port — параметры проверки состояния (см. документацию). Эти параметры аналогичны тем, что задаются в консоли управления при создании балансировщика. Вы изучали их в первом курсе.
  Укажите 80-й порт, на котором запущен NGINX.
  Проверить синтаксис команды
  yc load-balancer network-load-balancer attach-target-group my-load-balancer \
    --target-group target-group-id=<идентификатор целевой группы>,healthcheck-name=test-health-check,healthcheck-interval=2s,healthcheck-timeout=1s,healthcheck-unhealthythreshold=2,healthcheck-healthythreshold=2,healthcheck-http-port=80
  Можно не выполнять две команды (создание балансировщика и подключение целевой группы) по очереди, а одной командой create создать балансировщик с привязанной целевой группой.
  Убедитесь, что балансировщик создан, а целевая группа подключена через консоль управления или с помощью yc.
  Часть 3. Доступ к машинам группы
  Проверьте состояние машин группы. Для этого запросите список машин и дождитесь статуса HEALTHY.
  yc load-balancer network-load-balancer target-states my-load-balancer \
      --target-group-id <идентификатор_целевой_группы>
  Теперь откройте в браузере страницу балансировщика. IP-адрес балансировщика вы можете узнать с помощью консоли управления или yc.
  На странице вы увидите приветственное сообщение и в нём идентификатор одной из машин.
  Часть 4. Обновление Instance Group
  При создании на ВМ группы была установлена ОС Ubuntu 18.04 LTS. Теперь обновите её до Ubuntu 20.04 LTS (ubuntu-2004-lts в столбце FAMILY). Ещё раз посмотрите список доступных образов (см. часть 1) и в файле спецификации specification.yaml измените параметр image_id.
  ...
  boot_disk_spec:
    mode: READ_WRITE
    disk_spec:
        image_id: <идентификатор_образа>
        type_id: network-hdd
        size: 32g
  ...
  Теперь запустите обновление группы с изменённым файлом спецификации.
  Проверить синтаксис команды
  yc compute instance-group update \
    --name my-group \
    --file <путь_к_файлу_specification.yaml>
  Группа будет обновляться постепенно: когда одна машина из группы удаляется, ей на замену создаётся новая. Общее число машин в группе не увеличится. Именно такую политику обновления мы задали в файле спецификации (см. часть 1):
  ...
  deploy_policy:
      max_unavailable: 1
      max_expansion: 0
  ...
  Есть и другой режим обновления: сначала в группу добавляется ВМ с новой конфигурацией, а затем отключается старая машина. Это повторяется, пока не обновятся все машины. Такому режиму соответствовала бы другая конфигурация:
  ...
  deploy_policy:
      max_unavailable: 0
      max_expansion: 1
  ...
  Убедитесь, что машины обновились. На приветственной странице должна выводиться новая версия ОС.
  Часть 5. Удаление машины из группы
  На приветственной странице балансировщика посмотрите имя активной машины и попробуйте удалить ее. Убедитесь, что приветственная страница остаётся доступна всё время: балансировщик переключит трафик на другую машину группы. А Yandex Cloud тем временем пересоздаст удалённую машину.
  Проверить синтаксис команды
  yc compute instance delete <имя_ВМ>
  Часть 6. Удаление Instance Group
  Теперь удалите группу и балансировщик командами yc.
  Проверить синтаксис команд
  yc compute instance-group delete --name my-group
  yc load-balancer network-load-balancer delete --name my-load-balancer
  Кстати, ключевой параметр --name можно и не писать. Достаточно указать имя группы или балансировщика.
  Убедитесь, что группы и балансировщика больше нет, через консоль управления или с помощью yc.
  Decision:
  $ yc vpc network list
  $ yc iam service-account list
  $ vim specification.yaml
  $ cat specification.yaml
  name: my-group
  service_account_id: ajeq7kga9ms7bhup4gbe
  $ yc compute image list --folder-id standard-images
  $ vim specification.yaml
  $ cat specification.yaml
  name: my-group
  service_account_id: ajeq7kga9ms7bhup4gbe
  instance_template:
      platform_id: standard-v1
      resources_spec:
          memory: 2g
          cores: 2
      boot_disk_spec:
          mode: READ_WRITE
          disk_spec:
              image_id: fd8k6joqhuk8ts8eb1ao
              type_id: network-hdd
              size: 32g
      network_interface_specs:
          - network_id: enpboucd6803lg6jspnh
            primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }}
      scheduling_policy:
          preemptible: false
      metadata:
        user-data: |-
          #cloud-config
            package_update: true
            runcmd:
              - [ apt-get, install, -y, nginx ]
              - [/bin/bash, -c, 'source /etc/lsb-release; sed -i "s/Welcome to nginx/It is $(hostname) on $DISTRIB_DESCRIPTION/" /var/www/html/index.nginx-debian.html']
  deploy_policy:
      max_unavailable: 1
      max_expansion: 0
  scale_policy:
      fixed_scale:
          size: 3
  allocation_policy:
      zones:
          - zone_id: ru-central1-a
  load_balancer_spec:
      target_group_spec:
          name: my-target-group
  $ yc compute instance-group --help
  $ yc compute instance-group create --file /YOUR-DIR/specification.yaml
  $ yc compute instance-group list
  $ yc load-balancer network-load-balancer create --help
  $ yc load-balancer network-load-balancer create \
    --region-id ru-central1 \
    --name my-load-balancer \
    --listener name=my-listener,external-ip-version=ipv4,port=80
  $ yc load-balancer target-group list
  $ yc load-balancer network-load-balancer attach-target-group my-load-balancer \
  --target-group target-group-id=enp3edjdaoot0v64qth0,healthcheck-name=test-health-check,healthcheck-interval=2s,healthcheck-timeout=1s,healthcheck-unhealthythreshold=2,healthcheck-healthythreshold=2,healthcheck-http-port=80
  $ yc load-balancer network-load-balancer target-states my-load-balancer \
  --target-group-id enp3edjdaoot0v64qth0
  $ vim specification.yaml
  $ cat specification.yaml
  ...
  boot_disk_spec:
    mode: READ_WRITE
    disk_spec:
        image_id: <идентификатор_образа>
        type_id: network-hdd
        size: 32g
  ...
  $ yc compute instance-group update \
    --name my-group \
    --file /YOUR-DIR/specification.yaml
  $ yc compute instance-group delete --name my-group
  $ yc load-balancer network-load-balancer delete --name my-load-balancer
  Task:
  Создаём образ виртуальной машины.
  В этой практической работе вы установите Packer, подготовите с его помощью образ, а затем создадите из образа виртуальную машину.
  Decision:
  Установите Packer, если ещё не сделали это на предыдущем уроке. Он поддерживает все популярные операционные системы — Windows, macOS, Linux и FreeBSD.
  Скачать дистрибутив Packer для вашей ОС также можно с зеркала Yandex Cloud.
  Подготовьте файл в формате HCL со спецификацией образа, например my-ubuntu-nginx.pkr.hcl.
  При создании файла опирайтесь на документацию Packer.
  В качестве примера можете взять спецификацию из предыдущего урока:
  source "yandex" "ubuntu-nginx" {
    token               = "<OAuth-токен>"
    folder_id           = "<идентификатор_каталога>"
    source_image_family = "ubuntu-2004-lts"
    ssh_username        = "ubuntu"
    use_ipv4_nat        = "true"
    image_description   = "my custom ubuntu with nginx"
    image_family        = "ubuntu-2004-lts"
    image_name          = "my-ubuntu-nginx"
    subnet_id           = "<идентификатор_подсети>"
    disk_type           = "network-ssd"
    zone                = "ru-central1-a"
  }
  build {
    sources = ["source.yandex.ubuntu-nginx"]
    provisioner "shell" {
      inline = ["sudo apt-get update -y",
            "sudo apt-get install -y nginx",
            "sudo systemctl enable nginx.service"]
    }
  }
  Не забудьте подставить в спецификацию идентификаторы своего каталога и подсети (подсеть должна быть в той же зоне доступности, которая указана в параметре zone). Также укажите свой OAuth-токен (или воспользуйтесь переменной окружения YC_TOKEN при сборке образа).
  Теперь создайте образ ВМ на основе файла спецификации:
  packer build <путь_к_файлу_my-ubuntu-nginx.pkr.hcl>
  После того как команда отработает, убедитесь, что образ появился в каталоге. Для этого в консоли управления перейдите в сервис Compute Cloud. Найдите образ на вкладке Образы.
  Перейдите на вкладку Виртуальные машины и начните создавать ВМ.
  Раньше для создания загрузочного диска вы выбирали один из публичных образов, например Ubuntu 20.04. Теперь вместо этого переключитесь на вкладку Пользовательские. Нажмите кнопку Выбрать и в открывшемся окне переключитесь на вкладку Образ.
  Выберите созданный образ и нажмите Применить.
  Из образа создастся загрузочный диск.
  Завершите создание ВМ.
  Проверьте ВМ: введите её IP-адрес в адресную строку браузера. Убедитесь, что веб-сервер работает.
  Удалите ВМ: на следующих уроках она не понадобится. А вот образ удалять не стоит.
  Decision:
  $ sudo apt-get install packer
  $ vim my-ubuntu-nginx.pkr.hcl
  $ cat my-ubuntu-nginx.pkr.hcl
  source "yandex" "ubuntu-nginx" {
    token               = "YOUR-KEY"
    folder_id           = "YOUR-ID"
    source_image_family = "ubuntu-2004-lts"
    ssh_username        = "ubuntu"
    use_ipv4_nat        = "true"
    image_description   = "my custom ubuntu with nginx"
    image_family        = "ubuntu-2004-lts"
    image_name          = "my-ubuntu-nginx"
    subnet_id           = "enpboucd6803lg6jspnh"
    disk_type           = "network-ssd"
    zone                = "ru-central1-a"
  }
  build {
    sources = ["source.yandex.ubuntu-nginx"]
    provisioner "shell" {
      inline = ["sudo apt-get update -y",
            "sudo apt-get install -y nginx",
            "sudo systemctl enable nginx.service"]
      }
  }
  $ packer build /YOUR-DIR/my-ubuntu-nginx.pkr.hcl
  Task:
  Создаём виртуальную машину из образа и базу данных.
  В этой практической работе вы установите Terraform и подготовите спецификацию, с помощью которой создадите виртуальную машину, а затем управляемую базу данных.
  Decision:
  Установите Terraform. Дистрибутив для вашей платформы можно скачать из зеркала. После загрузки добавьте путь к папке, в которой находится исполняемый файл, в переменную PATH.
  Настройте провайдер. Если раньше у вас был настроен провайдер из реестра Hashicorp, сохраните его настройки:
  mv ~/.terraformrc ~/.terraformrc.old
  Укажите источник, из которого будет устанавливаться провайдер.
  Откройте файл конфигурации Terraform CLI:
  nano ~/.terraformrc
  Добавьте в него следующий блок:
  provider_installation {
    network_mirror {
      url = "https://terraform-mirror.yandexcloud.net/"
      include = ["registry.terraform.io/*/*"]
    }
    direct {
      exclude = ["registry.terraform.io/*/*"]
    }
  }
  В начале конфигурационного файла .tf добавьте следующие блоки:
  terraform {
    required_providers {
      yandex = {
        source = "yandex-cloud/yandex"
      }
    }
    required_version = ">= 0.13"
  }
  provider "yandex" {
    zone = "<зона доступности по умолчанию>"
  }
  Где: source — глобальный адрес источника провайдера. required_version — минимальная версия Terraform, с которой совместим провайдер. provider — название провайдера. zone — зона доступности, в которой по умолчанию будут создаваться все облачные ресурсы.
  Выполните команду terraform init в папке с конфигурационным файлом .tf. Эта команда инициализирует провайдеров, указанных в конфигурационных файлах, и позволяет работать с ресурсами и источниками данных провайдера.
  Если провайдер не установился, создайте обращение в поддержку с именем и версией провайдера.
  Если вы использовали файл .terraform.lock.hcl, то перед инициализацией выполните команду terraform providers lock, указав адрес зеркала, откуда будет загружаться провайдер, и платформы, на которых будет использоваться конфигурация:
  terraform providers lock -net-mirror=https://terraform-mirror.yandexcloud.net -platform=linux_amd64 -platform=darwin_arm64 yandex-cloud/yandex
  Если вы использовали модули, то сначала выполните terraform init, затем удалите lock-файл, а затем выполните команду terraform providers lock.
  Создайте файл спецификации my-config.tf и укажите в нём Yandex Cloud в качестве провайдера.
  terraform {
    required_providers {
      yandex = {
        source = "yandex-cloud/yandex"
      }
    }
  }
  provider "yandex" {
    token  =  "<OAuth-токен>"
    cloud_id  = "<идентификатор_облака>"
    folder_id = "<идентификатор_каталога>"
    zone      = "<зона_доступности_по_умолчанию>"
  }
  Далее мы будем считать, что в качестве зоны доступности по умолчанию выбрана ru-central1-a.
  Добавьте в файл блок, описывающий создание ВМ. Его сложно написать с нуля, поэтому опирайтесь на пример из документации. Чтобы вам было проще опознать в консоли управления объекты, созданные по этой спецификации, указывайте уникальные имена для ВМ, сети и подсети, а не оставляйте имена по умолчанию (default).
  Для создания ВМ используйте образ, созданный с помощью Packer в предыдущей практической работе.
  Можно использовать переменные в спецификации Terraform и передавать в них разные значения при запуске команд. Например, если сделать переменную для идентификатора образа image-id, тогда с помощью одного и того же файла спецификации вы сможете создавать ВМ с разным наполнением.
  Переменные Terraform хранятся в файлах с расширением .tfvars. Создайте файл my-variables.tfvars и укажите в нём идентификатор своего образа Packer (узнайте идентификатор с помощью команды yc compute image list):
  image-id = "<идентификатор_образа>"
  В файле спецификации my-config.tf объявите эту переменную (ключевое слово variable). Тогда в секции, где описываются настройки ВМ, вы сможете обратиться к переменной как var.image-id:
  ...
  variable "image-id" {
      type = string
  }
  resource "yandex_compute_instance" "vm-1" {
  ...   
      boot_disk {
          initialize_params {
              image_id = var.image-id
          }
      }
  ...
  Скорректируйте описание для сети и подсети.
  Для сети достаточно указать имя:
    resource "yandex_vpc_network" "network-1" {
        name = "from-terraform-network"
    }
  Для подсети укажите зону доступности и сеть, а также внутренние IP-адреса, уникальные в рамках сети. Используйте адреса из адресного пространства 10.0.0.0/16.
    resource "yandex_vpc_subnet" "subnet-1" {
        name           = "from-terraform-subnet"
        zone           = "ru-central1-a"
        network_id     = "${yandex_vpc_network.network-1.id}"
        v4_cidr_blocks = ["10.2.0.0/16"]
    }
  Проверьте синтаксис спецификации:
  variable "image-id" {
    type = string
  }
  resource "yandex_compute_instance" "vm-1" {
    name = "from-terraform-vm"
    platform_id = "standard-v1"
    zone = "ru-central1-a"
    resources {
      cores  = 2
      memory = 2
    }
    boot_disk {
      initialize_params {
        image_id = var.image-id
      }
    }
    network_interface {
      subnet_id = yandex_vpc_subnet.subnet-1.id
      nat       = true
    }
    metadata = {
      ssh-keys = "ubuntu:${file("~/.ssh/id_rsa.pub")}"
    }
  }
  resource "yandex_vpc_network" "network-1" {
    name = "from-terraform-network"
  }
  resource "yandex_vpc_subnet" "subnet-1" {
    name           = "from-terraform-subnet"
    zone           = "ru-central1-a"
    network_id     = "${yandex_vpc_network.network-1.id}"
    v4_cidr_blocks = ["10.2.0.0/16"]
  }
  output "internal_ip_address_vm_1" {
    value = yandex_compute_instance.vm-1.network_interface.0.ip_address
  }
  output "external_ip_address_vm_1" {
    value = yandex_compute_instance.vm-1.network_interface.0.nat_ip_address
  }
  Теперь попробуйте применить спецификацию. Перейдите в папку с файлом спецификации и выполните инициализацию.
  terraform init
  Если всё сделано верно, Terraform покажет сообщение:
  ...
  Terraform has been successfully initialized!
  ...
  Важно: выполняйте команды Terraform в папке, где находится файл спецификации.
  Проверьте спецификацию с помощью команды terraform plan.
  Terraform использует все файлы .tf из папки, в которой запущена команда. Поэтому название файла спецификации my-config.tf указывать не нужно: его Terraform подхватит и так.
  Если файл с переменными называется стандартно (terraform.tfvars), его тоже можно не указывать при запуске команды. А если название файла нестандартное, то его нужно указывать:
  terraform plan -var-file=my-variables.tfvars
  Terraform выведет план: объекты, которые будут созданы, и т. п.:
  ...
  Terraform will perform the following actions:
  ...
  На самом деле необязательно помещать переменные в файл, их можно просто указывать при запуске команды. Поскольку у вас только одна переменная, это было бы несложно:
  terraform plan -var="image-id=<идентификатор_образа>"
  Создайте в облаке инфраструктуру по описанной вами спецификации. Выполните команду:
  terraform apply -var-file=my-variables.tfvars
  Terraform запросит подтверждение:
  ...
  Do you want to perform these actions?
        Terraform will perform the actions described above.
        Only 'yes' will be accepted to approve.
        Enter a value:
  В ответ введите yes.
  Когда команда будет выполнена, вы увидите сообщение:
    Apply complete! Resources: ... added, 0 changed, 0 destroyed.
    Outputs:
    external_ip_address_vm_1 = "84.201.133.49"
    internal_ip_address_vm_1 = "10.2.0.24"
  В консоли управления убедитесь, что ВМ создана. Откройте в браузере страницу с указанным IP-адресом и проверьте, доступна ли ВМ.
  Как мы говорили на предыдущем уроке, Terraform хранит описание инфраструктуры в стейт-файлах. Посмотрите, как выглядит стейт-файл сейчас:
  terraform state list
  Вы увидите список объектов:
  yandex_compute_instance.vm-1
  yandex_vpc_network.network-1
  yandex_vpc_subnet.subnet-1
  Теперь добавьте в файл спецификации блок, описывающий создание кластера БД PostgreSQL. Подсказки ищите в справочнике ресурсов. Не забудьте заменить в спецификации имя подсети.
  Проверьте синтаксис спецификации:
  resource "yandex_mdb_postgresql_cluster" "postgres-1" {
    name        = "postgres-1"
    environment = "PRESTABLE"
    network_id  = yandex_vpc_network.network-1.id
    config {
      version = 12
      resources {
        resource_preset_id = "s2.micro"
        disk_type_id       = "network-ssd"
        disk_size          = 16
      }
      postgresql_config = {
        max_connections                   = 395
        enable_parallel_hash              = true
        vacuum_cleanup_index_scale_factor = 0.2
        autovacuum_vacuum_scale_factor    = 0.34
        default_transaction_isolation     = "TRANSACTION_ISOLATION_READ_COMMITTED"
        shared_preload_libraries          = "SHARED_PRELOAD_LIBRARIES_AUTO_EXPLAIN,SHARED_PRELOAD_LIBRARIES_PG_HINT_PLAN"
      }
    }
    database {
      name  = "postgres-1"
      owner = "my-name"
    }
    user {
      name       = "my-name"
      password   = "Test1234"
      conn_limit = 50
      permission {
        database_name = "postgres-1"
      }
      settings = {
        default_transaction_isolation = "read committed"
        log_min_duration_statement    = 5000
      }
    }
    host {
      zone      = "ru-central1-a"
      subnet_id = yandex_vpc_subnet.subnet-1.id
    }
  }
  Сохраните файл спецификации.
  Проверьте синтаксис спецификации:
  terraform {
    required_providers {
      yandex = {
        source = "yandex-cloud/yandex"
      }
    }
  }
  provider "yandex" {
    token  =  "<OAuth-токен>"
    cloud_id  = "<идентификатор_облака>"
    folder_id = "<идентификатор_каталога>"
    zone      = "ru-central1-a"
  }
  variable "image-id" {
    type = string
  }
  resource "yandex_compute_instance" "vm-1" {
    name = "from-terraform-vm"
    platform_id = "standard-v1"
    zone = "ru-central1-a"
    resources {
      cores  = 2
      memory = 2
    }
    boot_disk {
      initialize_params {
        image_id = var.image-id
      }
    }
    network_interface {
      subnet_id = yandex_vpc_subnet.subnet-1.id
      nat       = true
    }
    metadata = {
      ssh-keys = "ubuntu:${file("~/.ssh/id_rsa.pub")}"
    }
  }
  resource "yandex_vpc_network" "network-1" {
    name = "from-terraform-network"
  }
  resource "yandex_vpc_subnet" "subnet-1" {
    name           = "from-terraform-subnet"
    zone           = "ru-central1-a"
    network_id     = yandex_vpc_network.network-1.id
    v4_cidr_blocks = ["10.2.0.0/16"]
  }
  resource "yandex_mdb_postgresql_cluster" "postgres-1" {
    name        = "postgres-1"
    environment = "PRESTABLE"
    network_id  = yandex_vpc_network.network-1.id
    config {
      version = 12
      resources {
        resource_preset_id = "s2.micro"
        disk_type_id       = "network-ssd"
        disk_size          = 16
      }
      postgresql_config = {
        max_connections                   = 395
        enable_parallel_hash              = true
        vacuum_cleanup_index_scale_factor = 0.2
        autovacuum_vacuum_scale_factor    = 0.34
        default_transaction_isolation     = "TRANSACTION_ISOLATION_READ_COMMITTED"
        shared_preload_libraries          = "SHARED_PRELOAD_LIBRARIES_AUTO_EXPLAIN,SHARED_PRELOAD_LIBRARIES_PG_HINT_PLAN"
      }
    }
    database {
      name  = "postgres-1"
      owner = "my-name"
    }
    user {
      name       = "my-name"
      password   = "Test1234"
      conn_limit = 50
      permission {
        database_name = "postgres-1"
      }
      settings = {
        default_transaction_isolation = "read committed"
        log_min_duration_statement    = 5000
      }
    }
    host {
      zone      = "ru-central1-a"
      subnet_id = yandex_vpc_subnet.subnet-1.id
    }
  }
  output "internal_ip_address_vm_1" {
    value = yandex_compute_instance.vm-1.network_interface.0.ip_address
  }
  output "external_ip_address_vm_1" {
    value = yandex_compute_instance.vm-1.network_interface.0.nat_ip_address
  }
  Теперь примените обновлённую спецификацию. В папке с файлом спецификации выполните команду terraform plan:
  terraform plan -var-file=my-variables.tfvars
  Если появляются сообщения об ошибках — исправьте ошибки и снова выполните команду.
  Обновите инфраструктуру в соответствии с дополненной спецификацией командой terraform apply:
  terraform apply -var-file=my-variables.tfvars
  Поскольку спецификация теперь включает создание БД, команда может выполняться довольно долго (около 10 минут).
  В консоли управления откройте раздел Managed Service for PostgreSQL и убедитесь, что кластер postgres-1 создан и имеет статус Alive.
  Проверьте, как изменился стейт-файл:
  terraform state list
  В списке появился новый объект:
  yandex_compute_instance.vm-1
  yandex_mdb_postgresql_cluster.postgres-1
  yandex_vpc_network.network-1
  yandex_vpc_subnet.subnet-1
  Удалите инфраструктуру:
  terraform destroy -var-file=my-variables.tfvars
  В конце вы увидите сообщение о выполнении команды:
  ...
  Destroy complete! Resources: 4 destroyed.
  В консоли управления убедитесь, что объекты удалены.
  Decision:
  $ wget https://hashicorp-releases.yandexcloud.net/terraform/1.6.5/terraform_1.6.5_linux_amd64.zip
  $ unzip terraform_1.6.5_linux_amd64.zip
  $ vim my-config.tf
  $ cat my-config.tf
  terraform {
    required_providers {
      yandex = {
        source = "yandex-cloud/yandex"
      }
    }
  }
  provider "yandex" {
    token  =  "YOUR-TOKEN"
    cloud_id  = "YOUR-ID1"
    folder_id = "YOUR-ID2"
    zone      = "ru-central1-a"
  }
  variable "image-id" {
    type = string
  }
  resource "yandex_compute_instance" "vm-1" {
    name = "from-terraform-vm"
    platform_id = "standard-v1"
    zone = "ru-central1-a"
    resources {
      cores  = 2
      memory = 2
    }
    boot_disk {
      initialize_params {
        image_id = var.image-id
      }
    }
    network_interface {
      subnet_id = yandex_vpc_subnet.subnet-1.id
      nat       = true
    }
    metadata = {
      ssh-keys = "ubuntu:${file("/YOUR-DIR/YOUR-KEY.pub")}"
    }
  }
  resource "yandex_vpc_network" "network-1" {
    name = "from-terraform-network"
  }
  resource "yandex_vpc_subnet" "subnet-1" {
    name           = "from-terraform-subnet"
    zone           = "ru-central1-a"
    network_id     = "${yandex_vpc_network.network-1.id}"
    v4_cidr_blocks = ["10.2.0.0/16"]
  }
  output "internal_ip_address_vm_1" {
    value = yandex_compute_instance.vm-1.network_interface.0.ip_address
  }
  output "external_ip_address_vm_1" {
    value = yandex_compute_instance.vm-1.network_interface.0.nat_ip_address
  }
  $ vim my-variables.tfvars
  $ cat my-variables.tfvars
  image-id = "YOUR-ID3"
  $ terraform init
  $ terraform plan -var-file=my-variables.tfvars
  $ terraform apply -var-file=my-variables.tfvars
  $ terraform state list
  $ vim my-config.tf
  $ cat my-config.tf
  terraform {
    required_providers {
      yandex = {
        source = "yandex-cloud/yandex"
      }
    }
  }
  provider "yandex" {
    token  =  "YOUR-TOKEN"
    cloud_id  = "YOUR-ID1"
    folder_id = "YOUR-ID2"
    zone      = "ru-central1-a"
  }
  variable "image-id" {
    type = string
  }
  resource "yandex_compute_instance" "vm-1" {
    name = "from-terraform-vm"
    platform_id = "standard-v1"
    zone = "ru-central1-a"
    resources {
      cores  = 2
      memory = 2
    }
    boot_disk {
      initialize_params {
        image_id = var.image-id
      }
    }
    network_interface {
      subnet_id = yandex_vpc_subnet.subnet-1.id
      nat       = true
    }
    metadata = {
      ssh-keys = "ubuntu:${file("/YOUR-DIR/YOUR-KEY.pub")}"
    }
  }
  resource "yandex_vpc_network" "network-1" {
    name = "from-terraform-network"
  }
  resource "yandex_vpc_subnet" "subnet-1" {
    name           = "from-terraform-subnet"
    zone           = "ru-central1-a"
    network_id     = yandex_vpc_network.network-1.id
    v4_cidr_blocks = ["10.2.0.0/16"]
  }
  resource "yandex_mdb_postgresql_cluster" "YOUR-DB" {
    name        = "YOUR-DB"
    environment = "PRESTABLE"
    network_id  = yandex_vpc_network.network-1.id
    config {
      version = 12
      resources {
        resource_preset_id = "s2.micro"
        disk_type_id       = "network-ssd"
        disk_size          = 16
      }
      postgresql_config = {
        max_connections                   = 395
        enable_parallel_hash              = true
        vacuum_cleanup_index_scale_factor = 0.2
        autovacuum_vacuum_scale_factor    = 0.34
        default_transaction_isolation     = "TRANSACTION_ISOLATION_READ_COMMITTED"
        shared_preload_libraries          = "SHARED_PRELOAD_LIBRARIES_AUTO_EXPLAIN,SHARED_PRELOAD_LIBRARIES_PG_HINT_PLAN"
      }
    }
    database {
      name  = "YOUR-DB"
      owner = "YOUR-USERNAME"
    }
    user {
      name       = "YOUR-USERNAME"
      password   = "YOUR-PASSWORD"
      conn_limit = 50
      permission {
        database_name = "YOUR-DB"
      }
      settings = {
        default_transaction_isolation = "read committed"
        log_min_duration_statement    = 5000
      }
    }
    host {
      zone      = "ru-central1-a"
      subnet_id = yandex_vpc_subnet.subnet-1.id
    }
  }
  output "internal_ip_address_vm_1" {
    value = yandex_compute_instance.vm-1.network_interface.0.ip_address
  }
  output "external_ip_address_vm_1" {
    value = yandex_compute_instance.vm-1.network_interface.0.nat_ip_address
  }
  $ terraform plan -var-file=my-variables.tfvars
  $ terraform apply -var-file=my-variables.tfvars
  $ terraform state list
  $ terraform destroy -var-file=my-variables.tfvars
  Task:
  Создание докер-образа и загрузка его в Container Registry
  В этой практической работе вы создадите реестр в Yandex Container Registry, подготовите Docker-образ виртуальной машины и поместите его в реестр,
  а затем создадите машину из этого образа.
  Decision:
  Установите Docker. Создайте реестр в Yandex Container Registry:
  yc container registry create --name my-registry
  Обратите внимание, что в выводе есть уникальный идентификатор (id) реестра. Он пригодится вам для следующих команд.
  id: crpfpd8jhhldiqah91rc
  folder_id: b1gfdbij3ijgopgqv9m9
  name: my-registry
  status: ACTIVE
  created_at: "2021-04-06T00:46:48.150Z"
  Аутентифицируйтесь в Yandex Container Registry с помощью Docker Credential helper. Это нужно для того, чтобы внешняя платформа Docker могла от вашего имени отправить образ в ваш приватный реестр в Yandex Cloud.
  yc container registry configure-docker
  Подготовьте Dockerfile. Можно использовать файл из урока о Docker:
  FROM ubuntu:latest
  RUN apt-get update -y
  RUN apt-get install -y nginx
  ENTRYPOINT ["nginx", "-g", "daemon off;"]
  По умолчанию Docker использует файл с именем Dockerfile и без расширения.
  Перейдите в папку с Dockerfile и соберите образ (не забудьте подставить идентификатор своего реестра):
  docker build . -t cr.yandex/<идентификатор_реестра>/ubuntu-nginx:latest
  Ключ -t позволяет задать образу имя.
  Напоминаем, что в Yandex Container Registry можно загрузить только образы, названные по такому шаблону:
  cr.yandex/<ID реестра>/<имя Docker-образа>:<тег>
  Загрузите Docker-образ в реестр:
  docker push cr.yandex/<идентификатор_реестра>/ubuntu-nginx:latest
  В консоли управления перейдите в реестр и предоставьте всем пользователям право использовать хранящиеся образы. Для этого перейдите на вкладку Права доступа, в правом верхнем углу нажмите кнопку Назначить роли. В открывшемся окне нажмите кнопку Выбрать пользователя, на вкладке Группы выберите All users. Нажмите кнопку Добавить роль и последовательно введите viewer и container-registry.images.puller. Нажмите кнопку Сохранить.
  В консоли управления создайте ВМ с помощью Container Optimized Image.
  При создании машины в разделе Выбор образа загрузочного диска переключитесь на вкладку Container Solution и нажмите Настроить. Выберите из реестра созданный образ, остальные настройки оставьте по умолчанию и нажмите Применить.
  Другие настройки ВМ мы уже разбирали.
  Когда новая ВМ получит статус Running, найдите её внешний IP адрес в консоли управления и убедитесь, что по этому адресу отображается приветственная страница NGINX.
  Обратите внимание! C помощью Docker-образа вы создали и запустили виртуальную машину с предустановленным, нужным вам ПО. При этом вам даже не потребовалось заходить внутрь ВМ и выполнять установку или настройку ПО вручную.
  Decision:
  $ sudo apt update
  $ sudo apt install apt-transport-https ca-certificates curl software-properties-common
  $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
  $ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable"
  $ sudo apt update
  $ apt-cache policy docker-ce
  $ sudo apt install docker-ce
  $ sudo /etc/init.d/docker start
  $ yc container registry create --name my-registry
  $ yc container registry configure-docker
  $ cd docker/
  $ vim Dockerfile
  $ cat Dockerfile
  FROM ubuntu:latest
  RUN apt-get update -y
  RUN apt-get install -y nginx
  ENTRYPOINT ["nginx", "-g", "daemon off;"]
  $ sudo docker build . -t cr.yandex/YOUR-ID/ubuntu-nginx:latest
  $ sudo docker push cr.yandex/YOUR-ID/ubuntu-nginx:latest
  Task:
  Создание кластера. В этой практической работе вы создадите кластер Kubernetes и группу узлов в нём.
  Decision:
  Выберите каталог для кластера.
  Выберите сервис Managed Service for Kubernetes. Нажмите кнопку Создать кластер. Дальше заполним настройки кластера:
  Для Kubernetes необходим сервисный аккаунт для ресурсов и узлов.
  Сервисный аккаунт для ресурсов — это аккаунт, под которым сервису Kubernetes будут выделяться ресурсы в нашем облаке.
  Сервисный аккаунт для узлов необходим уже созданным узлам самого кластера Kubernetes для доступа к другим ресурсам. Например, чтобы получить Docker-образы из Container Registry.
  Этим аккаунтам нужны разные права, и поэтому у них бывают разные роли. В общем случае вы можете использовать один и тот же сервисный аккаунт. Выберите аккаунт, который создали на первом курсе, или заведите новый.
  Ключ шифрования Yandex Key Management Service позволяет защитить конфиденциальную информацию (пароли, OAuth-токены и SSH-ключи) и повысить безопасность. Это необязательно — кластер запустится и без ключа. Для этой практической работы не создавайте его.
  Релизные каналы RAPID, REGULAR и STABLE отличаются процессом обновления и доступными вам версиями Kubernetes.
  RAPID и REGULAR содержат все версии, включая минорные. STABLE — только стабильные версии. RAPID обновляется автоматически, а в REGULAR и STABLE обновление можно отключить. Когда появляется обновление, информация о нём отображается в консоли управления.
  Выберите REGULAR.
  Внимательно выбирайте релизный канал! Изменить его после создания кластера Kubernetes нельзя.
  Конфигурация мастера. Мастер — ведущая нода группы узлов кластера — следит за состоянием Kubernetes и запускает управляющие процессы. Сконфигурируем мастер:
  Выберите версию Kubernetes. Их набор зависит от релизного канала. Версии мастера и других нод могут не совпадать, но это достаточно тонкая настройка, могут возникнуть проблемы совместимости, которые повлияют на работу всего кластера.
  Кластеру может назначаться публичный IP-адрес. Выберите вариант Автоматически. В этом случае IP выбирается из пула свободных IP-адресов. Если вы не используете Cloud Interconnect или VPN для подключения к облаку, то без автоматического назначения IP-адресов вы не сможете подключиться к кластеру: он будет доступен только во внутренней сети вашего облака.
  Тип мастера влияет на отказоустойчивость. Зональный работает только в одной зоне доступности, а региональный — в трёх подсетях в каждой зоне доступности.
  Выберите зональный тип. В будущем для рабочей среды используйте региональные кластеры, а для разработки и тестирования — более дешёвые зональные.
  Выбор типа мастера также влияет на подсети, в которых будет развёрнут кластер. У вас уже есть подсети, созданные по умолчанию для функционирования облака. Выберите их.
  Настройки окна обновлений.
  Режимов обновления четыре: Отключено, В любое время, Ежедневно и В выбранные дни. Региональный мастер во время обновления остаётся доступен, зональный — нет.
  Группа узлов кластера обновляется с выделением дополнительных ресурсов, так как при обновлении создаются узлы с обновлённой конфигурацией. При обновлении поды с контейнерами будут переезжать с одного узла на другой.
  По умолчанию выставлен пункт В любое время. Оставьте его.
  Сетевые настройки кластера. Сетевые политики для кластера Kubernetes необязательны. Эта опция включает сетевой контроллер Calico, который позволяет применять тонкие настройки политик доступа для кластера. Не выбирайте эту опцию.
  Во время работы кластера подам с контейнерами и сервисам самого кластера Kubernetes будут автоматически присваиваться внутренние IP-адреса. Чтобы IP-адреса подов и сервисов Kubernetes не пересеклись с другими адресами в вашем облаке, задайте CIDR (Classless Inter-Domain Routing — бесклассовая междоменная маршрутизация). Оставьте адреса пустыми: они будут назначены автоматически.
  Маска подсети узлов влияет на количество подов, которые могут запускаться. Если адресов не хватит, под не запустится.
  Вы заполнили все настройки, теперь нажмите Создать кластер. Дождитесь, пока статус кластера станет RUNNING, а состояние — HEALTHY. Это может занять около 10 минут.
  Создание группы узлов. Зайдите в созданный кластер, перейдите на вкладку Управление узлами и нажмите Создать группу узлов. Группы узлов — это группы виртуальных машин.
  Введите имя и описание группы, выберите версию Kubernetes. Выберите Автоматический тип масштабирования и количество узлов от 1 до 5. Укажите среду запуска контейнеров — Docker.
  В сетевых настройках задайте автоматический IP-адрес и выберите зону доступности (кластер зональный, поэтому зона доступности только одна). Задайте SSH-ключ, чтобы иметь доступ к виртуальным машинам кластера. Настройки обновления идентичны настройкам мастера.
  Остальные настройки группы, которые мы не упомянули (вычислительные ресурсы, хранилище и т. д.), оставьте по умолчанию.
  Нажмите Создать группу узлов и дождитесь, пока операция выполнится.
  Task:
  Первое приложение в кластере.
  На прошлом уроке вы создали в консоли управления Yandex Cloud кластер Kubernetes и группу узлов в нём.
  Теперь с помощью командной строки вы развернете в кластере приложение — веб-сервер NGINX.
  Decision:
  В консоли управления войдите в созданный кластер Managed Service for Kubernetes и нажмите кнопку Подключиться. В открывшемся окне скопируйте команду для подключения:
  yc managed-kubernetes cluster get-credentials \
    --id <идентификатор_кластера> \
    --external
  Чтобы проверить правильность установки и подключения, посмотрите на конфигурацию:
  kubectl config view
  Ответ получится примерно таким (IP-адрес сервера и название кластера будут отличаться):
  apiVersion: v1
  clusters:
  - cluster:
      certificate-authority-data: DATA+OMITTED
      server: https://178.154.206.242
    name: yc-managed-k8s-cat2oek6hbp7mnhhhr4m
  contexts:
  ...
  Создание манифеста
  Для описания настроек приложения в кластере создадим файл my-nginx.yaml. Такой файл называется манифестом.
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: my-nginx-deployment
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: nginx
    template:
      metadata:
        labels:
          app: nginx
      spec:
        containers:
        - name: nginx
          image: cr.yandex/<идентификатор_реестра>/ubuntu-nginx:latest
  Рассмотрим, из чего он состоит.
  Директива apiVersion определяет, для какой версии Kubernetes написан манифест. От версии к версии обозначение может меняться.
  apiVersion: apps/v1
  Директива kind описывает механизм использования. Она может принимать значения Deployment, Namespace, Service, Pod, LoadBalancer и т. д. Для развёртывания приложения укажите значение Deployment.
  kind: Deployment
  Директива metadata определяет метаданные приложения: имя, метки (labels), аннотации.
  С помощью Меток можно идентифицировать, группировать объекты, выбирать их подмножества. Добавляйте и изменяйте метки при создании объектов или позднее, в любое время.
  Аннотации используют, чтобы добавить собственные метаданные к объектам.
  Укажем имя приложения:
  metadata:
    name: my-nginx-deployment
  В основном блоке spec содержится описание объектов Kubernetes.
  Директива replicas определяет масштабирование. Для первого запуска укажите, что приложению нужен один под. Позже вы посмотрите, как приложения масштабируются, и сможете увеличить число подов.
  Директива selector определяет, какими подами будет управлять контейнер (подробнее о ней можно прочитать в документации). Поды отбираются с помощью метки (label).
  Директива template определяет шаблон пода. Метка в шаблоне должна совпадать с меткой селектора — nginx.
  В шаблоне содержится ещё одна, собственная директива spec. Она задаёт настройки контейнеров, которые будет развёрнуты на поде. Нам нужен один контейнер. Используйте для него образ, созданный ранее с помощью Docker и помещённый в реестр Yandex Container Registry.
  spec:
    matchLabels:
      app: nginx
    replicas: 1
    selector: ~
    template:
      metadata:
        labels:
          app: nginx
      spec:
        containers:
          - name: nginx
            image: "cr.yandex/<идентификатор_реестра>/ubuntu-nginx:latest"
  Выполнение манифеста. Для создания или обновления ресурсов в кластере используется команда apply. Файл манифеста указывается после флага -f.
  kubectl apply -f <путь_к_файлу_my-nginx.yaml>
  Если результат будет успешным, вы увидите сообщение:
  deployment.apps/my-nginx-deployment created
  Чтобы убедиться, что приложение создано, посмотрите список подов:
  kubectl get pods
  Дождитесь статуса Running:
  NAME                                   READY   STATUS    RESTARTS   AGE
  my-nginx-deployment-65b9b678b6-zmfww   1/1     Running   0          5m27s
  Теперь получите более подробную информацию, выполнив ту же команду с флагом -o wide:
  kubectl get pods -o wide
  Вы увидите внутренний IP-адрес, который присвоен поду. Это пригодится, если нужно узнать, где именно развёрнуто приложение.
  Чтобы получить максимально подробную информацию о запущенном приложении, используйте команду describe:
  kubectl describe deployment/my-nginx-deployment
  Масштабирование. Теперь увеличьте количество подов. Вручную это можно сделать двумя способами:
          изменить файл манифеста, указав в директиве replicas нужное число подов, и снова выполнить команду apply;
          если файла манифеста нет под рукой — использовать команду scale:
  kubectl scale --replicas=3 deployment/my-nginx-deployment
  Если всё получится, в выводе команды kubectl get pods вы увидите сообщение:
  NAME                                   READY   STATUS    RESTARTS   AGE
  my-nginx-deployment-65b9b678b6-6whpp   1/1     Running   0          117s
  my-nginx-deployment-65b9b678b6-wtph9   1/1     Running   0          117s
  my-nginx-deployment-65b9b678b6-zmfww   1/1     Running   0          14m
  На следующей практической работе мы посмотрим, как обращаться извне к кластеру Kubernetes и развёрнутому в нём приложению.
  Кластер как код. Как видите, управление кластерами Kubernetes отлично вписывается в концепцию Infrastructure as Code: вы можете описать конфигурацию кластера в текстовом файле — манифесте. Вы также можете разворачивать кластеры Kubernetes с помощью Terraform.
  Decision:
  $ sudo apt-get update && sudo apt-get install -y apt-transport-https
  $ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
  $ echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
  $ sudo apt-get update
  $ sudo apt-get install -y kubectl
  $ yc managed-kubernetes cluster get-credentials \
    --id YOUR-ID1 \
    --external
  $ kubectl config view
  $ vim my-nginx.yaml
  $ cat my-nginx.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: my-nginx-deployment
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: nginx
    template:
      metadata:
        labels:
          app: nginx
      spec:
        containers:
        - name: nginx
          image: cr.yandex/YOUR-ID/ubuntu-nginx:latest
  $ kubectl apply -f my-nginx.yaml
  $ kubectl get pods
  $ kubectl get pods -o wide
  $ kubectl describe deployment/my-nginx-deployment
  $ kubectl scale --replicas=3 deployment/my-nginx-deployment
  $ kubectl get pods
  Task:
  Балансировка нагрузки.
  Большинство веб-приложений созданы, чтобы взаимодействовать через интернет.
  Вы развернули в кластере приложение, но у вас пока нет к нему доступа из интернета. Чтобы исправить эту проблему, воспользуемся сервисом LoadBalancer.
  У созданного пода есть внутренний IP-адрес.
  Помните, мы говорили о том, что в кластере есть собственный сервис DNS? Он работает с внутренними IP-адресами объектов кластера, чтобы те могли взаимодействовать.
  Однако внутренний IP-адрес может меняться, когда ресурсы группы узлов обновляются. Чтобы обращаться к приложению извне, требуется неизменный публичный IP-адрес — это и будет IP-адрес балансировщика.
  Decision:
  Создайте файл-манифест load-balancer.yaml:
  apiVersion: v1
  kind: Service
  metadata:
    name: my-loadbalancer
  spec:
    selector:
        app: nginx
    ports:
    - port: 80
      targetPort: 80
    type: LoadBalancer
  Где: port — порт сетевого балансировщика, на котором будут обслуживаться пользовательские запросы; targetPort — порт контейнера, на котором доступно приложение; selector — метка селектора из шаблона подов в манифесте объекта Deployment.
  Выполните манифест:
  kubectl apply -f <путь_к_файлу_load-balancer.yaml>
  Вы увидите сообщение:
  service/my-loadbalancer created
  В консоли управления откройте раздел Load Balancer. Там должен появиться балансировщик нагрузки с префиксом k8s в имени и уникальным идентификатором кластера Kubernetes.
  Скопируйте IP-адрес балансировщика в адресную строку браузера. Вы увидите приветственную страницу NGINX.
  Если при создании ресурсов вы получаете ошибку failed to ensure cloud loadbalancer: failed to start cloud lb creation: Permission denied, убедитесь, что вашему сервисному аккаунту хватает прав. Подробнее читайте в документации.
  Decision:
  $ vim load-balancer.yaml
  $ cat load-balancer.yaml
  apiVersion: v1
  kind: Service
  metadata:
    name: my-loadbalancer
  spec:
    selector:
        app: nginx
    ports:
    - port: 80
      targetPort: 80
    type: LoadBalancer
  $ kubectl apply -f load-balancer.yaml
  Task:
  Автомасштабирование в Yandex Managed Kubernetes.
  В этой работе вы увидите, как в Kubernetes выполняется горизонтальное автомасштабирование.
  Decision:
  Создайте манифест load-balancer-hpa.yaml.
  Для начала скопируйте в него настройки спецификаций, которые вы составляли на предыдущих уроках: из my-nginx.yaml (в примере ниже это раздел Deployment) и из load-balancer.yaml (раздел Service).
  Поскольку новый балансировщик должен отслеживать отдельную группу контейнеров, используйте для контейнеров другие метки (labels), например nginx-hpa.
  ---
  ### Deployment
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: my-loadbalancer-hpa
    labels:
      app: nginx-hpa
  spec:
    replicas: 1
    selector:
      matchLabels:
            app: nginx-hpa
    template:
      metadata:
                name: nginx-hpa
              labels:
                app: nginx-hpa
      spec:
              containers:
                - name: nginx-hpa
                  image: k8s.gcr.io/hpa-example
  ---
  ### Service
  apiVersion: v1
  kind: Service
  metadata:
    name: my-loadbalancer-hpa
  spec:
    selector:
      app: nginx-hpa
    ports:
      - protocol: TCP
        port: 80
        targetPort: 80
    type: LoadBalancer
  В разделе Deployment смените образ с Yandex Container Registry на k8s.gcr.io/hpa-example — это специальный тестовый образ из публичного репозитория, создающий высокую нагрузку на процессор. Так вам будет удобно отслеживать работу Horizontal Pod Autoscaler.
        ...
        spec:
            containers:
                - name: nginx-hpa
                image: k8s.gcr.io/hpa-example
  Теперь добавьте в шаблон контейнера настройки requests и limits: мы попросим по умолчанию 256 мебибайтов памяти и 500 милли-CPU (половину ядра), а ограничим контейнер 500 мебибайтами и 1 CPU.
      ...
      spec:
      containers:
        - name: nginx-hpa
          image: k8s.gcr.io/hpa-example
          resources:
            requests:
              memory: "256Mi"
              cpu: "500m"
            limits:
              memory: "500Mi"
              cpu: "1"
  Дополните манифест настройками для Horizontal Pod Autoscaler:
  apiVersion: autoscaling/v1
  kind: HorizontalPodAutoscaler
  metadata:
    name: my-hpa
  spec:
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: my-nginx-deployment-hpa
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 20
  В результате должен получиться такой манифест:
  ---
  ### Deployment
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: my-nginx-deployment-hpa
    labels:
      app: nginx-hpa
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: nginx-hpa
    template:
      metadata:
        name: nginx-hpa
        labels:
          app: nginx-hpa
      spec:
        containers:
          - name: nginx-hpa
            image: k8s.gcr.io/hpa-example
            resources:
              requests:
                memory: "256Mi"
                cpu: "500m"
              limits:
                memory: "500Mi"
                cpu: "1"
  ---
  ### Service
  apiVersion: v1
  kind: Service
  metadata:
    name: my-loadbalancer-hpa
  spec:
    selector:
      app: nginx-hpa
    ports:
      - protocol: TCP
        port: 80
        targetPort: 80
    type: LoadBalancer
  ---
  ### HPA
  apiVersion: autoscaling/v1
  kind: HorizontalPodAutoscaler
  metadata:
    name: my-hpa
  spec:
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: my-nginx-deployment-hpa
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 20
  Примените манифест:
  kubectl apply -f <путь_к_load-balancer-hpa.yaml>
  Вы увидите три сообщения:
  deployment.apps/my-nginx-deployment-hpa created
  service/my-loadbalancer-hpa created
  horizontalpodautoscaler.autoscaling/my-hpa created
  В консоли управления перейдите в раздел Network Load Balancer. Дождитесь, пока статус my-nginx-deployment-hpa станет Running, после чего посмотрите IP-адрес балансировщика. Убедитесь, что в браузере этот адрес доступен. В терминале сохраните IP-адрес в переменную. Например, так:
  LOAD_BALANCER_IP=<IP-адрес балансировщика>
  Запустите в отдельном окне отслеживание интересующих вас компонентов кластера Kubernetes:
  while true; do kubectl get pod,svc,hpa,nodes -o wide; sleep 5; done  
  Теперь сымитируйте рабочую нагрузку на приложение. Для этого подойдёт утилита wget (установите её с помощью пакетного менеджера или с сайта).
  while true; do wget -q -O- http://$LOAD_BALANCER_IP; done  
  Вы увидите, что сначала увеличится число подов, а затем добавятся узлы. Число узлов ограничено настройками группы узлов кластера, которые вы задали при создании кластера (в нашем случае максимальное количество узлов — пять).
  Остановите цикл создания нагрузки на приложение (комбинация клавиш Ctrl + C). В окне консоли с отслеживанием компонентов кластера вы увидите, как удаляются узлы и поды без нагрузки.
  Decision:
  $ vim load-balancer-hpa.yaml
  $ cat load-balancer-hpa.yaml
  ### Deployment
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: my-nginx-deployment-hpa
    labels:
      app: nginx-hpa
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: nginx-hpa
    template:
      metadata:
        name: nginx-hpa
        labels:
          app: nginx-hpa
      spec:
        containers:
          - name: nginx-hpa
            image: k8s.gcr.io/hpa-example
            resources:
              requests:
                memory: "256Mi"
                cpu: "500m"
              limits:
                memory: "500Mi"
                cpu: "1"
  ### Service
  apiVersion: v1
  kind: Service
  metadata:
    name: my-loadbalancer-hpa
  spec:
    selector:
      app: nginx-hpa
    ports:
      - protocol: TCP
        port: 80
        targetPort: 80
    type: LoadBalancer
  ### HPA
  apiVersion: autoscaling/v1
  kind: HorizontalPodAutoscaler
  metadata:
    name: my-hpa
  spec:
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: my-nginx-deployment-hpa
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 20
  $ kubectl apply -f load-balancer-hpa.yaml
  Task:
  Сбой виртуальной машины.
  Давайте посмотрим, как принципы построения отказоустойчивых систем реализованы в Yandex Cloud.
  В практических работах этой темы вы проверите четыре основных сценария отказов: сбой виртуальной машины, сбой всей зоны доступности, обновление приложения, сбой приложения.
  Вы сымитируете эти отказы и понаблюдаете, как Yandex Cloud обеспечивает доступность приложения и восстанавливает инфраструктуру после сбоев.
  Decision:
  Начнем с самого простого сценария — сбоя виртуальной машины.
  Создайте группу из трёх ВМ в трёх зонах доступности под балансировщиком нагрузки. Используйте образ с ОС Ubuntu 18.04 (потом мы обновим его на более свежую версию ОС).
  Используйте спецификацию specification.yaml из практической работы по CLI Yandex Cloud, но адаптируйте её для того, чтобы на ней можно было проверить разные сценарии сбоев.
  Во-первых, будут задействованы все три зоны доступности, поэтому нужно немного исправить блок allocation_policy:
  allocation_policy:
      zones:
          - zone_id: ru-central1-a
          - zone_id: ru-central1-b
          - zone_id: ru-central1-c
  Также пропишите подсети для каждой зоны (не забывайте подставлять идентификаторы ваших подсетей):
      network_interface_specs:
          - network_id: <идентификатор_сети>
            subnet_ids:
              - <идентификатор_подсети_№1>
              - <идентификатор_подсети_№2>
              - <идентификатор_подсети_№3>
            primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }}
  Во-вторых, в секции #cloud-config укажите пользователя, которого нужно создать для входа в виртуальные машины по SSH (это понадобится позднее, на одной из следующих практических работ):
          users:
            - name: my-user
              groups: sudo
              lock_passwd: true
              sudo: 'ALL=(ALL) NOPASSWD:ALL'
              ssh-authorized-keys:
                - ssh-rsa AAAAB3Nza...
  Обновленный файл спецификации specification.yaml:
  name: my-group
  service_account_id: <идентификатор_сервисного_аккаунта>
  instance_template:
      platform_id: standard-v1
      resources_spec:
          memory: 2g
          cores: 2
      boot_disk_spec:
          mode: READ_WRITE
          disk_spec:
              image_id: <идентификатор_образа_Ubuntu_18.04>
              type_id: network-hdd
              size: 32g
      network_interface_specs:
          - network_id: <идентификатор_сети>
            subnet_ids:
              - <идентификатор_подсети_№1>
              - <идентификатор_подсети_№2>
              - <идентификатор_подсети_№3>
            primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }}
      scheduling_policy:
          preemptible: false
      metadata:
        user-data: |-
          #cloud-config
            users:
              - name: my-user
                groups: sudo
                lock_passwd: true
                sudo: 'ALL=(ALL) NOPASSWD:ALL'
                ssh-authorized-keys:
                  - <содержимое_публичной_части_SSH-ключа>
            package_update: true
            runcmd:
              - [ apt-get, install, -y, nginx ]
              - [/bin/bash, -c, 'source /etc/lsb-release; sed -i "s/Welcome to nginx/It is $(hostname) on $DISTRIB_DESCRIPTION/" /var/www/html/index.nginx-debian.html']
  deploy_policy:
      max_unavailable: 1
      max_expansion: 0
  scale_policy:
      fixed_scale:
          size: 3
  allocation_policy:
      zones:
          - zone_id: ru-central1-a
          - zone_id: ru-central1-b
          - zone_id: ru-central1-c
  load_balancer_spec:
      target_group_spec:
          name: my-target-group
  Создайте группу по новой спецификации:
  yc compute instance-group create --file <путь_к_файлу_specification.yaml>
  Если ранее вы удаляли балансировщик нагрузки, создайте его снова и привяжите к целевой группе:
  yc load-balancer network-load-balancer create \
    --region-id ru-central1 \
    --name my-load-balancer \
    --listener name=my-listener,external-ip-version=ipv4,port=80 \
    --target-group target-group-id=<идентификатор_целевой_группы>,healthcheck-name=test-health-check,healthcheck-interval=2s,healthcheck-timeout=1s,healthcheck-unhealthythreshold=2,healthcheck-healthythreshold=2,healthcheck-http-port=80
  В консоли управления убедитесь, что ресурсы созданы. Проверьте вывод по внешнему IP-адресу балансировщика — должна отображаться приветственная страница с идентификатором одной из виртуальных машин группы.
  Начните отслеживать состояние виртуальных машин группы и целевой группы балансировщика:
  while true; do \
  yc compute instance-group \
    --id <идентификатор_группы_ВМ> list-instances; \
  yc load-balancer network-load-balancer \
    --id <идентификатор_балансировщика> target-states \
    --target-group-id <идентификатор_целевой_группы>; \
  sleep 5; done
  Информация выводится в виде таблиц:
  +----------------------+---------------------------+----------------+-------------+------------------------+----------------+
  |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |         STATUS         | STATUS MESSAGE |
  +----------------------+---------------------------+----------------+-------------+------------------------+----------------+
  | ef34nv4tp3ha8gl6p3df | cl1m5ksvljnq5frekghi-uzex | 84.201.148.207 | 10.128.0.42 | RUNNING_ACTUAL [1m54s] |                |
  | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [13m]   |                |
  | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [6h]    |                |
  +----------------------+---------------------------+----------------+-------------+------------------------+----------------+
  +----------------------+-------------+---------+
  |      SUBNET ID       |   ADDRESS   | STATUS  |
  +----------------------+-------------+---------+
  | b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |
  | e2luooifg8ruecr7g6fk | 10.128.0.6  | HEALTHY |
  | e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY |
  +----------------------+-------------+---------+
  Сбой виртуальной машины может произойти из-за падения физического хоста, на котором она запущена. Иногда виртуальную машину могут удалить случайно, по ошибке. Чтобы сымитировать сбой, удалим одну из виртуальных машин в группе через консоль управления.
  Если бы это была единственная машина, на которую поступает трафик, система стала бы недоступна. Но у нас система развернута на нескольких виртуальных машинах, поэтому трафик будет перенаправлен на две оставшиеся. Через несколько секунд будет обнаружена проблема, и виртуальная машина будет выведена из-под балансировки. Об этом говорит статус UNHEALTHY.
  +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
  |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS        | STATUS MESSAGE |
  +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
  | ef330no5frc5de91v77n | cl1m5ksvljnq5frekghi-uzex | 84.201.147.33  | 10.128.0.6  | RUNNING_ACTUAL [15m] |                |
  | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [32m] |                |
  | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [6h]  |                |
  +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
  +----------------------+-------------+-----------+
  |      SUBNET ID       |   ADDRESS   |  STATUS   |
  +----------------------+-------------+-----------+
  | b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY   |
  | e2luooifg8ruecr7g6fk | 10.128.0.6  | UNHEALTHY |
  | e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY   |
  +----------------------+-------------+-----------+
  Далее подсеть перейдет в статус DRAINING — ресурс удаляется, и с него снимается трафик. Балансировщик перестает передавать трафик этому ресурсу.
  +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
  |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS        | STATUS MESSAGE |
  +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
  | ef330no5frc5de91v77n | cl1m5ksvljnq5frekghi-uzex | 84.201.147.33  | 10.128.0.6  | CLOSING_TRAFFIC [0s] |                |
  | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [33m] |                |
  | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [6h]  |                |
  +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
  +----------------------+-------------+----------+
  |      SUBNET ID       |   ADDRESS   |  STATUS  |
  +----------------------+-------------+----------+
  | b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY  |
  | e2luooifg8ruecr7g6fk | 10.128.0.6  | DRAINING |
  | e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY  |
  +----------------------+-------------+----------+
  +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
  |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS        | STATUS MESSAGE |
  +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
  | ef330no5frc5de91v77n | cl1m5ksvljnq5frekghi-uzex | 84.201.147.33  | 10.128.0.6  | CLOSING_TRAFFIC [9s] |                |
  | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [33m] |                |
  | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [6h]  |                |
  +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
  +----------------------+-------------+----------+
  |      SUBNET ID       |   ADDRESS   |  STATUS  |
  +----------------------+-------------+----------+
  | b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY  |
  | e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY  |
  +----------------------+-------------+----------+
  После этого Instance Group начнет пересоздавать удалённую виртуальную машину. Процесс восстановления может занять некоторое время. Понаблюдаем за ним.
  Сначала новая виртуальная машина появится в группе в статусе CREATING_INSTANCE.
  +----------------------+---------------------------+----------------+-------------+-------------------------+----------------+
  |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |         STATUS          | STATUS MESSAGE |
  +----------------------+---------------------------+----------------+-------------+-------------------------+----------------+
  | ef330no5frc5de91v77n | cl1m5ksvljnq5frekghi-uzex | 84.201.147.33  | 10.128.0.6  | CREATING_INSTANCE [-1s] |                |
  | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [33m]    |                |
  | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [6h]     |                |
  +----------------------+---------------------------+----------------+-------------+-------------------------+----------------+
  +----------------------+-------------+----------+
  |      SUBNET ID       |   ADDRESS   |  STATUS  |
  +----------------------+-------------+----------+
  | b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY  |
  | e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY  |
  +----------------------+-------------+----------+
  Далее виртуальная машина будет открыта для трафика (статус OPEN_TRAFFIC). Балансировщик начнет процесс включения машины в список доступных машин.
  +----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+
  |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS         |         STATUS MESSAGE         |
  +----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+
  | ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | OPENING_TRAFFIC [-1s] | Adding target(s)               |
  |                      |                           |                |             |                       | 10.128.0.32 to target group    |
  |                      |                           |                |             |                       | b7rh0bhm9f82dglb2p9r           |
  | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [34m]  |                                |
  | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [7h]   |                                |
  +----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+
  +----------------------+-------------+---------+
  |      SUBNET ID       |   ADDRESS   | STATUS  |
  +----------------------+-------------+---------+
  | b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |
  | e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY |
  +----------------------+-------------+---------+
  +----------------------+---------------------------+----------------+-------------+----------------------+--------------------------------+
  |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS        |         STATUS MESSAGE         |
  +----------------------+---------------------------+----------------+-------------+----------------------+--------------------------------+
  | ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | OPENING_TRAFFIC [1s] | Adding target(s)               |
  |                      |                           |                |             |                      | 10.128.0.32 to target group    |
  |                      |                           |                |             |                      | b7rh0bhm9f82dglb2p9r           |
  | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [34m] |                                |
  | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [7h]  |                                |
  +----------------------+---------------------------+----------------+-------------+----------------------+--------------------------------+
  +----------------------+-------------+---------+
  |      SUBNET ID       |   ADDRESS   | STATUS  |
  +----------------------+-------------+---------+
  | b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |
  | e2luooifg8ruecr7g6fk | 10.128.0.32 | INITIAL |
  | e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY |
  +----------------------+-------------+---------+
  +----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+
  |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS         |         STATUS MESSAGE         |
  +----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+
  | ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | OPENING_TRAFFIC [18s] | Awaiting HEALTHY state for     |
  |                      |                           |                |             |                       | target(s) 10.128.0.32. Elapsed |
  |                      |                           |                |             |                       | time: 3s.                      |
  | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [34m]  |                                |
  | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [7h]   |                                |
  +----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+
  +----------------------+-------------+----------+
  |      SUBNET ID       |   ADDRESS   |  STATUS  |
  +----------------------+-------------+----------+
  | b0c4h992tbuodl5hudpu | 10.128.0.32 | INITIAL  |
  | b0c4h992tbuodl5hudpu | 10.128.0.37 | INACTIVE |
  | b0c4h992tbuodl5hudpu | 10.128.0.9  | HEALTHY  |
  +----------------------+-------------+----------+
  +----------------------+---------------------------+----------------+-------------+-------------------------+--------------------------------+
  |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |         STATUS          |         STATUS MESSAGE         |
  +----------------------+---------------------------+----------------+-------------+-------------------------+--------------------------------+
  | ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | OPENING_TRAFFIC [1m32s] | [NLB unhealthy]; Awaiting      |
  |                      |                           |                |             |                         | HEALTHY state for target(s)    |
  |                      |                           |                |             |                         | 10.128.0.32. Elapsed time: 1m  |
  |                      |                           |                |             |                         | 17s.                           |
  | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [35m]    |                                |
  | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [7h]     |                                |
  +----------------------+---------------------------+----------------+-------------+-------------------------+--------------------------------+
  +----------------------+-------------+---------+
  |      SUBNET ID       |   ADDRESS   | STATUS  |
  +----------------------+-------------+---------+
  | b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |
  | e2luooifg8ruecr7g6fk | 10.128.0.32 | HEALTHY |
  | e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY |
  +----------------------+-------------+---------+
  И в завершение подсеть перейдет в статус HEALTHY, а машина — в статус RUNNING_ACTUAL, и трафик будет снова разделен между тремя машинами.
  +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
  |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS        | STATUS MESSAGE |
  +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
  | ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | RUNNING_ACTUAL [-1s] |                |
  | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [35m] |                |
  | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [7h]  |                |
  +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
  +----------------------+-------------+---------+
  |      SUBNET ID       |   ADDRESS   | STATUS  |
  +----------------------+-------------+---------+
  | b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |
  | e2luooifg8ruecr7g6fk | 10.128.0.32 | HEALTHY |
  | e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY |
  +----------------------+-------------+---------+
  Восстановление произошло автоматически без ручного вмешательства.
  Decision:
  $ vim specification1.yaml
  $ cat specification1.yaml
  name: my-group
  service_account_id: YOUR-ID
  instance_template:
      platform_id: standard-v1
      resources_spec:
          memory: 2g
          cores: 2
      boot_disk_spec:
          mode: READ_WRITE
          disk_spec:
              image_id: YOUR-ID1
              type_id: network-hdd
              size: 32g
      network_interface_specs:
          - network_id: YOUR-ID2
            subnet_ids:
              - YOUR-ID3
              - YOUR-ID4
              - YOUR-ID5
            primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }}
      scheduling_policy:
          preemptible: false
      metadata:
        user-data: |-
          #cloud-config
            users:
              - name: YOUR-USERNAME
                groups: sudo
                lock_passwd: true
                sudo: 'ALL=(ALL) NOPASSWD:ALL'
                ssh-authorized-keys:
                  - YOUR-KEY
            package_update: true
            runcmd:
              - [ apt-get, install, -y, nginx ]
              - [/bin/bash, -c, 'source /etc/lsb-release; sed -i "s/Welcome to nginx/It is $(hostname) on $DISTRIB_DESCRIPTION/" /var/www/html/index.nginx-debian.html']
  deploy_policy:
      max_unavailable: 1
      max_expansion: 0
  scale_policy:
      fixed_scale:
          size: 3
  allocation_policy:
      zones:
          - zone_id: ru-central1-a
          - zone_id: ru-central1-b
          - zone_id: ru-central1-c
  load_balancer_spec:
      target_group_spec:
          name: my-target-group
  $ yc compute instance-group create --file specification.yaml
  $ yc load-balancer network-load-balancer create \
    --region-id ru-central1 \
    --name my-load-balancer \
    --listener name=my-listener,external-ip-version=ipv4,port=80 \
    --target-group target-group-id=YOUR-ID6,healthcheck-name=test-health-check,healthcheck-interval=2s,healthcheck-timeout=1s,healthcheck-unhealthythreshold=2,healthcheck-healthythreshold=2,healthcheck-http-port=80
  $ while true; do \
  yc compute instance-group \
    --id YOUR-ID8 list-instances; \
  yc load-balancer network-load-balancer \
    --id YOUR-ID7 target-states \
    --target-group-id YOUR-ID6; \
  sleep 5; done
  Task:
  Сбой зоны доступности.
  В этом сценарии рассмотрим ситуацию, когда произошел сбой сразу всей зоны доступности. Такие ситуации возникают крайне редко и могут быть связаны с какими-то масштабными стихийными бедствиями, однако и их стоит предусмотреть.
  Посмотрим, как будет решаться проблема неожиданного выхода из строя зоны доступности.
  Decision:
  В нашем примере (см. предыдущий урок) используются все три зоны доступности — ru-central1-a, ru-central1-b и ru-central1-c. В каждой зоне располагается одна ВМ.
  Установите такие настройки политики развертывания — пусть группу можно расширять на 1 ВМ и уменьшать на 1 ВМ:
  Теперь в настройках группы виртуальных машин уберите одну зону доступности, например ru-central1-c. Переключитесь на вкладку Список ВМ и посмотрите, что будет происходить.
  Для ВМ, которая располагалась в зоне ru-central1-c, отключается трафик (статус Closing traffic), а затем сама машина удаляется (статус Deleting instance). Одновременно в другой зоне доступности создаётся и запускается новая ВМ. Остальные машины в группе продолжают работать без изменений.
  Таким образом, даже при выходе из строя всей зоны доступности группа виртуальных машин продолжит работать и будет способна принимать прежнюю нагрузку.
  Task:
  Обновление приложения.
  На практической работе с CLI мы уже рассматривали обновление операционной системы для группы виртуальных машин. Любые приложения, установленные на ВМ, обновляются по тем же правилам. Давайте рассмотрим этот процесс ещё раз.
  Decision:
  Первый вариант обновления
  Если вы работаете в консоли управления, измените шаблон ВМ и выберите образ с ОС Ubuntu 20.04. Убедитесь, что параметры политики развёртывания такие: группу нельзя расширять, а уменьшать можно только на одну ВМ.
  Политика развёртывания группы виртуальных машин (вариант 1)
  Если вы работаете в командной строке, в спецификации specification.yaml измените параметр image_id (например с fd8s2gbn4d5k2rcf12d9 на fd8ju9iqf6g5bcq77jns) и запустите обновление группы:
  yc compute instance-group update \
    --name my-group \
    --file <путь_к_файлу_specification.yaml>
  В консоли управления на странице группы ВМ перейдите на вкладку Список ВМ и проследите, как меняются статусы машин.
  Сначала вы увидите статус Running outdated. Это означает, что машины работают со старой версией приложения.
  Затем одна из машин начинает обновляться: для неё закрывается трафик (статус Closing traffic), она останавливается (статус Stopping instance), обновляется (статус Updating instance), затем трафик снова открывается (статус Opening traffic), и наконец статус меняется на Running actual. Обновление выполнено.
  Затем то же самое последовательно выполняется для остальных машин в группе.
  Порядок обновления зависит от политики развёртывания. Мы запретили увеличивать размер группы и указали, что одновременно неработоспособной может быть только одна машина. Именно так и произошло обновление: машины по одной выводились из строя, обновлялись и запускались снова.
  Второй вариант обновления
  Теперь давайте изменим настройки политики развёртывания.
  Если вы работаете в консоли управления, измените шаблон ВМ и выберите образ с Ubuntu и NGINX, созданный ранее и помещённый в Container Registry.
  Измените параметры развёртывания. Теперь группу можно расширять на одну ВМ, а уменьшать нельзя:
  Политика развёртывания группы виртуальных машин (вариант 2)
  Если вы работаете в командной строке, в спецификации specification.yaml измените параметр image_id (например, снова с fd8ju9iqf6g5bcq77jns на fd8s2gbn4d5k2rcf12d9). Параметры обновления измените так:
  deploy_policy:
      max_unavailable: 0
      max_expansion: 1
  Запустите обновление группы.
  В консоли управления на странице группы ВМ перейдите на вкладку Список ВМ и проследите, как меняются статусы машин.
  Сначала вы увидите статус Running outdated. Затем создаётся новая машина (статус Creating instance), для неё открывается трафик (статус Opening traffic), статус машины меняется на Running actual, при этом одна из «устаревших» ВМ выводится из строя (статусы Closing traffic и Stopping instance).
  Затем то же самое последовательно выполняется для остальных машин в группе.
  Decision:
  $ yc compute instance-group update \
    --id YOUR-ID \
    --file specification1.yaml
  $ cat specification1.yaml
  ...
  deploy_policy:
      max_unavailable: 1
      max_expansion: 0
  ...
  $ vim specification1.yaml
  $ cat specification1.yaml
  ...
  deploy_policy:
      max_unavailable: 0
      max_expansion: 1
  ...
  Task:
  Сбой приложения.
  Последний сценарий, который мы рассмотрим, это сбой приложения. Ситуация, когда сама ВМ работоспособна, но по каким-то причинам произошла ошибка в приложении. Это может быть потеря соединения с базой данных или какой-то баг в запущенном приложении (например утечка памяти). Давайте сымитируем такой сценарий. На наших виртуальных машинах запущен только веб-сервер NGINX, давайте остановим его. Но сначала включим проверку состояния ВМ.
  Decision:
  В консоли управления откройте вкладку Обзор для вашей группы виртуальных машин, нажмите кнопку Изменить и активируйте проверку состояний. Сохраните изменения.
  В браузере откройте страницу с внешним IP-адресом балансировщика, привязанного к вашей группе, и посмотрите, на какую из машин выводится трафик. Узнайте внешний IP-адрес этой машины.
  В новой вкладке браузера откройте IP-адрес этой виртуальной машины и убедитесь, что выводится приветственная страница, т. е. сервер доступен.
  Помните, когда вы меняли файл конфигурации для группы машин, вы добавили в него пользователя my-user? Теперь он вам пригодится — из консоли зайдите на ВМ от его имени:
  ssh my-user@<внешний_IP-адрес_ВМ>
  Посмотрите список запущенных процессов:
  ps axu
  Убедитесь, что в списке есть процессы nginx:
  Теперь остановите эти процессы, чтобы сделать сервер недоступным:
  sudo killall nginx
  В браузере обновите страницу балансировщика. Вы увидите, что теперь трафик направляется на другую виртуальную машину группы. Это означает, что Instance Group обнаружил сбой приложения и переключил трафик.
  Теперь обновите страницу виртуальной машины, на которой вы остановили NGINX. Убедитесь, что сервер теперь недоступен.
  Откройте список машин вашей группы и проследите, как меняется состояние одной из машин.
  Сначала будет закрыт трафик (статус Closing traffic), затем виртуальная машина будет остановлена (статус Stopping instance), а затем перезапущена (статус Running actual).
  Убедитесь, что веб-сервер на этой ВМ снова доступен.
  Мы проверили четыре основных сценария сбоев и убедились, что Yandex Cloud автоматически отрабатывает их и восстанавливает работоспособность группы.
  $ ssh YOUR-USERNAME@YOUR-IP
  $ ps axu
  $ sudo killall nginx
  Task:
  Отправка собственных метрик.
  Часто бывает полезно отслеживать более широкий набор метрик, чем тот, что доступен в Yandex Monitoring «из коробки».
  Предположим, вам интересно узнать, сколько людей заходит на ваш сайт и как их число зависит от времени дня или дня недели.
  Вы можете выгружать эти данные из Яндекс Метрики или вашей собственной аналитической системы и самостоятельно загружать в Yandex Monitoring с помощью API.
  Давайте попробуем сделать это с нашим сайтом.
  Decision:
  Отправка метрик через API
  Получите IAM-токен: Инструкция для аккаунта на Яндексе, Инструкция для сервисного аккаунта.
  Обратите внимание — токены устаревают через 12 часов после создания. Поэтому если вы сделаете паузу при выполнении данной практической работы, для продолжения лучше запросить новый токен.
  Сохраните токен в переменной окружения, так его будет проще использовать:
  export IAM_TOKEN=<IAM-токен>
  Создайте файл с телом запроса, например my-metrics.json. В свойстве metrics указывается список метрик для записи. Пусть это будет количество пользователей сайта. В массиве timeseries указываются значения на разные моменты времени (измените число на сегодняшнее в формате год-месяц-день).
  {
    "metrics": [
      {
        "name": "number_of_users",
        "labels": {
        "site": "aibolit"
        },
        "type": "IGAUGE",
        "timeseries": [
          {
            "ts": "2021-05-10T10:00:00Z",
            "value": "22"
          },
          {
            "ts": "2021-05-10T11:00:00Z",
            "value": "44"
          },
          {
            "ts": "2021-05-10T12:00:00Z",
            "value": "11"
          },
          {
            "ts": "2021-05-10T13:00:00Z",
            "value": "55"
          },
          {
            "ts": "2021-05-10T14:00:00Z",
            "value": "33"
          }
        ]
      }
    ]
  }
  Отправьте запрос, указав в нем идентификатор каталога и имя сервиса custom (это имя указывается для всех пользовательских метрик):
  curl -X POST \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer ${IAM_TOKEN}" \
      -d '@<путь_к_файлу_my-metrics.json>' \
  'https://monitoring.api.cloud.yandex.net/monitoring/v2/data/write?folderId=<идентификатор_каталога>&service=custom'
  Мониторинг пользовательских метрик
  Создайте на вашем дашборде новый виджет с графиком, назовите его «Число пользователей сайта».
  В виджете создайте запрос с параметрами service = Custom Metrics и name = number_of_users. Убедитесь, что в виджете выбран нужный период:
  Этот график станет нагляднее, если вместо точек отображать столбцы. Тип графика можно изменить с помощью кнопки в правом верхнем углу виджета:
  Мониторинг метрик Linux
  Другой пример — ваши приложения запущены на виртуальных машинах под Linux. По умолчанию вы можете посмотреть утилизацию ресурсов процессора или диска для ВМ в целом. Но вам будет полезно знать, сколько ресурсов потребляет каждое из них. В Yandex Monitoring вы можете отслеживать системные метрики Linux, такие как объём свободной памяти или загрузка процессора. Но для этого нужно дополнительно настроить отправку этих метрик с помощью Yandex Unified Agent, который мы уже упоминали.
  Установка Yandex Unified Agent
  Создайте виртуальную машину. На неё вы будете устанавливать Yandex Unified Agent. Можете использовать образ с ОС Ubuntu, который вы создали ранее и поместили в Container Registry. Назовите машину, например, for-ua.
  При создании используйте ваш сервисный аккаунт. Задайте логин (например ua-user) и ssh-ключ.
  Для сервисного аккаунта добавьте роль monitoring.editor.
  Посмотрите публичный IP-адрес машины for-ua и зайдите на неё по ssh:
  ssh ua-user@<публичный_адрес_ВМ>
  Теперь вы можете установить Yandex Unified Agent:
  ua_version=$(curl -s https://storage.yandexcloud.net/yc-unified-agent/latest-version) bash -c 'curl -s -O https://storage.yandexcloud.net/yc-unified-agent/releases/$ua_version/unified_agent && chmod +x ./unified_agent'
  Также вы можете выбрать опцию Установить в поле Агент сбора метрик при создании ВМ, тогда Yandex Unified Agent будет установлен автоматически.
  Создайте файл config.yml с типовой спецификацией для доставки метрик Linux.
  В параметре folder_id укажите идентификатор вашего каталога.
  status:
  port: "16241"
  storages:
  - name: main
    plugin: fs
    config:
      directory: /var/lib/yandex/unified_agent/main
      max_partition_size: 100mb
      max_segment_size: 10mb
  channels:
  - name: cloud_monitoring
    channel:
      pipe:
        - storage_ref:
            name: main
      output:
        plugin: yc_metrics
        config:
          folder_id: "<идентификатор_каталога>"
          iam:
            cloud_meta: {}
  routes:
  - input:
      plugin: linux_metrics
      config:
        namespace: sys
    channel:
      channel_ref:
        name: cloud_monitoring
  - input:
      plugin: agent_metrics
      config:
        namespace: ua
    channel:
      pipe:
        - filter:
            plugin: filter_metrics
            config:
              match: "{scope=health}"
      channel_ref:
        name: cloud_monitoring
  import:
  - /etc/yandex/unified_agent/conf.d/*.yml
  В секции status достаточно указать порт для просмотра статуса Yandex Unified Agent.
  Секция storage содержит список хранилищ, в которых будут находиться выгруженные данные. Для практической работы достаточно одного файлового хранилища (fs).
  Секция channels содержит список именованных каналов, к этим каналам можно обращаться по имени из других секций спецификации. Здесь обозначен один канал с именем cloud_monitoring. К нему идёт обращение из секции routes, которая содержит список маршрутов доставки метрик.
  Подробнее о конфигурировании Yandex Unified Agent вы можете почитать в документации.
  Скопируйте файл спецификации в виртуальную машину for-ua:
  scp config.yml ua-user@84.252.135.237:config.yml
  Теперь запустите Unified Agent с созданной спецификацией:
  sudo ./unified_agent --config config.yml
  Если запуск прошел успешно, в конце вы увидите сообщение такого вида:
  ... NOTICE agent started
  Настройка виджета для мониторинга метрик Linux. Создайте на вашем дашборде новый виджет с графиком, назовите его «Метрики Linux».
  В виджете создайте запрос с параметром service = Custom Metrics. В параметре name выберите любой параметр, начинающийся с sys — всё это системные метрики, поставляемые Unified Agent. Например, name = sys.memory.MemAvailable.
  Теперь в виджете отображается график наличия свободной оперативной памяти в виртуальной машине for-ua.
  Decision:
  $ yc iam key create --service-account-name monitortest --output key.json
  $ yc config profile create monitortest-profile
  $ yc config set service-account-key key.json
  $ yc iam create-token
  $ cat key.json
  ...
  t1.9euelZqMx8vGx5rNj4qezM7MyJ2Vj-3rnpWal47Gj8vOzZuZzc2ejZycks7l8_cfU1Fl-e8cMVEZ_d3z918BT2X57xwxURn9zef1656Vmp6RyMqKl8fIxo2Rm5XHzcbM7_0.XW7UF1ymvhuU-4VWP34TmF1Yw4YEDuWUggt0pWoxIqaC8dkP8UKcbjfHwk3JLcYRsrkgvCosBvnHl1IRfUvECA
  $ export IAM_TOKEN=t1.9euelZqMx8vGx5rNj4qezM7MyJ2Vj-3rnpWal47Gj8vOzZuZzc2ejZycks7l8_cfU1Fl-e8cMVEZ_d3z918BT2X57xwxURn9zef1656Vmp6RyMqKl8fIxo2Rm5XHzcbM7_0.XW7UF1ymvhuU-4VWP34TmF1Yw4YEDuWUggt0pWoxIqaC8dkP8UKcbjfHwk3JLcYRsrkgvCosBvnHl1IRfUvECA
  $ vim my-metrics.json
  $ cat my-metrics.json
  {
    "metrics": [
      {
        "name": "number_of_users",
        "labels": {
        "site": "aibolit"
        },
        "type": "IGAUGE",
        "timeseries": [
          {
            "ts": "2021-05-10T10:00:00Z",
            "value": "22"
          },
          {
            "ts": "2021-05-10T11:00:00Z",
            "value": "44"
          },
          {
            "ts": "2021-05-10T12:00:00Z",
            "value": "11"
          },
          {
            "ts": "2021-05-10T13:00:00Z",
            "value": "55"
          },
          {
            "ts": "2021-05-10T14:00:00Z",
            "value": "33"
          }
        ]
      }
    ]
  }
  $ curl -X POST \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer ${t1.9euelZqMx8vGx5rNj4qezM7MyJ2Vj-3rnpWal47Gj8vOzZuZzc2ejZycks7l8_cfU1Fl-e8cMVEZ_d3z918BT2X57xwxURn9zef1656Vmp6RyMqKl8fIxo2Rm5XHzcbM7_0.XW7UF1ymvhuU-4VWP34TmF1Yw4YEDuWUggt0pWoxIqaC8dkP8UKcbjfHwk3JLcYRsrkgvCosBvnHl1IRfUvECA}" \
      -d '@/home/YOUR-DIR/my-metrics.json' \
  'https://monitoring.api.cloud.yandex.net/monitoring/v2/data/write?folderId=ajehq9p412df22arccm1&service=custom'
  $ ssh ua-user@<публичный_адрес_ВМ>
  $ ua_version=$(curl -s https://storage.yandexcloud.net/yc-unified-agent/latest-version) bash -c 'curl -s -O https://storage.yandexcloud.net/yc-unified-agent/releases/$ua_version/unified_agent && chmod +x ./unified_agent'
  $ exit
  $ vim config.yml
  $ cat config.yml
  status:
  port: "16241"
  storages:
  - name: main
      plugin: fs
      config:
        directory: /var/lib/yandex/unified_agent/main
        max_partition_size: 100mb
        max_segment_size: 10mb
  channels:
  - name: cloud_monitoring
      channel:
        pipe:
          - storage_ref:
              name: main
        output:
          plugin: yc_metrics
          config:
            folder_id: "<идентификатор_каталога>"
            iam:
              cloud_meta: {}
  routes:
  - input:
      plugin: linux_metrics
      config:
        namespace: sys
    channel:
      channel_ref:
        name: cloud_monitoring
  - input:
      plugin: agent_metrics
      config:
        namespace: ua
    channel:
      pipe:
        - filter:
            plugin: filter_metrics
            config:
              match: "{scope=health}"
      channel_ref:
        name: cloud_monitoring
  import:
  - /etc/yandex/unified_agent/conf.d/*.yml
  $ scp config.yml ua-user@84.252.135.237:config.yml
  $ sudo ./unified_agent --config config.yml
  Task:
  Выгрузка метрик в формате Prometheus
  Как мы уже говорили раньше, метрики можно выгружать из Yandex Cloud Monitoring в сторонние приложения и сервисы.
  Пожалуй, чаще всего их выгружают для сервера Prometheus.
  На сегодняшний день Prometheus — один из самых популярных инструментов для мониторинга приложений и сервисов. В основе его лежит специализированная СУБД для анализа временных рядов, которая обеспечивает высокое быстродействие. В отличие от большинства систем мониторинга, Prometheus не ждёт, пока сторонние приложения передадут ему свои метрики, а сам опрашивает подключенные к нему приложения и собирает нужные данные.
  Prometheus и Yandex Cloud Monitoring решают схожие задачи — хранят значения разных метрик. Prometheus фактически является стандартом для обмена метриками. Поэтому даже используя сервисы Yandex Cloud, IT-администраторы часто хотят отслеживать их работу с помощью Prometheus. Чтобы не лишать специалистов привычных инструментов, Yandex Cloud Monitoring поддерживает выгрузку данных в формате Prometheus. Для этого используется метод prometheusMetrics.
  Для визуализации данных, собираемых Prometheus, можно использовать сервис Grafana (в нем можно зарегистрироваться бесплатно на тестовый период).
  Вы можете установить Grafana на свой компьютер, а можете работать в облачной версии.
  Посмотрим, как происходит выгрузка метрик в Prometheus и работа с ними в Grafana. Вы снова будете мониторить сайт клиники «Доктор Айболит».
  Decision:
  Создайте API-ключ через консоль управления Yandex Cloud или CLI.
  Если вы создаете ключ в консоли управления, то перейдите в каталог, из которого будете выгружать метрики (например default). Затем перейдите на вкладку Сервисные аккаунты и выберите существующий аккаунт. Нажмите кнопку Создать новый ключ и выберите Создать API-ключ. В описании ключа можно указать, например, «для доступа к Prometheus». Сохраните секретную часть ключа в отдельный файл, например, prometheus-key.txt.
  Назначьте сервисному аккаунту роль monitoring.viewer на выбранный каталог.
  Создайте файл спецификации prometheus.yml (см. пример ниже, замените в нем значение параметра folderId на идентификатор каталога, а значение для bearer_token — на ключ доступа из файла prometheus-key.txt):
  global:
    scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
    evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
    # scrape_timeout is set to the global default (10s).
  rule_files:
  scrape_configs:
    - job_name: 'prometheus'
      static_configs:
      - targets: ['localhost:9090']
    - job_name: 'yc-monitoring-export'
      metrics_path: '/monitoring/v2/prometheusMetrics'
      params:
        folderId:
        - '<идентификатор_каталога>'
        service:
        - 'storage'
      bearer_token: '<секретная_часть_API-ключа>'
      static_configs:
      - targets: ['monitoring.api.cloud.yandex.net']
        labels:
            folderId: '<идентификатор_каталога>'
            service: 'storage'
  Запуск сервера Prometheus. Если вы уже работаете с Prometheus, пропустите все шаги по установке — просто добавьте секцию scrape_configs из примера выше в спецификацию вашего сервера Prometheus и перезапустите сервер, а затем переходите к настройке Grafana.
  Для запуска сервера Prometheus используйте официальный Docker-образ prom/prometheus.
  Сначала загрузите образ. Для этого запустите Docker Desktop (в терминале выполните команду):
  docker pull prom/prometheus
  Чтобы на сервере сразу был ваш файл спецификации, создайте свой образ на основе prom/prometheus. Подготовьте Dockerfile с двумя командами:
  FROM prom/prometheus
  ADD prometheus.yml /etc/prometheus/
  Сохраните этот файл в тот же каталог, где находится prometheus.yml. Назовите его именем по умолчанию: Dockerfile.
  В терминале перейдите в каталог с Dockerfile. Создайте образ с вашей конфигурацией (используйте ваш идентификатор в Yandex Container Registry):
  docker build . -t cr.yandex/<идентификатор_реестра>/my-prometheus:latest -f Dockerfile
  Аутентифицируйтесь в Yandex Container Registry с помощью Docker Credential helper (чтобы Docker мог от вашего имени отправить образ в ваш реестр):
  yc container registry configure-docker
  Теперь отправьте образ в ваше хранилище в облаке:
  docker push cr.yandex/<идентификатор_реестра>/my-prometheus:latest
  Создайте виртуальную машину с помощью Container Optimized Image, вы уже делали это раньше в практической работе (в разделе Выбор образа загрузочного диска переключитесь на вкладку Container Solution и нажмите Настроить. Выберите из реестра созданный вами образ, остальные настройки оставьте по умолчанию и нажмите Применить).
  При создании виртуальной машины используйте ваш сервисный аккаунт. Задайте логин (например prom) и ssh-ключ.
  Назовите машину, например, for-prometheus.
  Проверьте статус сервера по адресу http://<публичный IP-адрес ВМ с Prometheus>:9090/targets. Через несколько минут после запуска статус процессов prometheus и yc-monitoring-export должен стать UP.
  Подайте нагрузку на ваш сайт:
  while true; do wget -q -O- <адрес_сайта>; done
  Подождите несколько минут и проверьте, как поставляются метрики в Prometheus.
  В верхнем меню выберите пункт Graph. Нажмите на значок «Земли». Откроется меню с доступными метриками. Выберите метрику, которую вы хотите проверить, например, traffic и нажмите кнопку Execute.
  Переключитесь на вкладку Graph. Выберите текущее время, для наглядности уменьшите интервал запроса данных (например до 15 минут). Вскоре вы увидите график изменения выбранной метрики.
  Настройка Grafana. Теперь посмотрим, как метрики визуализируются в системе Grafana.
  Если у вас еще нет аккаунта в Grafana, создайте его с помощью нескольких простых шагов, это бесплатно. Вам откроется интерфейс по адресу https://<ваш_логин>.grafana.net/.
  Добавление источника данных. Настройте Prometheus в качестве источника данных. На главной странице нажмите кнопку Connect data. Из предложенного списка выберите источник Prometheus data source и нажмите кнопку Create Prometheus data source.
  В следующем окне в поле URL введите endpoint сервера Prometheus http://<публичный IP-адрес ВМ с Prometheus>:9090. Больше никакие настройки менять не нужно.
  Внизу нажмите кнопку Save & Test. Должна отобразиться надпись Data source is working.
  Добавление дашборда. Вернитесь на главную страницу (нажав на логотип в левом верхнем углу) и нажмите Create your first dashboard. Откроется окно настройки дашборда.
  В нижней части экрана на вкладке Query выберите источник данных — Prometheus.
  Выберите метрику, которую вы хотите отслеживать. Нажмите на поле Metrics, в открывшемся списке выберите метрику traffic.
  Сверху отобразится график выбранной метрики.
  Вверху справа в поле Panel Title укажите название графика (например, «Трафик сайта»).
  Теперь сохраните настройки — в правом верхнем углу нажмите кнопку Save и укажите название дашборда (например, «Мой дашборд»).
  Decision:
  $ vim prometheus-key.txt
  $ cat prometheus-key.txt
  YOUR-KEY
  $ vim prometheus.yml
  $ cat prometheus.yml
  global:
    scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
    evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
    # scrape_timeout is set to the global default (10s).
  rule_files:
  scrape_configs:
    - job_name: 'prometheus'
      static_configs:
      - targets: ['localhost:9090']
    - job_name: 'yc-monitoring-export'
      metrics_path: '/monitoring/v2/prometheusMetrics'
      params:
        folderId:
        - 'YOUR-ID'
        service:
        - 'storage'
      bearer_token: 'YOUR-KEY'
      static_configs:
      - targets: ['monitoring.api.cloud.yandex.net']
        labels:
            folderId: 'YOUR-ID'
            service: 'storage'
  $ docker pull prom/prometheus
  $ vim Dockerfile
  $ cat Dockerfile
  FROM prom/prometheus
  ADD prometheus.yml /etc/prometheus/
  $ docker build . -t cr.yandex/YOUR-ID1/my-prometheus:latest -f Dockerfile
  $ yc container registry configure-docker
  $ docker push cr.yandex/YOUR-ID1/my-prometheus:latest
  $ while true; do wget -q -O- YOUR-IP; done  
  Task:
  В этой практической работе вы создадите алерт для случая, если трафик на сайте вдруг начнет существенно расти.
  Снова используйте сайт клиники «Доктор Айболит», для которого настраивали графики на дашборде.
  Вы можете перейти на вкладку Алерты и там настроить алерт с нуля. А можете отталкиваться от графиков, которые уже выведены в виджете.
  Ниже рассматривается именно второй вариант.
  Decision:
  Создание алерта. Вернитесь на созданный вами дашборд и в меню виджета «Трафик сайта» выберите пункт Создать алерт.
  Поскольку в виджете используются два запроса, вам будет предложено выбрать, для какого запроса вы хотите создать алерт. Выберите запрос с суммирующей функцией и нажмите Продолжить.
  Теперь задайте имя и, если хотите, описание алерта. Укажите значение для статусов Alarm и Warning.
  Откройте спойлер Показать дополнительные настройки. Там вы увидите, что система предложила вам использовать среднее значение за 5 минут. Оставьте эти параметры.
  Теперь нужно выбрать канал для получения алертов. У вас пока ещё нет настроенных каналов, поэтому система предложить вам создать его. Нажмите кнопку Добавить канал и далее Создать канал.
  Укажите имя канала, выберите метод — Email, SMS или Push-уведомления. Укажите получателей — себя. Затем нажмите кнопку Создать.
  В настройках алерта выберите только что созданный канал.
  Вы можете указать для одного алерта несколько каналов уведомлений. Например, если вы хотите получать алерты об увеличении трафика сайта не только в виде Push-уведомлений, но и по электронной почте, создайте еще один канал с методом Email и выберите также и его.
  Для каждого канала можно настроить режим повторения уведомлений. Например, в данном случае при превышении трафика будет отправлен один алерт по электронной почте, а алерты в виде push-уведомлений будут отправляться каждые 5 минут до тех пор, пока проблема не будет устранена.
  Нажмите кнопку Создать алерт.
  Вы увидите настройки созданного алерта, а сверху — его текущий статус OK.
  Нажмите слева на вкладку Алерты. Вы увидите ваш алерт, сейчас он единственный в списке. Когда алертов станет больше, вам понадобятся инструменты для работы с ними. Например, вы сможете отобрать из списка только алерты, имеющие статус Alarm или Warning, или временно деактивировать отдельные алерты.
  Срабатывание алерта. Теперь посмотрим, как срабатывает алерт. Подайте трафик на сайт, который вы мониторите:
  while true; do wget -q -O- <адрес_сайта>; done
  Подождите немного и понаблюдайте за ростом нагрузки. Через какое-то время трафик начнет превышать пороговое значение Warning, и вы начнете получать Push-уведомления.
  Если у администратора настроены другие каналы для алертов, он получил бы SMS или Email с предупреждением о пороговом значении трафика.
  Как видите, алерты позволяют вовремя привлекать внимание администратора и устранять даже потенциальные, ещё не случившиеся проблемы.
  Decision:
  $ while true; do wget -q -O- YOUR-IP; done

Serverless
  Task:
  Для защиты практической работы по теме "Serverless" разработал навык Алисы, которая повторяет всё, что вы ему напишете с сохранением фраз в новом файле в бакете, разработал функцию для проверки доступности сайта ya.ru, которая будет измерять время ответа, передавать в БД PostgreSQL результаты работы функции и запускать триггер-таймер для регулярного опроса сайта ya.ru, с помощью REST API получил до 50 результатов проверки из БД, реализовал проекты, которые позволят пользователям конвертировать видеофайлы в GIF конвертировать длинные ссылки в короткие
  Task:
  Создаём вашу первую функцию
  Мы уже достаточно сказали о том, что создавать облачные функции — просто. Давайте сделаем это на практике.
  Decision:
  На главной странице консоли управления в списке сервисов выберите Cloud Functions:
  На открывшейся странице нажмите кнопку Создать функцию:
  Укажите имя функции, введите короткое описание того, что она будет делать, и нажмите кнопку Создать:
  Затем выберите среду выполнения кода и нажмите кнопку Продолжить:
  По умолчанию сервис предлагает создать Hello World — файл с примером кода на выбранном языке программирования. Этот файл будет создан и автоматически загружен в контейнер. В поле Способ укажите Редактор кода и выберите файл index.go.
  По умолчанию сервис предлагает работать с редактором кода прямо в веб-интерфейсе (как на скриншоте выше). Однако вместо этого вы можете загрузить файл с кодом из бакета Object Storage (этот способ подойдёт для файлов больше 3,5 МБ) или загрузить ZIP-архив с кодом с локальной машины. Переключатель способа добавления кода находится прямо над окном редактора.
  Код вашей функции может находиться как в одном файле, так и в нескольких. Вы также можете создавать папки. При этом обязательно нужно указывать точку входа — часть кода, которая будет вызываться первой и принимать параметры вызова. Формат точки входа — <имя файла с функцией>.<имя обработчика вызова>. Например, index.Handler.
  Вверху справа нажмите кнопку Создать версию, чтобы сохранить текущее состояние функции.
  Сервис создаст версию функции и покажет справочную страницу о ней.
  Как протестировать созданную функцию
  Теперь в панели слева перейдите на вкладку Тестирование. В поле Шаблон данных выберите HTTPS-вызов. Сервис автоматически сгенерирует входные данные в формате JSON.
  Под полем с входными данными нажмите кнопку Запустить тест. Сервис выполнит HTTPS-вызов созданной функции и сформирует ответ (также в формате JSON).
  Task:
  Запускаем функцию с помощью CLI.
  В предыдущей практической работе вы познакомились с созданием функции через консоль управления. 
  На этом уроке вы научитесь создавать функцию с помощью интерфейса командной строки (утилиты yc).
  Пользоваться консолью управления бывает очень удобно, но вести большой проект всё же лучше локально, с помощью среды разработки. 
  Артефакты локальной разработки можно с лёгкостью переносить в облако с помощью консольных утилит. 
  Decision:
  Шаг 1. Создание сервисного аккаунта
  Создание аккаунта
  Для начала убедитесь, что у вас установлена и инициализирована утилита yc.
  У вас уже есть сервисные аккаунты, созданные на предыдущих занятиях. Однако гораздо лучше, когда для каждой конкретной задачи (или блока задач) вы заводите отдельный сервисный аккаунт. Это обеспечивает прозрачность в управлении доступом и контроле за ролями в сервисах.
  Предварительно установите утилиту jq, она потребуется для выполнения задания:
  sudo apt install jq 
  Создайте сервисный аккаунт с именем service-account-for-cf:
  export SERVICE_ACCOUNT=$(yc iam service-account create \
    --name service-account-for-cf \
    --description "service account for cloud functions" \
    --format json | jq -r .) 
  Проверьте текущий список сервисных аккаунтов:
  yc iam service-account list
  echo $SERVICE_ACCOUNT 
  После проверки запишите идентификатор (ID) созданного сервисного аккаунта в переменную SERVICE_ACCOUNT_ID:
  echo "export SERVICE_ACCOUNT_ID=<идентификатор_сервисного_аккаунта>" >> ~/.bashrc && . ~/.bashrc
  echo $SERVICE_ACCOUNT_ID 
  Назначение роли сервисному аккаунту
  Добавьте вновь созданному сервисному аккаунту роль editor:
  echo "export FOLDER_ID=$(yc config get folder-id)" >> ~/.bashrc && . ~/.bashrc 
  echo $FOLDER_ID
  yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_ID \
    --role editor 
  Не удаляйте файл ~/.bashrc после прохождения практической работы, он понадобится нам в дальнейшем.
  Шаг 2. Создание и настройка функции
  Создание функции
  Создайте функцию с именем my-first-function:
  yc serverless function create --name my-first-function 
  Вы получите URL, по которому можно будет сделать вызов функции http_invoke_url. По умолчанию функция будет непубличной.
  Загрузка кода функции
  Создайте файл  index.py :
  sudo nano index.py 
  Добавьте в index.py следующее содержимое:
  def handler(event, context):
      return {
          'statusCode': 200,
          'body': 'Hello World!',
      } 
  Успешное выполнение этой функции вернёт небольшую веб-страницу.
  Загрузите код функции в облако и создайте её версию. Для этого перейдите в папку с файлом index.py и выполните команду:
  yc serverless function version create \
      --function-name my-first-function \
      --memory 256m \
      --execution-timeout 5s \
      --runtime python37 \
      --entrypoint index.handler \
      --service-account-id $SERVICE_ACCOUNT_ID \
      --source-path index.py 
  Успешное выполнение команды приведёт к созданию версии функции. С помощью консоли управления убедитесь, что версия создана.
  Вызов функции
  Получите список функций, а затем — информацию о функции my-first-function:
  yc serverless function list
  yc serverless function version list --function-name my-first-function 
  В результате вызова последней команды из столбца FUNCTION ID вы узнаете идентификатор функции и сможете сделать вызов функции с помощью следующей команды:
  yc serverless function invoke <идентификатор_функции> 
  По умолчанию функция создаётся непубличной. Чтобы сделать функцию my-first-function публичной, выполните следующую команду:
  yc serverless function allow-unauthenticated-invoke my-first-function 
  После этого вы сможете вызвать её в браузере. Получите параметр http_invoke_url для функции my-first-function:
  yc serverless function get my-first-function 
  Введите значение параметра http_invoke_url в браузере и наслаждайтесь вызовом вашей функции.
  Decision:
  $ sudo apt install jq
  $ export SERVICE_ACCOUNT=$(yc iam service-account create \
    --name service-account-for-cf \
    --description "service account for cloud functions" \
    --format json | jq -r .) 
  $ yc iam service-account list
  $ echo $SERVICE_ACCOUNT
  $ echo "export SERVICE_ACCOUNT_ID=YOUR-ID" >> ~/.bashrc && . ~/.bashrc
  $ echo $SERVICE_ACCOUNT_ID
  $ echo "export FOLDER_ID=$(yc config get folder-id)" >> ~/.bashrc && . ~/.bashrc
  $ echo $FOLDER_ID
  YOUR-ID1
  $ yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_ID \
    --role editor 
  $ yc serverless function create --name my-first-function
  $ vim index.py
  $ cat index.py
  def handler(event, context):
      return {
          'statusCode': 200,
          'body': 'Hello World!',
      }
  $ yc serverless function version create \
      --function-name my-first-function \
      --memory 256m \
      --execution-timeout 5s \
      --runtime python37 \
      --entrypoint index.handler \
      --service-account-id $SERVICE_ACCOUNT_ID \
      --source-path index.py
  $ yc serverless function list
  $ yc serverless function version list --function-name my-first-function
  $ yc serverless function invoke YOUR-ID2
  $ yc serverless function allow-unauthenticated-invoke my-first-function
  $ yc serverless function get my-first-function
  Task:
  Создание триггера от Object Storage.
  В предыдущем практическом уроке вы познакомились с созданием одной функции с помощью интерфейса командной строки (yc). 
  В этом уроке мы продолжим разработку этой функции: модифицируем её содержание, добавим переменные окружения и т.д.
  Decision:
  Шаг 1. Модификация сервисного аккаунта
  Добавление роли сервисному аккаунту
  По итогам прохождения предыдущей практической работы у вас есть сервисный аккаунт с именем service-account-for-cf. Для работы с Object Storage добавьте этому сервисному аккаунту роль storage.editor:
  yc resource-manager folder add-access-binding $FOLDER_ID \
      --role storage.editor \
      --subject serviceAccount:$SERVICE_ACCOUNT_ID 
  Создание ключа доступа для сервисного аккаунта
  Этот этап нужен для получения идентификатора ключа доступа и секретного ключа, которые будут использованы для загрузки файлов в Object Storage, а также в том случае, если на следующем шаге для создания бакета в Object Storage вы планируете использовать Terraform.
  Для создания ключа доступа необходимо вызвать следующую команду:
  yc iam access-key create --service-account-name service-account-for-cf 
  В результате вы получите примерно следующее:
  access_key:
      id: ajefraollq5puj2tir1o
      service_account_id: ajetdv28pl0a1a8r41f0
      created_at: "2021-08-23T21:13:05.677319393Z"
      key_id: BTPNvWthv0ZX2xVmlPIU
  secret: cWLQ0HrTM0k_qAac43cwMNJA8VV_rfTg_kd4xVPi 
  Где: key_id — идентификатор ключа доступа, ACCESS_KEY. secret — секретный ключ, SECRET_KEY.
  Переменные ACCESS_KEY и SECRET_KEY будут использованы для задания соответствующих значений aws_access_key_id и aws_secret_access_key при использовании библиотеки boto3 на следующих этапах.
  Шаг 2. Object Storage
  Самый простой способ создания бакета в Object Storage — через консоль управления. Более сложный, позволяющий автоматизировать разработку, — использование Terraform. Вы можете выбрать любой из них.
  Способ 1. Консоль управления
  В консоли управления в вашем рабочем каталоге выберите сервис Object Storage. Нажмите кнопку Создать бакет.
  На странице создания бакета: Введите имя бакета, пусть это будет bucket-for-trigger. При необходимости ограничьте максимальный размер бакета, установив значение, например, 1 ГБ. Выберите тип доступа, в нашем уроке установим значения в Публичный во всех случаях. Выберите класс хранилища, по умолчанию используется Стандартное.
  Нажмите кнопку Создать бакет для завершения операции. Далее вы всегда сможете поменять класс хранилища, его размер и настройки доступа.
  Способ 2. Terraform
  Прежде всего необходимо получить OAuth-токен для работы с Yandex Cloud. Для этого можно сделать запрос к сервису Яндекс.OAuth. Подробнее прочитать можно в документации.
  Сохраните OAuth-токен в переменную OAuth, но никому не передавайте. Также вам потребуются значения переменных: идентификатор облака — CLOUD_ID и идентификатор каталога FOLDER_ID (сохранен в переменную ранее).
  Также на предыдущем шаге вы получили ключ доступа для сервисного аккаунта. Нам потребуется идентификатор ключа доступа ACCESS_KEY и секретный ключ SECRET_KEY.
  В файл main.tf, представленный далее, внесём все собранные переменные. Важно: переменная BUCKET_NAME содержит имя создаваемого бакета в Object Storage, куда будем загружать файлы. Допустим, переменная будет равна bucket-for-trigger. Сохраним все значения:
  terraform {
    required_providers {
      yandex = {
        source = "yandex-cloud/yandex"
      }
    }
    required_version = ">= 0.13"
  }
  provider "yandex" {
    token     = "<OAuth>"
    cloud_id  = "<CLOUD_ID>"
    folder_id = "<FOLDER_ID>"
  }
  resource "yandex_storage_bucket" "bucket" {
    access_key = "<ACCESS_KEY>"
    secret_key = "<SECRET_KEY>"
    bucket = "<BUCKET_NAME>"
  } 
  После внесения правок, находясь в каталоге с файлом main.tf, последовательно выполните следующие команды:
  terraform init
  terraform plan
  terraform apply 
  Успешное выполнение команд приведёт к созданию бакета bucket-for-trigger в объектном хранилище в вашем рабочем каталоге.
  Шаг 3. Модификация функции
  В предыдущей практической работе мы создали функцию с именем my-first-function с помощью следующей команды:
  yc serverless function create --name my-first-function 
  При создании функции вы получили URL, по которому можно будет сделать вызов функции http_invoke_url.
  Загрузка кода новой версии
  Новая версия функции имеет зависимости, которые описаны в файле requirements.txt, а это значит, что для загрузки функции в облако необходимо файлы index.py и requirements.txt заархивировать и получить файл my-first-function.zip.
  Новая версия index.py:
  import os
  import datetime
  import boto3
  import pytz
  ACCESS_KEY = os.getenv("ACCESS_KEY")
  SECRET_KEY = os.getenv("SECRET_KEY")
  BUCKET_NAME = os.getenv("BUCKET_NAME")
  TIME_ZONE = os.getenv("TIME_ZONE", "Europe/Moscow")
  TEMP_FILENAME = "/tmp/temp_file"
  TEXT_FOR_TEMP_FILE = "This is text file"
  def write_temp_file():
      temp_file = open(TEMP_FILENAME, 'w')
      temp_file.write(TEXT_FOR_TEMP_FILE)
      temp_file.close()
      print("\U0001f680 Temp file is written")
  def get_now_datetime_str():
      now = datetime.datetime.now(pytz.timezone(TIME_ZONE))    
      return now.strftime('%Y-%m-%d__%H-%M-%S')
  def get_s3_instance():
      session = boto3.session.Session()
      return session.client(
          aws_access_key_id=ACCESS_KEY,
          aws_secret_access_key=SECRET_KEY,
          service_name='s3',
          endpoint_url='https://storage.yandexcloud.net'
      )
  def upload_dump_to_s3():
      print("\U0001F4C2 Starting upload to Object Storage")
      get_s3_instance().upload_file(
          Filename=TEMP_FILENAME,
          Bucket=BUCKET_NAME,
          Key=f'file-{get_now_datetime_str()}.txt'
      )
      print("\U0001f680 Uploaded")
  def remove_temp_files():
      os.remove(TEMP_FILENAME)
      print("\U0001F44D That's all!")
  def handler(event, context):
      write_temp_file()
      upload_dump_to_s3()
      remove_temp_files()
      return {
          'statusCode': 200,
          'body': 'File is uploaded',
      } 
  Первая версия requirements.txt:
  boto3==1.13.10
  botocore==1.16.10
  python-dateutil==2.8.1
  pytz==2020.1 
  Находясь в каталоге с файлом my-first-function.zip вызовите следующую команду, это позволит вам загрузить код функции в облако и создать её версию:
  yc serverless function version create \
    --function-name my-first-function \
    --memory 256m \
    --execution-timeout 5s \
    --runtime python37 \
    --entrypoint index.handler \
    --service-account-id $SERVICE_ACCOUNT_ID \
    --source-path my-first-function.zip 
  Новая версия функции при вызове будет загружать в Object Storage новый файл. Для создания этой версии необходимо подготовить несколько переменных. Переменные ACCESS_KEY и SECRET_KEY вы получили на первом шаге, а значение BUCKET_NAME на втором:
  echo "export ACCESS_KEY=<ACCESS_KEY>" >> ~/.bashrc && . ~/.bashrc
  echo "export SECRET_KEY=<SECRET_KEY>" >> ~/.bashrc && . ~/.bashrc
  echo "export BUCKET_NAME=bucket-for-trigger" >> ~/.bashrc && . ~/.bashrc 
  Определим идентификатор (ID) для последней загруженной версии функции:
  yc serverless function version list --function-name my-first-function 
  Создадим новую версию функции, задав при этом переменные окружения. Для этого выставим значение параметра source-version-id равное полученному ID в следующей команде:
  yc serverless function version create \
    --function-name my-first-function \
    --memory 256m \
    --execution-timeout 5s \
    --runtime python37 \
    --entrypoint index.handler \
    --service-account-id $SERVICE_ACCOUNT_ID \
    --source-version-id <ID> \
    --environment ACCESS_KEY=$ACCESS_KEY \
    --environment SECRET_KEY=$SECRET_KEY \
    --environment BUCKET_NAME=$BUCKET_NAME 
  Успешное выполнение команды приведёт к созданию версии функции.
  Вызов функции
  Получите список функций и информацию о функции my-first-function:
  yc serverless function list
  yc serverless function version list --function-name my-first-function 
  В результате вызова последней команды в столбце FUNCTION ID вы узнаете идентификатор функции и сможете сделать вызов функции с помощью следующей команды:
  yc serverless function invoke <идентификатор_функции> 
  В предыдущей практической работе мы сделали функцию my-first-function публичной с помощью команды:
  yc serverless function allow-unauthenticated-invoke my-first-function 
  Теперь мы можем сделать её вызов в браузере. Получите параметр http_invoke_url для функции my-first-function
  yc serverless function get my-first-function 
  Введите значение параметра http_invoke_url в браузере и наслаждайтесь вызовом вашей функции. Во время её вызова в Object Storage будет создан новый файл.
  Шаг 4. Создание триггера
  Создание функции
  Для создания триггера нам необходима функция, которую триггер будет запускать. Аналогично предыдущему шагу создадим функцию my-trigger-function и её версию на основе файла index.py.
  def handler(event, context):
      print("\U0001F4C2 Starting function after trigger")
      print(event)     
      return {
          'statusCode': 200,
          'body': 'File is uploaded',
      } 
  Находясь в каталоге с файлом index.py, вызовите следующие команды:
  yc serverless function create --name my-trigger-function
  yc serverless function version create \
    --function-name my-trigger-function \
    --memory 256m \
    --execution-timeout 5s \
    --runtime python37 \
    --entrypoint index.handler \
    --service-account-id $SERVICE_ACCOUNT_ID \
    --source-path index.py
  yc serverless function version list --function-name my-trigger-function 
  Создание триггера
  Чтобы создать триггер my-first-trigger, который вызывает функцию my-trigger-function при создании нового объекта в бакете BUCKET_NAME, выполните команду:
  yc serverless trigger create object-storage \
    --name my-first-trigger \
    --bucket-id $BUCKET_NAME \
    --events 'create-object' \
    --invoke-function-name my-trigger-function \
    --invoke-function-service-account-id $SERVICE_ACCOUNT_ID 
  Вызов цепочки событий
  Чтобы запустить цепочку событий, вызовем первую функцию my-first-function. Получите список функций и информацию о функции my-first-function:
  yc serverless function list
  yc serverless function version list --function-name my-first-function 
  В результате вызова последней команды в столбце FUNCTION ID вы узнаете идентификатор функции и сможете сделать вызов функции с помощью команды:
  yc serverless function invoke <идентификатор_функции> 
  После этого вы можете сделать её вызов в браузере. Получите параметр http_invoke_url для функции my-first-function
  yc serverless function get my-first-function 
  Введите значение параметра http_invoke_url в браузере. Во время вызова функции в Object Storage будет создан новый объект. Сразу после этого сработает триггер my-first-trigger, который вызовет функцию my-trigger-function. В итоге, наша вторая функция запишет в логи содержание переменной event. Убедиться в этом вы сможете как в UI, так и через CLI.
  yc serverless function logs my-trigger-function 
  Decision:
  $ yc resource-manager folder add-access-binding $FOLDER_ID \
      --role storage.editor \
      --subject serviceAccount:$SERVICE_ACCOUNT_ID
  $ yc iam access-key create --service-account-name service-account-for-cf
  $ vim main.tf
  $ cat main.tf
  terraform {
    required_providers {
      yandex = {
        source = "yandex-cloud/yandex"
      }
    }
    required_version = ">= 0.13"
  }
  provider "yandex" {
    token     = "YOUR-KEY"
    cloud_id  = "YOUR-ID"
    folder_id = "YOUR-ID1"
  }
  resource "yandex_storage_bucket" "bucket" {
    access_key = "YOUR-KEY1"
    secret_key = "YOUR-KEY2"
    bucket = "YOUR-NAME"
  }
  $ yc serverless function create --name my-first-function
  $ vim index.py
  $ cat index.py
  import os
  import datetime
  import boto3
  import pytz
  ACCESS_KEY = os.getenv("ACCESS_KEY")
  SECRET_KEY = os.getenv("SECRET_KEY")
  BUCKET_NAME = os.getenv("BUCKET_NAME")
  TIME_ZONE = os.getenv("TIME_ZONE", "Europe/Moscow")
  TEMP_FILENAME = "/tmp/temp_file"
  TEXT_FOR_TEMP_FILE = "This is text file"
  def write_temp_file():
      temp_file = open(TEMP_FILENAME, 'w')
      temp_file.write(TEXT_FOR_TEMP_FILE)
      temp_file.close()
      print("\U0001f680 Temp file is written")
  def get_now_datetime_str():
      now = datetime.datetime.now(pytz.timezone(TIME_ZONE))
      return now.strftime('%Y-%m-%d__%H-%M-%S')
  def get_s3_instance():
      session = boto3.session.Session()
      return session.client(
          aws_access_key_id=ACCESS_KEY,
          aws_secret_access_key=SECRET_KEY,
          service_name='s3',
          endpoint_url='https://storage.yandexcloud.net'
      )
  def upload_dump_to_s3():
      print("\U0001F4C2 Starting upload to Object Storage")
      get_s3_instance().upload_file(
          Filename=TEMP_FILENAME,
          Bucket=BUCKET_NAME,
          Key=f'file-{get_now_datetime_str()}.txt'
      )
      print("\U0001f680 Uploaded")
  def remove_temp_files():
      os.remove(TEMP_FILENAME)
      print("\U0001F44D That's all!")
  def handler(event, context):
      write_temp_file()
      upload_dump_to_s3()
      remove_temp_files()
      return {
          'statusCode': 200,
          'body': 'File is uploaded',
      }
  $ vim requirements.txt
  $ cat requirements.txt
  boto3==1.13.10
  botocore==1.16.10
  python-dateutil==2.8.1
  pytz==2020.1boto3==1.13.10
  botocore==1.16.10
  python-dateutil==2.8.1
  pytz==2020.1
  $ zip my-first-function.zip requirements.txt index.py
  $ yc serverless function version create \
    --function-name my-first-function \
    --memory 256m \
    --execution-timeout 5s \
    --runtime python37 \
    --entrypoint index.handler \
    --service-account-id $SERVICE_ACCOUNT_ID \
    --source-path my-first-function.zip
  $ echo "export ACCESS_KEY=YOUR-KEY1" >> ~/.bashrc && . ~/.bashrc
  $ echo "export SECRET_KEY=YOUR-KEY2" >> ~/.bashrc && . ~/.bashrc
  $ echo "export BUCKET_NAME=bucket-for-trigger" >> ~/.bashrc && . ~/.bashrc
  $ yc serverless function version list --function-name my-first-function
  $ yc serverless function version create \
    --function-name my-first-function \
    --memory 256m \
    --execution-timeout 5s \
    --runtime python37 \
    --entrypoint index.handler \
    --service-account-id $SERVICE_ACCOUNT_ID \
    --source-version-id YOUR-ID2 \
    --environment ACCESS_KEY=$ACCESS_KEY \
    --environment SECRET_KEY=$SECRET_KEY \
    --environment BUCKET_NAME=$BUCKET_NAME
  $ yc serverless function list
  $ yc serverless function version list --function-name my-first-function
  $ yc serverless function invoke YOUR-ID3
  $ yc serverless function allow-unauthenticated-invoke my-first-function
  $ yc serverless function get my-first-function
  $ vim index1.py
  $ cat index1.py
  import os
  import datetime
  import boto3
  import pytz
  ACCESS_KEY = os.getenv("ACCESS_KEY")
  SECRET_KEY = os.getenv("SECRET_KEY")
  BUCKET_NAME = os.getenv("BUCKET_NAME")
  TIME_ZONE = os.getenv("TIME_ZONE", "Europe/Moscow")
  TEMP_FILENAME = "/tmp/temp_file"
  TEXT_FOR_TEMP_FILE = "This is text file"
  def write_temp_file():
      temp_file = open(TEMP_FILENAME, 'w')
      temp_file.write(TEXT_FOR_TEMP_FILE)
      temp_file.close()
      print("\U0001f680 Temp file is written")
  def get_now_datetime_str():
      now = datetime.datetime.now(pytz.timezone(TIME_ZONE))    
      return now.strftime('%Y-%m-%d__%H-%M-%S')
  def get_s3_instance():
      session = boto3.session.Session()
      return session.client(
          aws_access_key_id=ACCESS_KEY,
          aws_secret_access_key=SECRET_KEY,
          service_name='s3',
          endpoint_url='https://storage.yandexcloud.net'
      )
  def upload_dump_to_s3():
      print("\U0001F4C2 Starting upload to Object Storage")
      get_s3_instance().upload_file(
          Filename=TEMP_FILENAME,
          Bucket=BUCKET_NAME,
          Key=f'file-{get_now_datetime_str()}.txt'
      )
      print("\U0001f680 Uploaded")
  def remove_temp_files():
      os.remove(TEMP_FILENAME)
      print("\U0001F44D That's all!")
  def handler(event, context):
      print("\U0001F4C2 Starting function after trigger")
      print(event)     
      return {
          'statusCode': 200,
          'body': 'File is uploaded',
      }
  $ yc serverless function create --name my-trigger-function
  $ yc serverless function version create \
    --function-name my-trigger-function \
    --memory 256m \
    --execution-timeout 5s \
    --runtime python37 \
    --entrypoint index.handler \
    --service-account-id $SERVICE_ACCOUNT_ID \
    --source-path index.py
  $ yc serverless function version list --function-name my-trigger-function
  $ yc serverless trigger create object-storage \
    --name my-first-trigger \
    --bucket-id $BUCKET_NAME \
    --events 'create-object' \
    --invoke-function-name my-trigger-function \
    --invoke-function-service-account-id $SERVICE_ACCOUNT_ID
  $ yc serverless function list
  $ yc serverless function version list --function-name my-first-function
  $ yc serverless function invoke YOUR-ID3
  $ yc serverless function get my-first-function
  $ yc serverless function logs my-trigger-function
  Task:
  Навык Алисы. 
  В предыдущих практических работах вы создали сервисный аккаунт с именем service-account-for-cf, добавили ему роли editor и storage.editor и создали ключ доступа.
  Также вы создали бакет в Object Storage с именем bucket-for-trigger, триггер my-first-trigger для его обработки и вызываемую им функцию my-trigger-function.
  Ещё была создана функция my-first-function, её использовали для того, чтобы запустить цепочку событий. 
  Публичный вызов этой функции приводил к созданию нового объекта в бакете в Object Storage. 
  Это запускало вызов триггера my-first-trigger, который стартовал функцию my-trigger-function. 
  В итоге последняя функция записывала в логи содержание переменной event.
  Decision:
  Шаг 1. Создание функции
  На предыдущем уроке мы создали функцию с именем my-first-function. Поменяем её исходный код так, чтобы обрабатывать запросы от Алисы.
  На основе функции будет создан навык Попугай, который повторяет все, что ему написал или сказал пользователь.
  Функция parrot. Создадим новую функцию с именем parrot с помощью команды:
  yc serverless function create \
    --name parrot \
    --description "function for Alice" 
  По умолчанию функция не является публичной.
  Загрузка кода новой версии
  Функция имеет зависимости, которые описаны в файле requirements.txt, а это значит, что для загрузки функции в облако необходимо заархивировать файлы parrot.py и requirements.txt и получить файл parrot.zip.
  Содержание функции parrot.py:
  import os
  import datetime
  import boto3
  import pytz
  ACCESS_KEY = os.getenv("ACCESS_KEY")
  SECRET_KEY = os.getenv("SECRET_KEY")
  BUCKET_NAME = os.getenv("BUCKET_NAME")
  TIME_ZONE = os.getenv("TIME_ZONE", "Europe/Moscow")
  TEMP_FILENAME = "/tmp/temp_file"
  TEXT_FOR_TEMP_FILE = "This is text file"
  def write_temp_file(text_for_s3):
      TEXT_FOR_TEMP_FILE = text_for_s3
      temp_file = open(TEMP_FILENAME, 'w')    
      temp_file.write(TEXT_FOR_TEMP_FILE)
      temp_file.close()
      print("\U0001f680 Temp file is written")
  def get_now_datetime_str():
      now = datetime.datetime.now(pytz.timezone(TIME_ZONE))
      return now.strftime('%Y-%m-%d__%H-%M-%S')
  def get_s3_instance():
      session = boto3.session.Session()
      return session.client(
          aws_access_key_id=ACCESS_KEY,
          aws_secret_access_key=SECRET_KEY,
          service_name='s3',
          endpoint_url='https://storage.yandexcloud.net'
      )
  def upload_dump_to_s3():
      print("\U0001F4C2 Starting upload to Object Storage")
      get_s3_instance().upload_file(
          Filename=TEMP_FILENAME,
          Bucket=BUCKET_NAME,
          Key=f'file-{get_now_datetime_str()}.txt'
      )
      print("\U0001f680 Uploaded")
  def remove_temp_files():
      os.remove(TEMP_FILENAME)
      print("\U0001F44D That's all!")
  def handler(event, context):
      """
      Entry-point for Serverless Function.
      :param event: request payload.
      :param context: information about current execution context.
      :return: response to be serialized as JSON.
      """
      text = 'Hello! I\'ll repeat anything you say to me.'
      if 'request' in event and \
              'original_utterance' in event['request'] \
              and len(event['request']['original_utterance']) > 0:
          text = event['request']['original_utterance']
          write_temp_file(text)
          upload_dump_to_s3()
          remove_temp_files()
      return {
          'version': event['version'],
          'session': event['session'],
          'response': {
              # Respond with the original request or welcome the user if this is the beginning of the dialog and the request has not yet been made.
              'text': text,
              # Don't finish the session after this response.
              'end_session': 'false'
          },
      }
  Содержание файла зависимостей requirements.txt:
  boto3==1.13.10
  botocore==1.16.10
  python-dateutil==2.8.1
  pytz==2020.1 
  Находясь в каталоге с файлом parrot.zip, вызовите приведенную ниже команду. Это позволит вам загрузить код функции в облако и создать её версию:
  yc serverless function version create \
    --function-name=parrot \
    --memory=256m \
    --execution-timeout=5s \
    --runtime=python37 \
    --entrypoint=parrot.handler \
    --service-account-id $SERVICE_ACCOUNT_ID \
    --source-path parrot.zip 
  Шаг 2. Создание новой версии функции
  Новая версия функции при вызове будет загружать в Object Storage новый файл. Для создания этой новой версии функции необходимы переменные.
  Если переменные среды не сохранились, то в консоли управления можно посмотреть имя бакета, а ACCESS_KEY и SECRET_KEY скопировать из предыдущей функции my-first-function:
  echo "export ACCESS_KEY=<ACCESS_KEY>" >> ~/.bashrc && . ~/.bashrc
  echo "export SECRET_KEY=<SECRET_KEY>" >> ~/.bashrc && . ~/.bashrc
  echo "export BUCKET_NAME=bucket-for-trigger" >> ~/.bashrc && . ~/.bashrc 
  Определим идентификатор (ID) последней загруженной версии функции:
  yc serverless function version list --function-name parrot 
  Создадим новую версию функции, задав переменные окружения. Для этого выставим значение параметра source-version-id равное полученному идентификатору версии функции (ID) в следующей команде:
  yc serverless function version create \
    --function-name parrot \
    --memory 256m \
    --execution-timeout 5s \
    --runtime python37 \
    --entrypoint parrot.handler \
    --service-account-id $SERVICE_ACCOUNT_ID \
    --source-version-id <идентификатор_версии_функции> \
    --environment ACCESS_KEY=$ACCESS_KEY \
    --environment SECRET_KEY=$SECRET_KEY \
    --environment BUCKET_NAME=$BUCKET_NAME 
  Успешное выполнение команды приведёт к созданию версии функции.
  Шаг 3. Вызов функции и её тестирование
  По умолчанию функция создаётся непубличной. Чтобы сделать функцию parrot публичной, вызовите следующую команду:
  yc serverless function allow-unauthenticated-invoke parrot 
  Протестируйте функцию parrot, чтобы проверить правильность кода перед созданием связки с Алисой. В консоли управления на странице сервиса Cloud Functions выберите созданную функцию и перейдите на вкладку Тестирование. В поле Шаблон данных данных укажите Навык Алисы и нажмите кнопку Запустить тест.
  В блоке Результат тестирования убедитесь, что функция выполнена и приведен ответ.
  Перейдите по ссылке https://dialogs.yandex.ru/developer/ и создайте новый диалог Алисы (подробности о создании навыков вы можете узнать из документации):
  Нажмите кнопку Создать диалог. Выберите тип диалога Навык в Алисе, у вас откроется форма на вкладке Настройки.
  Заполните имя навыка, оно должно состоять минимум из двух слов, например My parrot.
  В блоке Backend выберите вариант Функция в Яндекс.Облаке и в выпадающем списке выберите созданную вами функцию parrot.
  В блоке Тип доступа в выпадающем списке выберите Приватный.
  В блоке Публикация в каталоге выберите Примеры запросов, например Запусти навык - My parrot, Имя разработчика, Категорию, Описание и Иконку.
  Нажмите кнопку Сохранить и перейдите на вкладку Тестирование.
  Если вы сделали всё правильно, то на экране появится приветствие навыка. Далее навык будет повторять всё, что вы ему напишете. При этом фразы, которые вы отправите Алисе, будут сохраняться в новом файле в бакете. Вы можете это проверить в консоли управления.
  Decision:
  $ yc serverless function create \
    --name parrot \
    --description "function for Alice"
  $ vim parrot.py
  $ cat parrot.py
  import os
  import datetime
  import boto3
  import pytz
  ACCESS_KEY = os.getenv("ACCESS_KEY")
  SECRET_KEY = os.getenv("SECRET_KEY")
  BUCKET_NAME = os.getenv("BUCKET_NAME")
  TIME_ZONE = os.getenv("TIME_ZONE", "Europe/Moscow")
  TEMP_FILENAME = "/tmp/temp_file"
  TEXT_FOR_TEMP_FILE = "This is text file"
  def write_temp_file(text_for_s3):
      TEXT_FOR_TEMP_FILE = text_for_s3
      temp_file = open(TEMP_FILENAME, 'w')    
      temp_file.write(TEXT_FOR_TEMP_FILE)
      temp_file.close()
      print("\U0001f680 Temp file is written")
  def get_now_datetime_str():
      now = datetime.datetime.now(pytz.timezone(TIME_ZONE))
      return now.strftime('%Y-%m-%d__%H-%M-%S')
  def get_s3_instance():
      session = boto3.session.Session()
      return session.client(
          aws_access_key_id=ACCESS_KEY,
          aws_secret_access_key=SECRET_KEY,
          service_name='s3',
          endpoint_url='https://storage.yandexcloud.net'
      )
  def upload_dump_to_s3():
      print("\U0001F4C2 Starting upload to Object Storage")
      get_s3_instance().upload_file(
          Filename=TEMP_FILENAME,
          Bucket=BUCKET_NAME,
          Key=f'file-{get_now_datetime_str()}.txt'
      )
      print("\U0001f680 Uploaded")
  def remove_temp_files():
      os.remove(TEMP_FILENAME)
      print("\U0001F44D That's all!")
  def handler(event, context):
      """
      Entry-point for Serverless Function.
      :param event: request payload.
      :param context: information about current execution context.
      :return: response to be serialized as JSON.
      """
      text = 'Hello! I\'ll repeat anything you say to me.'
      if 'request' in event and \
              'original_utterance' in event['request'] \
              and len(event['request']['original_utterance']) > 0:
          text = event['request']['original_utterance']
          write_temp_file(text)
          upload_dump_to_s3()
          remove_temp_files()
      return {
          'version': event['version'],
          'session': event['session'],
          'response': {
              # Respond with the original request or welcome the user if this is the beginning of the dialog and the request has not yet been made.
              'text': text,
              # Don't finish the session after this response.
              'end_session': 'false'
          },
      }
  $ zip parrot.zip parrot.py
  $ zip parrot.zip requirements.txt
  $ yc serverless function version create \
    --function-name=parrot \
    --memory=256m \
    --execution-timeout=5s \
    --runtime=python37 \
    --entrypoint=parrot.handler \
    --service-account-id $SERVICE_ACCOUNT_ID \
    --source-path parrot.zip
  $ echo "export ACCESS_KEY=<ACCESS_KEY>" >> ~/.bashrc && . ~/.bashrc
  $ echo "export SECRET_KEY=<SECRET_KEY>" >> ~/.bashrc && . ~/.bashrc
  $ echo "export BUCKET_NAME=bucket-for-trigger" >> ~/.bashrc && . ~/.bashrc
  $ yc serverless function version list --function-name parrot
  $ yc serverless function version create \
    --function-name parrot \
    --memory 256m \
    --execution-timeout 5s \
    --runtime python37 \
    --entrypoint parrot.handler \
    --service-account-id $SERVICE_ACCOUNT_ID \
    --source-version-id <идентификатор_версии_функции> \
    --environment ACCESS_KEY=$ACCESS_KEY \
    --environment SECRET_KEY=$SECRET_KEY \
    --environment BUCKET_NAME=$BUCKET_NAME
  $ yc serverless function allow-unauthenticated-invoke parrot
  Task:
  Проверка доступности
  На этом практическом занятии вы создадите функцию для проверки доступности сайта yandex.ru, которая будет измерять время ответа. 
  Результаты работы функции будут передаваться в базу данных сервиса Yandex Managed Service for PostgreSQL с использованием подключения к управляемой БД из функции. 
  Также вы запустите триггер-таймер, который будет регулярно производить опрос сайта yandex.ru.
  Decision:
  Шаг 1. Дополнительная роль для сервисного аккаунта
  В предыдущих практических работах вы создали сервисный аккаунт с именем service-account-for-cf, назначили ему роли editor и  storage.editor и создали ключ доступа. Чтобы подключаться к управляемым БД из функции, нужно добавить сервисному аккаунту роль serverless.mdbProxies.user.
  Для этого выполните следующую команду:
  yc resource-manager folder add-access-binding $FOLDER_ID \
    --role serverless.mdbProxies.user \
    --subject serviceAccount:$SERVICE_ACCOUNT_ID 
  Шаг 2. Создание базы данных
  Создание кластера PostgreSQL
  Конечно, кластер PostgreSQL можно создать с помощью консоли управления, но в этой практической работе мы используем CLI. Прежде всего, давайте определим подсеть, в которой будет расположен кластер. Разместим кластер в зоне ru-central1-c и с помощью следующей команды узнаем идентификатор(ID) соответствующей подсети:
  yc vpc subnet list 
  Создадим кластер версии PostgreSQL 13 с именем my-pg-database. Установим тип хоста burstable b2.nano — это самый дешёвый и простой вариант хоста. Из-за невысокой производительности он подходит только для тестовых целей. Используем для хоста жёсткий диск (HDD) размером 10 ГБ.
  Сразу создадим пользователя с именем user1 и паролем user1user1, а также базу данных db1. Для удобства администрирования откроем доступ из консоли управления. Используйте опцию websql-access — это позволит выполнять SQL-запросы прямо в консоли управления. Чтобы открыть возможность подключения к PostgreSQL из функции, необходимо подключить опцию serverless-access.
  Следующая команда за несколько минут создаст кластер PostgreSQL (не забудьте подставить идентификатор вашей подсети):
  yc managed-postgresql cluster create \
    --name my-pg-database \
    --description 'For Serverless' \
    --postgresql-version 13 \
    --environment production \
    --network-name default \
    --resource-preset b2.nano \
    --host zone-id=ru-central1-c,subnet-id=<идентификатор_подсети> \
    --disk-type network-hdd \
    --disk-size 10 \
    --user name=user1,password=user1user1 \
    --database name=db1,owner=user1 \
    --websql-access \
    --serverless-access 
  После успешного создания кластера проверьте результат:
  yc managed-postgresql cluster list
  yc managed-postgresql cluster get <имя или идентификатор кластера> 
  Создание таблицы для хранения данных
  При создании кластера мы использовали опцию websql-access, что открывает нам возможности по исполнению SQL-команд в консоли управления. Воспользуемся этим и сделаем таблицу в созданной нами базе данных. В эту таблицу мы будем складывать результаты выполнения функции. В консоли управления перейдите в каталог, в котором создан кластер PostgreSQL. Откройте сервис Managed Service for PostgreSQL и перейдите в кластер my-pg-database.
  В боковом меню перейдите на вкладку SQL. Для базы данных db1введите имя user1 и пароль user1user1, нажмите кнопку Подключиться.
  В открывшемся окне введите SQL-запрос и исполните его:
  CREATE TABLE measurements (
      result integer,
      time float
  ); 
  Успешное выполнение команды создаст таблицу, куда мы будем складывать результаты.
  Шаг 3. Подключение к управляемой БД из функции
  Создание подключения
  В консоли управления перейдите в каталог, в котором хотите создать подключение. Откройте сервис Cloud Functions. В боковом меню перейдите на вкладку Подключения к БД. Нажмите кнопку Создать подключение.
      Введите имя, описание подключения и в выпадающем списке выберите тип подключения — PostgreSQL.
      Укажите кластер — my-pg-database.
      Укажите базу данных — db1.
      Введите данные пользователя БД: имя user1 и пароль user1user1.
      Нажмите кнопку Создать.
  Выберите созданное подключение. На вкладке Обзор скопируйте параметры Идентификатор и Точка входа. Они будут использованы в функции на следующем шаге.
  Шаг 4. Создание функции
  Перед созданием функции определите переменные для инициации подключения: CONNECTION_ID — идентификатор подключения, DB_USER — имя пользователя БД, DB_HOST — точка входа. Используйте следующие команды:
  echo "export CONNECTION_ID=<CONNECTION_ID>" >> ~/.bashrc && . ~/.bashrc
  echo "export DB_USER=<DB_USER>" >> ~/.bashrc && . ~/.bashrc
  echo "export DB_HOST=<DB_HOST>" >> ~/.bashrc && . ~/.bashrc 
  Они будут использованы в функции function-for-postgresql.py. Код функции:
  import datetime
  import logging
  import requests
  import os
  #Эти библиотеки нужны для работы с PostgreSQL
  import psycopg2
  import psycopg2.errors
  CONNECTION_ID = os.getenv("CONNECTION_ID")
  DB_USER = os.getenv("DB_USER")
  DB_HOST = os.getenv("DB_HOST")
  # Настраиваем функцию для записи информации в журнал функции
  # Получаем стандартный логер языка Python
  logger = logging.getLogger()
  logger.setLevel(logging.INFO)
  # Вычитываем переменную VERBOSE_LOG, которую мы указываем в переменных окружения 
  verboseLogging = eval(os.environ['VERBOSE_LOG'])  ## Convert to bool
  #Функция log, которая запишет текст в журнал выполнения функции, если в переменной окружения VERBOSE_LOG будет значение True
  def log(logString):
      if verboseLogging:
          logger.info(logString)
  #Запись в базу данных
  def save(result, time, context):
      connection = psycopg2.connect(
          database=CONNECTION_ID, # Идентификатор подключения
          user=DB_USER, # Пользователь БД
          password=context.token["access_token"],
          host=DB_HOST, # Точка входа
          port=6432,
          sslmode="require")
      cursor = connection.cursor()    
      postgres_insert_query = """INSERT INTO measurements (result, time) VALUES (%s,%s)"""
      record_to_insert = (result, time)
      cursor.execute(postgres_insert_query, record_to_insert)
      connection.commit()
  # Это обработчик. Он будет вызван первым при запуске функции
  def entry(event, context):
      #Выводим в журнал значения входных параметров event и context
      log(event)
      log(context)
      # Тут мы запоминаем текущее время, отправляем запрос к yandex.ru и вычисляем время выполнения запроса
      try:
          now = datetime.datetime.now()
          #здесь указано два таймаута: 1c для установки связи с сервисом и 3 секунды на получение ответа
          response = requests.get('https://yandex.ru', timeout=(1.0000, 3.0000))
          timediff = datetime.datetime.now() - now
          #сохраняем результат запроса
          result = response.status_code
      #если в процессе запроса сработали таймауты, то в результат записываем соответствующие коды
      except requests.exceptions.ReadTimeout:
          result = 601
      except requests.exceptions.ConnectTimeout:
          result = 602
      except requests.exceptions.Timeout:
          result = 603
      log(f'Result: {result} Time: {timediff.total_seconds()}')    
      save(result, timediff.total_seconds(), context)
      #возвращаем результат запроса
      return {
          'statusCode': result,
          'headers': {
              'Content-Type': 'text/plain'
          },
          'isBase64Encoded': False
      } 
  Перейдем в директорию с кодом функции и создадим нашу функцию function-for-postgresql. При этом сразу зададим все необходимые переменные и сервисный аккаунт:
  yc serverless function create \
    --name  function-for-postgresql \
    --description "function for postgresql"
  yc serverless function version create \
    --function-name=function-for-postgresql \
    --memory=256m \
    --execution-timeout=5s \
    --runtime=python37 \
    --entrypoint=function-for-postgresql.entry \
    --service-account-id $SERVICE_ACCOUNT_ID \
    --environment VERBOSE_LOG=True \
    --environment CONNECTION_ID=$CONNECTION_ID \
    --environment DB_USER=$DB_USER \
    --environment DB_HOST=$DB_HOST \
    --source-path function-for-postgresql.py 
  Проверим работоспособность функции:
  yc serverless function version list --function-name function-for-postgresql
  yc serverless function invoke --name function-for-postgresql 
  Успешный вызов функции приведёт к измерению времени ответа сайта и формированию записи в базе данных.
  Шаг 5. Создание триггера
  Создание триггера-таймера
  Проверять доступность сайта лучше в автоматическом режиме через равные промежутки времени. Для этой задачи создайте триггер-таймер. Он будет использовать cron-выражения:
  yc serverless trigger create timer \
    --name trigger-for-postgresql \
    --invoke-function-name function-for-postgresql \
    --invoke-function-service-account-id $SERVICE_ACCOUNT_ID \
    --cron-expression '* * * * ? *' 
  Cron-выражение * * * * ? * означает вызов функции function-for-postgresql один раз в минуту. Успешное выполнение функции раз в минуту будет создавать запись в базе данных, в чём вы можете убедиться, просмотрев записи в таблице.
  Убедились? Поздравляем: вы успешно создали функцию, которая через заданный промежуток времени выполняется по триггеру, чтобы проверить доступность yandex.ru и записать результат проверки в базу данных.
  Удаление триггера-таймера
  После завершения практической работы не забудьте удалить созданный триггер trigger-for-postgresql, иначе он будет продолжать работать:
  yc serverless trigger delete trigger-for-postgresql 
  Decision:
  $ yc resource-manager folder add-access-binding $FOLDER_ID \
    --role serverless.mdbProxies.user \
    --subject serviceAccount:$SERVICE_ACCOUNT_ID
  $ yc vpc subnet list
  $ yc managed-postgresql cluster create \
    --name my-pg-database \
    --description 'For Serverless' \
    --postgresql-version 13 \
    --environment production \
    --network-name default \
    --resource-preset b2.nano \
    --host zone-id=ru-central1-c,subnet-id=<идентификатор_подсети> \
    --disk-type network-hdd \
    --disk-size 10 \
    --user name=user1,password=user1user1 \
    --database name=db1,owner=user1 \
    --websql-access \
    --serverless-access
  $ yc managed-postgresql cluster list
  $ yc managed-postgresql cluster get <имя или идентификатор кластера>
  $ echo "export CONNECTION_ID=<CONNECTION_ID>" >> ~/.bashrc && . ~/.bashrc
  $ echo "export DB_USER=<DB_USER>" >> ~/.bashrc && . ~/.bashrc
  $ echo "export DB_HOST=<DB_HOST>" >> ~/.bashrc && . ~/.bashrc
  $ vim function-for-postgresql.py
  $ cat function-for-postgresql.py
  import datetime
  import logging
  import requests
  import os
  #Эти библиотеки нужны для работы с PostgreSQL
  import psycopg2
  import psycopg2.errors
  CONNECTION_ID = os.getenv("CONNECTION_ID")
  DB_USER = os.getenv("DB_USER")
  DB_HOST = os.getenv("DB_HOST")
  # Настраиваем функцию для записи информации в журнал функции
  # Получаем стандартный логер языка Python
  logger = logging.getLogger()
  logger.setLevel(logging.INFO)
  # Вычитываем переменную VERBOSE_LOG, которую мы указываем в переменных окружения 
  verboseLogging = eval(os.environ['VERBOSE_LOG'])  ## Convert to bool
  #Функция log, которая запишет текст в журнал выполнения функции, если в переменной окружения VERBOSE_LOG будет значение True
  def log(logString):
      if verboseLogging:
          logger.info(logString)
  #Запись в базу данных
  def save(result, time, context):
      connection = psycopg2.connect(
          database=CONNECTION_ID, # Идентификатор подключения
          user=DB_USER, # Пользователь БД
          password=context.token["access_token"],
          host=DB_HOST, # Точка входа
          port=6432,
          sslmode="require")
      cursor = connection.cursor()    
      postgres_insert_query = """INSERT INTO measurements (result, time) VALUES (%s,%s)"""
      record_to_insert = (result, time)
      cursor.execute(postgres_insert_query, record_to_insert)
      connection.commit()
  # Это обработчик. Он будет вызван первым при запуске функции
  def entry(event, context):
      #Выводим в журнал значения входных параметров event и context
      log(event)
      log(context)
      # Тут мы запоминаем текущее время, отправляем запрос к ya.ru и вычисляем время выполнения запроса
      try:
          now = datetime.datetime.now()
          #здесь указано два таймаута: 1c для установки связи с сервисом и 3 секунды на получение ответа
          response = requests.get('https://ya.ru', timeout=(1.0000, 3.0000))
          timediff = datetime.datetime.now() - now
          #сохраняем результат запроса
          result = response.status_code
      #если в процессе запроса сработали таймауты, то в результат записываем соответствующие коды
      except requests.exceptions.ReadTimeout:
          result = 601
      except requests.exceptions.ConnectTimeout:
          result = 602
      except requests.exceptions.Timeout:
          result = 603
      log(f'Result: {result} Time: {timediff.total_seconds()}')    
      save(result, timediff.total_seconds(), context)
      #возвращаем результат запроса
      return {
          'statusCode': result,
          'headers': {
              'Content-Type': 'text/plain'
          },
          'isBase64Encoded': False
      }
  $ yc serverless function create \
    --name  function-for-postgresql \
    --description "function for postgresql"
  $ yc serverless function version create \
    --function-name=function-for-postgresql \
    --memory=256m \
    --execution-timeout=5s \
    --runtime=python37 \
    --entrypoint=function-for-postgresql.entry \
    --service-account-id $SERVICE_ACCOUNT_ID \
    --environment VERBOSE_LOG=True \
    --environment CONNECTION_ID=$CONNECTION_ID \
    --environment DB_USER=$DB_USER \
    --environment DB_HOST=$DB_HOST \
    --source-path function-for-postgresql.py
  $ yc serverless function version list --function-name function-for-postgresql
  $ yc serverless function invoke --name function-for-postgresql
  $ yc serverless trigger create timer \
    --name trigger-for-postgresql \
    --invoke-function-name function-for-postgresql \
    --invoke-function-service-account-id $SERVICE_ACCOUNT_ID \
    --cron-expression '* * * * ? *'
  $ yc serverless trigger delete trigger-for-postgresql
  Task:
  Создание HTTP API с помощью Cloud Functions и API Gateway.
  На предыдущем практическом занятии мы создали простую систему, которая проверяет доступность сайта yandex.ru и измеряет время ответа на запрос. 
  Полученную информацию функция записывала в базу данных PostgreSQL. 
  На этом уроке мы доработаем начатый проект и добавим REST API, который позволит получать до 50 результатов проверки из базы данных.
  Decision:
  Шаг 1. Проверить наличие сервисного аккаунта
  Для работы нам понадобится сервисный аккаунт с именем service-account-for-cf и ролями editor, serverless.mdbProxies.user, который мы создали ранее.
  Шаг 2. Yandex API Gateway
  Создание спецификации
  В рабочем каталоге создадим спецификацию hello-world.yaml:
  openapi: "3.0.0"
  info:
    version: 1.0.0
    title: Test API
  paths:
    /hello:
      get:
        summary: Say hello
        operationId: hello
        parameters:
          - name: user
            in: query
            description: User name to appear in greetings
            required: false
            schema:
              type: string
              default: 'world'
        responses:
          '200':
            description: Greeting
            content:
              'text/plain':
                schema:
                  type: "string"
        x-yc-apigateway-integration:
          type: dummy
          http_code: 200
          http_headers:
            'Content-Type': "text/plain"
          content:
            'text/plain': "Hello, {user}!\n" 
  Мы можем создать API-шлюз с помощью консоли управления, но сейчас воспользуемся CLI.
  Инициализация спецификации
  Чтобы развернуть API-шлюз, используем спецификацию hello-world.yaml:
  yc serverless api-gateway create \
    --name hello-world \
    --spec=hello-world.yaml \
    --description "hello world" 
  В результате успешного создания API-шлюза получим значение параметра domain:
  yc serverless api-gateway list
  yc serverless api-gateway get --name hello-world 
  Скопируем служебный домен, чтобы проверить работоспособность API-шлюза. Вставим его в адресную строку браузера и допишем в конец /hello. Должно получиться следующее:
  https://<идентификатор API Gateway>.apigw.yandexcloud.net/hello 
  Теперь протестируем запрос с параметрами. Добавьте к предыдущему запросу ?user=my_user. Должно получиться следующее:
  https://<идентификатор API Gateway>.apigw.yandexcloud.net/hello?user=my_user 
  В первом случае в окне браузера вы увидите «Hello, world!», во втором «Hello, my_user!».
  Шаг 3. Создание функции
  Работа с библиотеками и переменными
  До этого момента мы использовали рантайм python37, который не требовал явного указания библиотек, но начиная с версии python39, нужно указывать библиотеки явно. Для работы с requirements.txt можно воспользоваться удобной Python-библиотекой pipreqs: чтобы сгенерировать requirements.txt с помощью pipreqs, достаточно указать рабочий каталог. В большинстве интерпретаторов Linux для указания текущего каталога предусмотрена переменная $PWD. Если файл requirements.txt уже существует, актуализируйте его с помощью флага --force, например:
  pip install pipreqs
  pipreqs $PWD --print
  pipreqs $PWD --force 
  Чтобы создать функцию, проверим доступность переменных для инициации подключения CONNECTION_ID, DB_USER, DB_HOST, которые мы создали в предыдущей работе с помощью следующих команд:
  echo "export CONNECTION_ID=<CONNECTION_ID>" >> ~/.bashrc && . ~/.bashrc
  echo "export DB_USER=<DB_USER>" >> ~/.bashrc && . ~/.bashrc
  echo "export DB_HOST=<DB_HOST>" >> ~/.bashrc && . ~/.bashrc 
  Создание функции
  Создадим функцию function-for-user-requests.py:
  import json
  import logging
  import requests
  import os
  #Эти библиотеки нужны для работы с PostgreSQL
  import psycopg2
  import psycopg2.errors
  import psycopg2.extras
  CONNECTION_ID = os.getenv("CONNECTION_ID")
  DB_USER = os.getenv("DB_USER")
  DB_HOST = os.getenv("DB_HOST")
  # Настраиваем функцию для записи информации в журнал функции
  # Получаем стандартный логер языка Python
  logger = logging.getLogger()
  logger.setLevel(logging.INFO)
  # Вычитываем переменную VERBOSE_LOG, которую мы указываем в переменных окружения
  verboseLogging = eval(os.environ['VERBOSE_LOG'])  ## Convert to bool
  #Функция log, которая запишет текст в журнал выполнения функции, если в переменной окружения VERBOSE_LOG будет значение True
  def log(logString):
      if verboseLogging:
          logger.info(logString)
  #Запись в базу данных
  def save(result, time, context):
      connection = psycopg2.connect(
          database=CONNECTION_ID, # Идентификатор подключения
          user=DB_USER, # Пользователь БД
          password=context.token["access_token"],
          host=DB_HOST, # Точка входа
          port=6432,
          sslmode="require")
      cursor = connection.cursor()   
      postgres_insert_query = """INSERT INTO measurements (result, time) VALUES (%s,%s)"""
      record_to_insert = (result, time)
      cursor.execute(postgres_insert_query, record_to_insert)
      connection.commit()
  #Формируем запрос
  def generateQuery():
      select = f"SELECT * FROM measurements LIMIT 50"
      result = select
      return result
  #Получаем подключение
  def getConnString(context):
      """
      Extract env variables to connect to DB and return a db string
      Raise an error if the env variables are not set
      :return: string
      """
      connection = psycopg2.connect(
          database=CONNECTION_ID, # Идентификатор подключения
          user=DB_USER, # Пользователь БД
          password=context.token["access_token"],
          host=DB_HOST, # Точка входа
          port=6432,
          sslmode="require")   
      return connection
  def handler(event, context):
      try:
          secret = event['queryStringParameters']['secret']
          if secret != 'cecfb23c-bc86-4ca2-b611-e79bc77e5c31':
              raise Exception()
      except Exception as error:
          logger.error(error)
          statusCode = 401
          return {
              'statusCode': statusCode
          }
      sql = generateQuery()
      log(f'Exec: {sql}')
      connection = getConnString(context)
      log(f'Connecting: {connection}')
      cursor = connection.cursor()
      try:
          cursor.execute(sql)
          statusCode = 200
          return {
              'statusCode': statusCode,
              'body': json.dumps(cursor.fetchall()),
          }
      except psycopg2.errors.UndefinedTable as error:
          connection.rollback()
          logger.error(error)
          statusCode = 500
      except Exception as error:
          logger.error(error)
          statusCode = 500
      cursor.close()
      connection.close()
      return {
          'statusCode': statusCode,
          'body': json.dumps({
              'event': event,
          }),
      }
  Обратите внимание, в коде функции мы заложили параметр secret и его значение cecfb23c-bc86-4ca2-b611-e79bc77e5c31, при котором функция будет выполняться. Таким образом мы обеспечиваем дополнительную защиту при доступе к БД.
  При создании функции сразу зададим все необходимые переменные и сервисный аккаунт:
  yc serverless function create \
    --name function-for-user-requests \
    --description "function for response to user"
  yc serverless function version create \
    --function-name=function-for-user-requests \
    --memory=256m \
    --execution-timeout=5s \
    --runtime=python37 \
    --entrypoint=function-for-user-requests.handler \
    --service-account-id $SERVICE_ACCOUNT_ID \
    --environment VERBOSE_LOG=True \
    --environment CONNECTION_ID=$CONNECTION_ID \
    --environment DB_USER=$DB_USER \
    --environment DB_HOST=$DB_HOST \
    --source-path function-for-user-requests.py 
  Шаг 4. Обновление спецификации API Gateway
  Наша функция готова, но по умолчанию она не является публичной. Предоставим доступ к этой функции с помощью API-шлюза — обновим ранее созданную спецификацию hello-world.yaml. Не забудьте вставить в файл идентификаторы вашей функции и вашего сервисного аккаунта:
  openapi: "3.0.0"
  info:
    version: 1.0.0
    title: Updated API
  paths:
    /results:
      get:
        x-yc-apigateway-integration:
          type: cloud-functions
          function_id: <идентификатор функции>
          service_account_id: <идентификатор сервисного аккаунта>
        operationId: function-for-user-requests 
  Вызовем перезагрузку нашей спецификации:
  yc serverless api-gateway update \
    --name hello-world \
    --spec=hello-world.yaml 
  Для тестирования вызовем функцию в браузере сначала без параметра secret, а затем — с ним:
  https://<идентификатор API Gateway>.apigw.yandexcloud.net/results
  https://<идентификатор API Gateway>.apigw.yandexcloud.net/results?secret=cecfb23c-bc86-4ca2-b611-e79bc77e5c31 
  В ответе увидим результаты тестирования сервиса yandex.ru из базы данных.
  Иногда приходится тестировать функцию в процессе разработки: для этого в консоли управления на странице функции перейдите на вкладку Тестирование, в поле Шаблон данных выберите HTTPS-вызов. Нажмите кнопку Запустить тест, и вы увидите код ошибки.
  Код функции проверяет параметр secret для авторизации, то есть при вызове вы должны передать секретную последовательность, чтобы функция выдала результат. Добавим secret в параметры запроса в поле Входные данные:
      "queryStringParameters": {
          "a": "2",
          "b": "1",
          "secret": "cecfb23c-bc86-4ca2-b611-e79bc77e5c31"
      }, 
  Запустим тест ещё раз. В ответе отобразятся данные из базы, как и с запросами через браузер.
  Decision:
  $ vim hello-world.yaml
  $ cat hello-world.yaml
  openapi: "3.0.0"
  info:
    version: 1.0.0
    title: Test API
  paths:
    /hello:
      get:
        summary: Say hello
        operationId: hello
        parameters:
          - name: user
            in: query
            description: User name to appear in greetings
            required: false
            schema:
              type: string
              default: 'world'
        responses:
          '200':
            description: Greeting
            content:
              'text/plain':
                schema:
                  type: "string"
        x-yc-apigateway-integration:
          type: dummy
          http_code: 200
          http_headers:
            'Content-Type': "text/plain"
          content:
            'text/plain': "Hello, {user}!\n"
  $ yc serverless api-gateway create \
    --name hello-world \
    --spec=hello-world.yaml \
    --description "hello world"
  $ yc serverless api-gateway list
  $ yc serverless api-gateway get --name hello-world
  $ touch requirements.txt
  $ sudo apt install python3-pip
  $ pip install pipreqs
  $ pipreqs $PWD --print
  $ pipreqs $PWD --force
  $ echo "export CONNECTION_ID=<CONNECTION_ID>" >> ~/.bashrc && . ~/.bashrc
  $ echo "export DB_USER=<DB_USER>" >> ~/.bashrc && . ~/.bashrc
  $ echo "export DB_HOST=<DB_HOST>" >> ~/.bashrc && . ~/.bashrc
  $ vim function-for-user-requests.py
  $ cat function-for-user-requests.py
  import json
  import logging
  import requests
  import os
  #Эти библиотеки нужны для работы с PostgreSQL
  import psycopg2
  import psycopg2.errors
  import psycopg2.extras
  CONNECTION_ID = os.getenv("CONNECTION_ID")
  DB_USER = os.getenv("DB_USER")
  DB_HOST = os.getenv("DB_HOST")
  # Настраиваем функцию для записи информации в журнал функции
  # Получаем стандартный логер языка Python
  logger = logging.getLogger()
  logger.setLevel(logging.INFO)
  # Вычитываем переменную VERBOSE_LOG, которую мы указываем в переменных окружения
  verboseLogging = eval(os.environ['VERBOSE_LOG'])  ## Convert to bool
  #Функция log, которая запишет текст в журнал выполнения функции, если в переменной окружения VERBOSE_LOG будет значение True
  def log(logString):
      if verboseLogging:
          logger.info(logString)
  #Запись в базу данных
  def save(result, time, context):
      connection = psycopg2.connect(
          database=CONNECTION_ID, # Идентификатор подключения
          user=DB_USER, # Пользователь БД
          password=context.token["access_token"],
          host=DB_HOST, # Точка входа
          port=6432,
          sslmode="require")
      cursor = connection.cursor()   
      postgres_insert_query = """INSERT INTO measurements (result, time) VALUES (%s,%s)"""
      record_to_insert = (result, time)
      cursor.execute(postgres_insert_query, record_to_insert)
      connection.commit()
  #Формируем запрос
  def generateQuery():
      select = f"SELECT * FROM measurements LIMIT 50"
      result = select
      return result
  #Получаем подключение
  def getConnString(context):
      """
      Extract env variables to connect to DB and return a db string
      Raise an error if the env variables are not set
      :return: string
      """
      connection = psycopg2.connect(
          database=CONNECTION_ID, # Идентификатор подключения
          user=DB_USER, # Пользователь БД
          password=context.token["access_token"],
          host=DB_HOST, # Точка входа
          port=6432,
          sslmode="require")   
      return connection
  def handler(event, context):
      try:
          secret = event['queryStringParameters']['secret']
          if secret != 'cecfb23c-bc86-4ca2-b611-e79bc77e5c31':
              raise Exception()
      except Exception as error:
          logger.error(error)
          statusCode = 401
          return {
              'statusCode': statusCode
          }
      sql = generateQuery()
      log(f'Exec: {sql}')
      connection = getConnString(context)
      log(f'Connecting: {connection}')
      cursor = connection.cursor()
      try:
          cursor.execute(sql)
          statusCode = 200
          return {
              'statusCode': statusCode,
              'body': json.dumps(cursor.fetchall()),
          }
      except psycopg2.errors.UndefinedTable as error:
          connection.rollback()
          logger.error(error)
          statusCode = 500
      except Exception as error:
          logger.error(error)
          statusCode = 500
      cursor.close()
      connection.close()
      return {
          'statusCode': statusCode,
          'body': json.dumps({
              'event': event,
          }),
      }
  $ yc serverless function create \
    --name function-for-user-requests \
    --description "function for response to user"
  $ yc serverless function version create \
    --function-name=function-for-user-requests \
    --memory=256m \
    --execution-timeout=5s \
    --runtime=python37 \
    --entrypoint=function-for-user-requests.handler \
    --service-account-id $SERVICE_ACCOUNT_ID \
    --environment VERBOSE_LOG=True \
    --environment CONNECTION_ID=$CONNECTION_ID \
    --environment DB_USER=$DB_USER \
    --environment DB_HOST=$DB_HOST \
    --source-path function-for-user-requests.py
  $ vim hello-world1.yaml
  $ cat hello-world1.yaml
  openapi: "3.0.0"
  info:
    version: 1.0.0
    title: Updated API
  paths:
    /results:
      get:
        x-yc-apigateway-integration:
          type: cloud-functions
          function_id: <идентификатор функции>
          service_account_id: <идентификатор сервисного аккаунта>
        operationId: function-for-user-requests
  $ yc serverless api-gateway update \
    --name hello-world \
    --spec=hello-world.yaml
  Task:
  Загрузка данных, выполнение запросов AWS CLI.
  B предыдущем уроке мы рассмотрели, как работать с YDB через Document API — низкоуровневый HTTP API, совместимый с AWS DynamoDB API.
  В этом уроке рассмотрим операции создания таблицы, записи, чтения, изменения и удаления данных в таблице с помощью AWS CLI.
  Decision:
  Сервисный аккаунт и ключ доступа. Для работы инструментов AWS вам понадобится создать сервисный аккаунт в облаке.
  Выберите вкладку Сервисные аккаунты в каталоге, где расположена БД.
  Нажмите кнопку Создать сервисный аккаунт.
  Введите имя сервисного аккаунта. Чтобы назначить сервисному аккаунту роль на текущий каталог, нажмите Добавить роль и выберите роль, например editor.
  Нажмите кнопку Создать.
  Выберите созданный сервисный аккаунт и нажмите на строку с его именем. Нажмите кнопку Создать новый ключ на верхней панели. Выберите пункт Создать статический ключ доступа.
  Сохраните идентификатор и секретный ключ.
  Работа с AWS CLI. Установите AWS CLI с сайта https://aws.amazon.com/ru/cli/.
  Для Windows: загрузите и запустите 64- или 32-разрядный установщик.
  Для Mac и Linux: установите AWS CLI с помощью утилиты pip (требуется Python 2.6.5 или более поздней версии).
  pip install awscli 
  Для настройки AWS CLI  запустите команду:
  aws configure 
  Введите сохраненные значения идентификатора ключа AWS Access Key ID и ключа AWS Secret Access Key и укажите ru-central1 в качестве Default region name.
  Убедитесь, что в качестве переменной окружения ENDPOINT указано корректное значение эндпойнта вашей базы данных, либо добавьте его, как вы это делали в прошлом уроке: сохраните значение эндпойнта, указанное в строке Document API эндпоинт, в переменной окружения с помощью команды
  export ENDPOINT=<значение endpoint> 
  Создание таблицы. Создайте таблицу с помощью команды:
  aws dynamodb create-table \
    --table-name docapitest/series \
    --attribute-definitions \
    AttributeName=series_id,AttributeType=N \
    AttributeName=title,AttributeType=S \
    --key-schema \
    AttributeName=series_id,KeyType=HASH \
    AttributeName=title,KeyType=RANGE \
    --endpoint $ENDPOINT 
  Убедитесь, что в директории docapitest появилась таблица series.
  Добавление данных в таблицу
  Добавьте в таблицу две строки c помощью команд:
  aws dynamodb put-item \
    --table-name docapitest/series \
    --item '{"series_id": {"N": "1"}, "title": {"S": "IT Crowd"}, "series_info": {"S": "The IT Crowd is a British sitcom produced by Channel 4, written by Graham Linehan, produced by Ash Atalla and starring Chris ODowd, Richard Ayoade, Katherine Parkinson, and Matt Berry."}, "release_date": {"S": "2006-02-03"}}' \
    --endpoint $ENDPOINT 
  и
  aws dynamodb put-item \
    --table-name docapitest/series \
    --item '{"series_id": {"N": "2"}, "title": {"S": "Silicon Valley"}, "series_info": {"S": "Silicon Valley is an American comedy television series created by Mike Judge, John Altschuler and Dave Krinsky."}, "release_date": {"S": "2014-04-06"}}' \
    --endpoint $ENDPOINT 
  Чтение данных из таблицы. Для того чтобы прочитать данные из таблицы, выполните команду:
  aws dynamodb get-item --consistent-read \
    --table-name docapitest/series \
    --key '{"series_id": {"N": "1"}, "title": {"S": "IT Crowd"}}' \
    --endpoint $ENDPOINT 
  В качестве вывода вы увидите:
  {
      "Item": {
          "release_date": {
              "S": "2006-02-03"
          },
          "series_id": {
              "N": "1"
          },
          "series_info": {
              "S": "The IT Crowd is a British sitcom produced by Channel 4, written by Graham Linehan, produced by Ash Atalla and starring Chris ODowd, Richard Ayoade, Katherine Parkinson, and Matt Berry."
          },
          "title": {
              "S": "IT Crowd"
          }
      }
  } 
  Для того, чтобы выбрать данные из таблицы series по ключу series_id, выполните следующую команду:
  aws dynamodb query \
    --table-name docapitest/series \
    --key-condition-expression "series_id = :name" \
    --expression-attribute-values '{":name":{"N":"2"}}' \
    --endpoint $ENDPOINT 
  В качестве результата вы увидите:
  {
      "Items": [
          {
              "release_date": {
                  "S": "2014-04-06"
              },
              "series_id": {
                  "N": "2"
              },
              "series_info": {
                  "S": "Silicon Valley is an American comedy television series created by Mike Judge, John Altschuler and Dave Krinsky."
              },
              "title": {
                  "S": "Silicon Valley"
              }
          }
      ],
      "Count": 1,
      "ScannedCount": 1,
      "ConsumedCapacity": null
  } 
  Удаление таблицы
  aws dynamodb delete-table \
    --table-name docapitest/series \
    --endpoint $ENDPOINT 
  На следующем практическом занятии мы разберём пример использования AWS SDK для работы с YDB в serverless-режиме.
  Decision:
  $ pip install awscli
  $ aws configure
  $ export ENDPOINT=<значение endpoint>
  $ aws dynamodb create-table \
    --table-name docapitest/series \
    --attribute-definitions \
    AttributeName=series_id,AttributeType=N \
    AttributeName=title,AttributeType=S \
    --key-schema \
    AttributeName=series_id,KeyType=HASH \
    AttributeName=title,KeyType=RANGE \
    --endpoint $ENDPOINT
  $ aws dynamodb put-item \
    --table-name docapitest/series \
    --item '{"series_id": {"N": "1"}, "title": {"S": "IT Crowd"}, "series_info": {"S": "The IT Crowd is a British sitcom produced by Channel 4, written by Graham Linehan, produced by Ash Atalla and starring Chris ODowd, Richard Ayoade, Katherine Parkinson, and Matt Berry."}, "release_date": {"S": "2006-02-03"}}' \
    --endpoint $ENDPOINT
  $ aws dynamodb put-item \
    --table-name docapitest/series \
    --item '{"series_id": {"N": "2"}, "title": {"S": "Silicon Valley"}, "series_info": {"S": "Silicon Valley is an American comedy television series created by Mike Judge, John Altschuler and Dave Krinsky."}, "release_date": {"S": "2014-04-06"}}' \
    --endpoint $ENDPOINT
  $ aws dynamodb get-item --consistent-read \
    --table-name docapitest/series \
    --key '{"series_id": {"N": "1"}, "title": {"S": "IT Crowd"}}' \
    --endpoint $ENDPOINT
  $ aws dynamodb query \
    --table-name docapitest/series \
    --key-condition-expression "series_id = :name" \
    --expression-attribute-values '{":name":{"N":"2"}}' \
    --endpoint $ENDPOINT
  $ aws dynamodb delete-table \
    --table-name docapitest/series \
    --endpoint $ENDPOINT
  Task:
  Запуск тестового приложения
  В предыдущем уроке вы прошли подготовительные этапы: создали и настроили сервисный аккаунт, выпустили статический ключ, а также научились работать с таблицами и данными с помощью низкоуровневого API и CLI.
  В этом уроке вы продолжите работу с инструментами AWS и с помощью AWS SDK для языка Python научитесь выполнять такие базовые операции, как создание таблиц БД, запись и чтение данных.
  Decision:
  Для выполнения работы вам понадобится Python версии 3.6 и выше и библиотека boto3.
  Установить эту библиотеку можно с помощью команды:
  pip install boto3 
  Создание таблицы
  Создайте файл с именем SeriesCreateTable.py и скопируйте в него исходный код программы:
  import boto3
  def create_series_table():
      ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
      table = ydb_docapi_client.create_table(
          TableName = 'docapitest/series', # Series — имя таблицы 
          KeySchema = [
              {
                  'AttributeName': 'series_id',
                  'KeyType': 'HASH'  # Ключ партицирования
              },
              {
                  'AttributeName': 'title',
                  'KeyType': 'RANGE'  # Ключ сортировки
              }
          ],
          AttributeDefinitions = [
              {
                  'AttributeName': 'series_id',
                  'AttributeType': 'N'  # Целое число
              },
              {
                  'AttributeName': 'title',
                  'AttributeType': 'S'  # Строка
              },
          ]
      )
      return table
  if __name__ == '__main__':
      series_table = create_series_table()
      print("Table status:", series_table.table_status) 
  Отредактируйте исходный код файла и укажите значение endpoint_url вашей базы. Затем запустите написанный код:
  python SeriesCreateTable.py 
  С помощью консоли управления убедитесь, что  в директории docapitest появилась таблица series.
  Первоначальная загрузка данных
  Для того чтобы вставить данные в созданную таблицу series, создайте файл с именем SeriesLoadData.py и скопируйте в него следующий исходный код программы:
  from decimal import Decimal
  import json
  import boto3
  def load_series(series):
      ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document API эндпоинт>")
      table = ydb_docapi_client.Table('docapitest/series')
      for serie in series:
          series_id = int(serie['series_id'])
          title = serie['title']
          print("Series added:", series_id, title)
          table.put_item(Item = serie)
  if __name__ == '__main__':
      with open("seriesdata.json") as json_file:
          serie_list = json.load(json_file, parse_float = Decimal)
      load_series(serie_list) 
  Отредактируйте файл SeriesLoadData.py и укажите значение endpoint_url вашей базы.
  Для загрузки данных приложение будет использовать данные, которые записаны в файл seriesdata.json. Создайте этот файл и скопируйте в него описание сериалов:
  [{
      "series_id": 1,
      "title": "IT Crowd",
      "info": {
        "release_date": "2006-02-03T00:00:00Z",
        "series_info": "The IT Crowd is a British sitcom produced by Channel 4, written by Graham Linehan, produced by Ash Atalla and starring Chris O'Dowd, Richard Ayoade, Katherine Parkinson, and Matt Berry"
      }
    },
    {
      "series_id": 2,
      "title": "Silicon Valley",
      "info": {
        "release_date": "2014-04-06T00:00:00Z",
        "series_info": "Silicon Valley is an American comedy television series created by Mike Judge, John Altschuler and Dave Krinsky. The series focuses on five young men who founded a startup company in Silicon Valley"
      }
    },
    {
      "series_id": 3,
      "title": "House of Cards",
      "info": {
        "release_date": "2013-02-01T00:00:00Z",
        "series_info": "House of Cards is an American political thriller streaming television series created by Beau Willimon. It is an adaptation of the 1990 BBC miniseries of the same name and based on the 1989 novel of the same name by Michael Dobbs"
      }
    },
    {
      "series_id": 3,
      "title": "The Office",
      "info": {
        "release_date": "2005-03-24T00:00:00Z",
        "series_info": "The Office is an American mockumentary sitcom television series that depicts the everyday work lives of office employees in the Scranton, Pennsylvania, branch of the fictional Dunder Mifflin Paper Company"
      }
    },
    {
      "series_id": 3,
      "title": "True Detective",
      "info": {
        "release_date": "2014-01-12T00:00:00Z",
        "series_info": "True Detective is an American anthology crime drama television series created and written by Nic Pizzolatto. The series, broadcast by the premium cable network HBO in the United States, premiered on January 12, 2014"
      }
    },
    {
      "series_id": 4,
      "title": "The Big Bang Theory",
      "info": {
        "release_date": "2007-09-24T00:00:00Z",
        "series_info": "The Big Bang Theory is an American television sitcom created by Chuck Lorre and Bill Prady, both of whom served as executive producers on the series, along with Steven Molaro"
      }
    },
    {
      "series_id": 5,
      "title": "Twin Peaks",
      "info": {
        "release_date": "1990-04-08T00:00:00Z",
        "series_info": "Twin Peaks is an American mystery horror drama television series created by Mark Frost and David Lynch that premiered on April 8, 1990, on ABC until its cancellation after its second season in 1991 before returning as a limited series in 2017 on Showtime"
      }
    }
  ] 
  Запустите программу (для её успешного выполнения может понадобиться указать в скрипте SeriesLoadData.py полный путь к файлу seriesdata.json):
  python SeriesLoadData.py 
  В результате выполнения вы увидите вывод программы:
  Series added: 1 IT Crowd
  Series added: 2 Silicon Valley
  Series added: 3 House of Cards
  Series added: 3 The Office
  Series added: 3 True Detective
  Series added: 4 The Big Bang Theory
  Series added: 5 Twin Peaks 
  Работа с записями
  Создание записи
  Теперь создайте файл SeriesItemPut.py и скопируйте в него следующий код:
  from pprint import pprint
  import boto3
  def put_serie(series_id, title, release_date, series_info):
      ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
      table = ydb_docapi_client.Table('docapitest/series')
      response = table.put_item(
        Item = {
              'series_id': series_id,
              'title': title,
              'info': {
                  'release_date': release_date,
                  'series_info': series_info
              }
          }
      )
      return response
  if __name__ == '__main__':
      serie_resp = put_serie(3, "Supernatural", "2015-09-13",
                            "Supernatural is an American television series created by Eric Kripke")
      print("Series added successfully:")
      pprint(serie_resp, sort_dicts = False) 
  В результате выполнения этого кода в таблице добавится запись о сериале Supernatural.
  Чтение записи
  Создайте файл SeriesItemGet.py и скопируйте в него следующий код:
  from pprint import pprint
  import boto3
  from botocore.exceptions import ClientError
  def get_serie(title, series_id):
      ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
      table = ydb_docapi_client.Table('docapitest/series')
      try:
          response = table.get_item(Key = {'series_id': series_id, 'title': title})
      except ClientError as e:
          print(e.response['Error']['Message'])
      else:
          return response['Item']
  if __name__ == '__main__':
      serie = get_serie("Supernatural", 3,)
      if serie:
          print("Record read:")
          pprint(serie, sort_dicts = False) 
  Результатом будет сообщение в формате JSON с данными о сериале.
  Обновление записи
  В файле SeriesItemUpdate.py разместите код обновления записи:
  from decimal import Decimal
  from pprint import pprint
  import boto3
  def update_serie(title, series_id, release_date,  rating):
      ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
      table = ydb_docapi_client.Table('docapitest/series')
      response = table.update_item(
          Key = {
              'series_id': series_id,
              'title': title
          },
          UpdateExpression = "set info.release_date = :d, info.rating = :r ",
          ExpressionAttributeValues = {
              ':d': release_date,
              ':r': Decimal(rating)
          },
          ReturnValues = "UPDATED_NEW"
      )
      return response
  if __name__ == '__main__':
      update_response = update_serie(
          "Supernatural", 3, "2005-09-13", 8)
      print("Series updated:")
      pprint(update_response, sort_dicts = False) 
  Результатом будет сообщение в формате JSON с измененными данными.
  Удаление записи
  Создайте файл SeriesItemDelete.py и скопируйте в него следующий код:
  from decimal import Decimal
  from pprint import pprint
  import boto3
  from botocore.exceptions import ClientError
  def delete_underrated_serie(title, series_id, rating):
      ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
      table = ydb_docapi_client.Table('docapitest/series')
      try:
          response = table.delete_item(
              Key = {
                  'series_id': series_id,
                  'title': title
              },
              ConditionExpression = "info.rating <= :val",
              ExpressionAttributeValues = {
                  ":val": Decimal(rating)
              }
          )
      except ClientError as e:
          if e.response['Error']['Code'] == "ConditionalCheckFailedException":
              print(e.response['Error']['Message'])
          else:
              raise
      else:
          return response
  if __name__ == '__main__':
      print("Deleting...")
      delete_response = delete_underrated_serie("Supernatural", 3, 8)
      if delete_response:
          print("Series data deleted:")
          pprint(delete_response, sort_dicts = False) 
  Убедитесь, что данные о сериале Supernatural удалены из таблицы.
  Поиск по ключам партицирования и сортировки
  Код поиска разместите в новом файле SeriesQuery.py:
  from pprint import pprint
  import boto3
  from boto3.dynamodb.conditions import Key
  def query_and_project_series(series_id, title_range):
      ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
      table = ydb_docapi_client.Table('docapitest/series')
      response = table.query(
          ProjectionExpression = "series_id, title, info.release_date",
          KeyConditionExpression = Key('series_id').eq(series_id) & Key('title').begins_with(title_range)
      )
      return response['Items']
  if __name__ == '__main__':
      query_id = 3
      query_range = 'T'
      print(f"Series with ID = {query_id} and names beginning with "
            f"{query_range}")
      series = query_and_project_series(query_id, query_range)
      for serie in series:
          print(f"\n{serie['series_id']} : {serie['title']}")
          pprint(serie['info']) 
  Результатом будет сообщение:
  Series with ID = 3 and names beginning with T
  3 : The Office
  {'release_date': '2005-03-24T00:00:00Z'}
  3 : True Detective
  {'release_date': '2014-01-12T00:00:00Z'} 
  Запуск операции Scan
  Создайте файл SeriesTableScan.py и скопируйте в него следующий код:
  from pprint import pprint
  import boto3
  from boto3.dynamodb.conditions import Key
  def scan_series(id_range, display_series):
      ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
      table = ydb_docapi_client.Table('docapitest/series')
      scan_kwargs = {
          'FilterExpression': Key('series_id').between(*id_range),
          'ProjectionExpression': "series_id, title, info.release_date"
      }
      done = False
      start_key = None
      while not done:
          if start_key:
              scan_kwargs['ExclusiveStartKey'] = start_key
          response = table.scan(**scan_kwargs)
          display_series(response.get('Items', []))
          start_key = response.get('LastEvaluatedKey', None)
          done = start_key is None
  if __name__ == '__main__':
      def print_series(series):
          for serie in series:
              print(f"\n{serie['series_id']} : {serie['title']}")
              pprint(serie['info'])
      query_range = (1, 3)
      print(f"Series with IDs from {query_range[0]} to {query_range[1]}...")
      scan_series(query_range, print_series) 
  Результатом будет сообщение:
  Series with IDs from 1 to 3...
  3 : House of Cards
  {'release_date': '2013-02-01T00:00:00Z'}
  3 : The Office
  {'release_date': '2005-03-24T00:00:00Z'}
  3 : True Detective
  {'release_date': '2014-01-12T00:00:00Z'}
  1 : IT Crowd
  {'release_date': '2006-02-03T00:00:00Z'}
  2 : Silicon Valley
  {'release_date': '2014-04-06T00:00:00Z'} 
  Удаление таблицы
  Создайте файл SeriesTableDelete.py и скопируйте в него следующий код:
  import boto3
  def delete_serie_table():
      ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
      table = ydb_docapi_client.Table('docapitest/series')
      table.delete()
  if __name__ == '__main__':
      delete_serie_table()
      print("Table Series deleted") 
  Убедитесь, что таблица удалена из базы данных.
  Decision:
  $ pip install boto3
  $ vim SeriesCreateTable.py
  $ cat SeriesCreateTable.py
  import boto3
  def create_series_table():
      ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
      table = ydb_docapi_client.create_table(
          TableName = 'docapitest/series', # Series — имя таблицы 
          KeySchema = [
              {
                  'AttributeName': 'series_id',
                  'KeyType': 'HASH'  # Ключ партицирования
              },
              {
                  'AttributeName': 'title',
                  'KeyType': 'RANGE'  # Ключ сортировки
              }
          ],
          AttributeDefinitions = [
              {
                  'AttributeName': 'series_id',
                  'AttributeType': 'N'  # Целое число
              },
              {
                  'AttributeName': 'title',
                  'AttributeType': 'S'  # Строка
              },
          ]
      )
      return table
  if __name__ == '__main__':
      series_table = create_series_table()
      print("Table status:", series_table.table_status)
  $ python SeriesCreateTable.py
  $ vim SeriesLoadData.py
  $ cat SeriesLoadData.py
  from decimal import Decimal
  import json
  import boto3
  def load_series(series):
      ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document API эндпоинт>")
      table = ydb_docapi_client.Table('docapitest/series')
      for serie in series:
          series_id = int(serie['series_id'])
          title = serie['title']
          print("Series added:", series_id, title)
          table.put_item(Item = serie)
  if __name__ == '__main__':
      with open("seriesdata.json") as json_file:
          serie_list = json.load(json_file, parse_float = Decimal)
      load_series(serie_list)
  $ vim seriesdata.json
  $ cat seriesdata.json
  [{
      "series_id": 1,
      "title": "IT Crowd",
      "info": {
        "release_date": "2006-02-03T00:00:00Z",
        "series_info": "The IT Crowd is a British sitcom produced by Channel 4, written by Graham Linehan, produced by Ash Atalla and starring Chris O'Dowd, Richard Ayoade, Katherine Parkinson, and Matt Berry"
      }
    },
    {
      "series_id": 2,
      "title": "Silicon Valley",
      "info": {
        "release_date": "2014-04-06T00:00:00Z",
        "series_info": "Silicon Valley is an American comedy television series created by Mike Judge, John Altschuler and Dave Krinsky. The series focuses on five young men who founded a startup company in Silicon Valley"
      }
    },
    {
      "series_id": 3,
      "title": "House of Cards",
      "info": {
        "release_date": "2013-02-01T00:00:00Z",
        "series_info": "House of Cards is an American political thriller streaming television series created by Beau Willimon. It is an adaptation of the 1990 BBC miniseries of the same name and based on the 1989 novel of the same name by Michael Dobbs"
      }
    },
    {
      "series_id": 3,
      "title": "The Office",
      "info": {
        "release_date": "2005-03-24T00:00:00Z",
        "series_info": "The Office is an American mockumentary sitcom television series that depicts the everyday work lives of office employees in the Scranton, Pennsylvania, branch of the fictional Dunder Mifflin Paper Company"
      }
    },
    {
      "series_id": 3,
      "title": "True Detective",
      "info": {
        "release_date": "2014-01-12T00:00:00Z",
        "series_info": "True Detective is an American anthology crime drama television series created and written by Nic Pizzolatto. The series, broadcast by the premium cable network HBO in the United States, premiered on January 12, 2014"
      }
    },
    {
      "series_id": 4,
      "title": "The Big Bang Theory",
      "info": {
        "release_date": "2007-09-24T00:00:00Z",
        "series_info": "The Big Bang Theory is an American television sitcom created by Chuck Lorre and Bill Prady, both of whom served as executive producers on the series, along with Steven Molaro"
      }
    },
    {
      "series_id": 5,
      "title": "Twin Peaks",
      "info": {
        "release_date": "1990-04-08T00:00:00Z",
        "series_info": "Twin Peaks is an American mystery horror drama television series created by Mark Frost and David Lynch that premiered on April 8, 1990, on ABC until its cancellation after its second season in 1991 before returning as a limited series in 2017 on Showtime"
      }
    }
  ]
  $ python SeriesLoadData.py
  $ vim SeriesItemPut.py
  $ cat SeriesItemPut.py
  from pprint import pprint
  import boto3
  def put_serie(series_id, title, release_date, series_info):
      ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
      table = ydb_docapi_client.Table('docapitest/series')
      response = table.put_item(
        Item = {
              'series_id': series_id,
              'title': title,
              'info': {
                  'release_date': release_date,
                  'series_info': series_info
              }
          }
      )
      return response
  if __name__ == '__main__':
      serie_resp = put_serie(3, "Supernatural", "2015-09-13",
                            "Supernatural is an American television series created by Eric Kripke")
      print("Series added successfully:")
      pprint(serie_resp, sort_dicts = False)
  $ vim SeriesItemGet.py
  $ cat SeriesItemGet.py
  from pprint import pprint
  import boto3
  from botocore.exceptions import ClientError
  def get_serie(title, series_id):
      ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
      table = ydb_docapi_client.Table('docapitest/series')
      try:
          response = table.get_item(Key = {'series_id': series_id, 'title': title})
      except ClientError as e:
          print(e.response['Error']['Message'])
      else:
          return response['Item']
  if __name__ == '__main__':
      serie = get_serie("Supernatural", 3,)
      if serie:
          print("Record read:")
          pprint(serie, sort_dicts = False)
  $ vim SeriesItemUpdate.py
  $ cat SeriesItemUpdate.py
  from decimal import Decimal
  from pprint import pprint
  import boto3
  def update_serie(title, series_id, release_date,  rating):
      ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
      table = ydb_docapi_client.Table('docapitest/series')
      response = table.update_item(
          Key = {
              'series_id': series_id,
              'title': title
          },
          UpdateExpression = "set info.release_date = :d, info.rating = :r ",
          ExpressionAttributeValues = {
              ':d': release_date,
              ':r': Decimal(rating)
          },
          ReturnValues = "UPDATED_NEW"
      )
      return response
  if __name__ == '__main__':
      update_response = update_serie(
          "Supernatural", 3, "2005-09-13", 8)
      print("Series updated:")
      pprint(update_response, sort_dicts = False)
  $ vim SeriesItemDelete.py
  $ cat SeriesItemDelete.py
  from decimal import Decimal
  from pprint import pprint
  import boto3
  from botocore.exceptions import ClientError
  def delete_underrated_serie(title, series_id, rating):
      ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
      table = ydb_docapi_client.Table('docapitest/series')
      try:
          response = table.delete_item(
              Key = {
                  'series_id': series_id,
                  'title': title
              },
              ConditionExpression = "info.rating <= :val",
              ExpressionAttributeValues = {
                  ":val": Decimal(rating)
              }
          )
      except ClientError as e:
          if e.response['Error']['Code'] == "ConditionalCheckFailedException":
              print(e.response['Error']['Message'])
          else:
              raise
      else:
          return response
  if __name__ == '__main__':
      print("Deleting...")
      delete_response = delete_underrated_serie("Supernatural", 3, 8)
      if delete_response:
          print("Series data deleted:")
          pprint(delete_response, sort_dicts = False)
  $ vim SeriesQuery.py
  $ cat SeriesQuery.py
  from pprint import pprint
  import boto3
  from boto3.dynamodb.conditions import Key
  def query_and_project_series(series_id, title_range):
      ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
      table = ydb_docapi_client.Table('docapitest/series')
      response = table.query(
          ProjectionExpression = "series_id, title, info.release_date",
          KeyConditionExpression = Key('series_id').eq(series_id) & Key('title').begins_with(title_range)
      )
      return response['Items']
  if __name__ == '__main__':
      query_id = 3
      query_range = 'T'
      print(f"Series with ID = {query_id} and names beginning with "
            f"{query_range}")
      series = query_and_project_series(query_id, query_range)
      for serie in series:
          print(f"\n{serie['series_id']} : {serie['title']}")
          pprint(serie['info'])
  $ vim SeriesTableScan.py
  $ cat SeriesTableScan.py
  from pprint import pprint
  import boto3
  from boto3.dynamodb.conditions import Key
  def scan_series(id_range, display_series):
      ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
      table = ydb_docapi_client.Table('docapitest/series')
      scan_kwargs = {
          'FilterExpression': Key('series_id').between(*id_range),
          'ProjectionExpression': "series_id, title, info.release_date"
      }
      done = False
      start_key = None
      while not done:
          if start_key:
              scan_kwargs['ExclusiveStartKey'] = start_key
          response = table.scan(**scan_kwargs)
          display_series(response.get('Items', []))
          start_key = response.get('LastEvaluatedKey', None)
          done = start_key is None
  if __name__ == '__main__':
      def print_series(series):
          for serie in series:
              print(f"\n{serie['series_id']} : {serie['title']}")
              pprint(serie['info'])
      query_range = (1, 3)
      print(f"Series with IDs from {query_range[0]} to {query_range[1]}...")
      scan_series(query_range, print_series)
  $ vim SeriesTableDelete.py
  $ cat SeriesTableDelete.py
  import boto3
  def delete_serie_table():
      ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
      table = ydb_docapi_client.Table('docapitest/series')
      table.delete()
  if __name__ == '__main__':
      delete_serie_table()
      print("Table Series deleted")
  Task:
  Проверка доступности веб-ресурсов. 
  В этом уроке вы доработаете систему проверки доступности веб-ресурсов, которую создали на предыдущих практических занятиях. 
  В текущем варианте она проверяет только доступность сайта yandex.ru. 
  Теперь давайте добавим в неё возможность ставить задачи по проверке доступности других веб-ресурсов.
  Decision:
  Общая архитектура системы. У системы есть два метода: CheckUrl — ставит задачу на проверку указанного URL. GetResult — считывает результаты проверки.
  Метод CheckUrl обрабатывается функцией, которая будет складывать все запросы в очередь. Функция-обработчик будет вызываться раз в секунду, считывать URL из очереди,  проверять его доступность и записывать результат в базу данных. Оттуда этот результат можно будет получить с помощью метода GetResult.
  Мы не будем менять уже созданные функции и таблицу в PostgreSQL, сделаем новые.
  Работать с YMQ из функций мы будем с помощью библиотеки boto3. Чтобы её использовать, нужно создать сервисный аккаунт с секретным ключом доступа, а затем настроить зависимости функции. Сделаем это после того, как создадим очередь.
  Шаг 1. Проверить наличие сервисного аккаунта
  Если вы ранее создавали сервисный аккаунт с именем service-account-for-cf, добавляли вновь созданному сервисному аккаунту роли editor и другие, то вам остаётся только создать ключ доступа:
  yc iam access-key create --service-account-name service-account-for-cf 
  В результате вы получите примерно следующее:
      access_key:
          id: ajefraollq5puj2tir1o
          service_account_id: ajetdv28pl0a1a8r41f0
          created_at: "2021-08-23T21:13:05.677319393Z"
          key_id: BTPNvWthv0ZX2xVmlPIU
      secret: cWLQ0HrTM0k_qAac43cwMNJA8VV_rfTg_kd4xVPi 
  Здесь key_id — это идентификатор ключа доступа ACCESS_KEY. А secret — это секретный ключ SECRET_KEY. Переменные ACCESS_KEY и SECRET_KEY могут быть использованы для задания соответствующих значений aws_access_key_id и aws_secret_access_key при использовании библиотеки boto3.
  Шаг 2. Создание очереди Yandex Message Queue
  Вы можете создать очередь одним из трёх способов: через консоль управления; с помощью консольной утилиты aws; с помощью Terraform.
  В этом уроке мы будем использовать консоль управления. Откройте раздел Message Queue и нажмите кнопку Создать очередь.
  В настройках создаваемой очереди задайте имя очереди my-first-queue, затем выберите тип очереди Стандартная и нажмите кнопку Создать.
  Очередь создана.
  Теперь зайдите в настройки очереди, чтобы посмотреть параметры подключения к ней. Нам потребуется значение URL.
  Шаг 3. Создание функции
  Для создания функции зададим ряд переменных: VERBOSE_LOG — определяет, пишет ли функция подробности своего выполнения в журнал. AWS_ACCESS_KEY_ID — значение «Идентификатор ключа» из сервисного аккаунта, который мы сделали ранее. AWS_SECRET_ACCESS_KEY — значение «Секретный ключ» из того же сервисного аккаунта. QUEUE_URL — URL на очередь, его можно получить на обзорной странице созданной ранее очереди.
  Чтобы задать переменные, в консоли выполните следующие команды:
  echo "export VERBOSE_LOG=True" >> ~/.bashrc && . ~/.bashrc
  echo "export AWS_ACCESS_KEY_ID=<AWS_ACCESS_KEY_ID>" >> ~/.bashrc && . ~/.bashrc
  echo "export AWS_SECRET_ACCESS_KEY=<AWS_SECRET_ACCESS_KEY>" >> ~/.bashrc && . ~/.bashrc
  echo "export QUEUE_URL=<QUEUE_URL>" >> ~/.bashrc && . ~/.bashrc 
  Воспользуйтесь командой pipreqs $PWD --force для формирования файла requirements.txt. Затем создайте функцию my-url-receiver-function.py:
  import logging
  import os
  import boto3
  logger = logging.getLogger()
  logger.setLevel(logging.INFO)
  verboseLogging = eval(os.environ['VERBOSE_LOG'])  ## Convert to bool
  queue_url = os.environ['QUEUE_URL']
  def log(logString):
      if verboseLogging:
          logger.info(logString)
  def handler(event, context):
      # Get url
      try:
          url = event['queryStringParameters']['url']
      except Exception as error:
          logger.error(error)
          statusCode = 400
          return {
              'statusCode': statusCode
          }
      # Create client
      client = boto3.client(
          service_name='sqs',
          endpoint_url='https://message-queue.api.cloud.yandex.net',
          region_name='ru-central1'
      )
      # Send message to queue
      client.send_message(
          QueueUrl=queue_url,
          MessageBody=url
      )
      log('Successfully sent test message to queue')
      statusCode = 200
      return {
          'statusCode': statusCode
      } 
  Перейдите в директорию с исходными файлами и упакуйте файлы с функцией и требованиями в ZIP-архив. При этом сразу задайте все необходимые переменные и сервисный аккаунт:
  zip my-url-receiver-function my-url-receiver-function.py requirements.txt
  yc serverless function create \
    --name  my-url-receiver-function \
    --description "function for url"
  yc serverless function version create \
    --function-name=my-url-receiver-function \
    --memory=256m \
    --execution-timeout=5s \
    --runtime=python37 \
    --entrypoint=my-url-receiver-function.handler \
    --service-account-id $SERVICE_ACCOUNT_ID \
    --environment VERBOSE_LOG=$VERBOSE_LOG \
    --environment AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
    --environment AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
    --environment QUEUE_URL=$QUEUE_URL \
    --source-path my-url-receiver-function.zip 
  Тестирование функции. Находясь в вашем рабочем каталоге, перейдите в раздел Cloud Functions консоли управления и выберите ранее созданную функцию my-url-receiver-function. Перейдите на вкладку Тестирование в боковом меню, выберите шаблон HTTPS-вызов и замените раздел queryStringParameters:
      "queryStringParameters": {
          "a": "2",
          "b": "1",
      },     
  на аналогичный, но с параметром url с любым сайтом. Важно указывать ссылку целиком.
      "queryStringParameters": {
          "url": "https://ya.ru/"
      },     
  Нажмите кнопку Запустить тест.
  Если вы всё сделали правильно, то увидите код статуса 200. При этом в очереди увеличится количество сообщений.
  Шаг 4. Обновление спецификации API Gateway
  Функция готова, но по умолчанию она не является публичной. Предоставим доступ к ней с помощью API-шлюза. Для этого необходимо обновить ранее созданную спецификацию hello-world.yaml. Если у вас нет её под рукой, выгрузите её из облака:
  yc serverless api-gateway get-spec \
    --name hello-world >> hello-world-new.yaml 
  Внесите изменения, добавив секцию о ранее созданной функции:
      /check:
          get:
              x-yc-apigateway-integration:
                  type: cloud-functions
                  function_id: <идентификатор функции>
                  service_account_id: <идентификатор сервисного аккаунта>
              operationId: add-url 
  Обновите конфигурацию:
  yc serverless api-gateway update \
    --name hello-world \
    --spec=hello-world-new.yaml 
  Для тестирования выполните вызов функции в браузере:
  https://<идентификатор API Gateway>.apigw.yandexcloud.net/check?url=https://ya.ru/ 
  После каждого запроса количество сообщений в очереди будет увеличиваться на одно.
  Шаг 5. Создание функции для чтения из очереди
  В предыдущих работах мы создавали функцию, использующую подключение к БД. Здесь мы повторим этот опыт.
  Проверим, что нам доступны переменные для инициации подключения: CONNECTION_ID, DB_USER, DB_HOST. Мы создавали их в предыдущей работе с помощью следующих команд:
  echo "export CONNECTION_ID=<CONNECTION_ID>" >> ~/.bashrc && . ~/.bashrc
  echo "export DB_USER=<DB_USER>" >> ~/.bashrc && . ~/.bashrc
  echo "export DB_HOST=<DB_HOST>" >> ~/.bashrc && . ~/.bashrc 
  Также для работы с очередью нам потребуются переменные VERBOSE_LOG, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY и QUEUE_URL, заданные на предыдущих шагах.
  Создадим функцию function-for-url-from-mq.py и воспользуемся командой pipreqs $PWD --force, чтобы сформировать для нее файл requirements.txt.
  import logging
  import os
  import boto3
  import datetime
  import requests
  #Эти библиотеки нужны для работы с PostgreSQL
  import psycopg2
  import psycopg2.errors
  import psycopg2.extras
  CONNECTION_ID = os.getenv("CONNECTION_ID")
  DB_USER = os.getenv("DB_USER")
  DB_HOST = os.getenv("DB_HOST")
  QUEUE_URL = os.environ['QUEUE_URL']
  # Настраиваем функцию для записи информации в журнал функции
  # Получаем стандартный логер языка Python
  logger = logging.getLogger()
  logger.setLevel(logging.INFO)
  # Вычитываем переменную VERBOSE_LOG, которую мы указываем в переменных окружения 
  verboseLogging = eval(os.environ['VERBOSE_LOG'])  ## Convert to bool
  #Функция log, которая запишет текст в журнал выполнения функции, если в переменной окружения VERBOSE_LOG будет значение True
  def log(logString):
      if verboseLogging:
          logger.info(logString)
  #Получаем подключение
  def getConnString(context):
      """
      Extract env variables to connect to DB and return a db string
      Raise an error if the env variables are not set
      :return: string
      """
      connection = psycopg2.connect(
          database=CONNECTION_ID, # Идентификатор подключения
          user=DB_USER, # Пользователь БД
          password=context.token["access_token"],
          host=DB_HOST, # Точка входа
          port=6432,
          sslmode="require")
      return connection
  """
      Create SQL query with table creation
  """
  def makeCreateDataTableQuery(table_name):
      query = f"""CREATE TABLE public.{table_name} (
      url text,
      result integer,
      time float
      )"""
      return query
  def makeInsertDataQuery(table_name, url, result, time):
      query = f"""INSERT INTO {table_name} 
      (url, result,time)
      VALUES('{url}', {result}, {time})
      """
      return query
  def handler(event, context):
      # Create client
      client = boto3.client(
          service_name='sqs',
          endpoint_url='https://message-queue.api.cloud.yandex.net',
          region_name='ru-central1'
      )
      # Receive sent message
      messages = client.receive_message(
          QueueUrl=QUEUE_URL,
          MaxNumberOfMessages=1,
          VisibilityTimeout=60,
          WaitTimeSeconds=1
      ).get('Messages')
      if messages is None:
          return {
              'statusCode': 200
          }
      for msg in messages:
          log('Received message: "{}"'.format(msg.get('Body')))
      # Get url from message
      url = msg.get('Body');
      # Check url
      try:
          now = datetime.datetime.now()
          response = requests.get(url, timeout=(1.0000, 3.0000))
          timediff = datetime.datetime.now() - now
          result = response.status_code
      except requests.exceptions.ReadTimeout:
          result = 601
      except requests.exceptions.ConnectTimeout:
          result = 602
      except requests.exceptions.Timeout:
          result = 603
      log(f'Result: {result} Time: {timediff.total_seconds()}')
      connection = getConnString(context)
      log(f'Connecting: {connection}')    
      cursor = connection.cursor()
      table_name = 'custom_request_result'
      sql = makeInsertDataQuery(table_name, url, result, timediff.total_seconds())
      log(f'Exec: {sql}')
      try:
          cursor.execute(sql)
      except psycopg2.errors.UndefinedTable as error:
          log(f'Table not exist - create and repeate insert')
          connection.rollback()
          logger.error(error)
          createTable = makeCreateDataTableQuery(table_name)
          log(f'Exec: {createTable}')
          cursor.execute(createTable)
          connection.commit()
          log(f'Exec: {sql}')
          cursor.execute(sql)
      except Exception as error:
          logger.error( error)
      connection.commit()
      cursor.close()
      connection.close()
      # Delete processed messages
      for msg in messages:
          client.delete_message(
              QueueUrl=QUEUE_URL,
              ReceiptHandle=msg.get('ReceiptHandle')
          )
          print('Successfully deleted message by receipt handle "{}"'.format(msg.get('ReceiptHandle')))
      statusCode = 200
      return {
          'statusCode': statusCode
      } 
  При создании сразу задайте все необходимые переменные и сервисный аккаунт:
  zip function-for-url-from-mq function-for-url-from-mq.py requirements.txt
  yc serverless function create \
    --name function-for-url-from-mq \
    --description "function for url from mq"
  yc serverless function version create \
    --function-name=function-for-url-from-mq \
    --memory=256m \
    --execution-timeout=5s \
    --runtime=python37 \
    --entrypoint=function-for-url-from-mq.handler \
    --service-account-id $SERVICE_ACCOUNT_ID \
    --environment VERBOSE_LOG=True \
    --environment CONNECTION_ID=$CONNECTION_ID \
    --environment DB_USER=$DB_USER \
    --environment DB_HOST=$DB_HOST \
    --environment AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
    --environment AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
    --environment QUEUE_URL=$QUEUE_URL \
    --source-path function-for-url-from-mq.zip 
  Протестируйте функцию.
  После её выполнения количество сообщений в очереди уменьшится, а в базе данных появится новая таблица с результатами тестирования доступности функции.
  Шаг 6. Создание триггера
  Создадим триггер, который будет вызывать функцию обработки сообщений из очереди один раз в минуту. Он будет использовать cron-выражение:
  yc serverless trigger create timer \
    --name trigger-for-mq \
    --invoke-function-name function-for-url-from-mq \
    --invoke-function-service-account-id $SERVICE_ACCOUNT_ID \
    --cron-expression '* * * * ? *' 
  Cron-выражение * * * * ? * означает вызов функции function-for-url-from-mq один раз в минуту. Подробнее про cron-выражения можно прочитать в документации.
  Теперь у нас есть функция, которая раз в минуту будет пробовать взять из очереди URL и проверить его. Также есть метод REST API, который позволяет записывать URL в очередь независимо от работы обработчика. Мы можем вызывать созданный метод как угодно часто. Очередь будет просто накапливаться, а затем обработчик будет постепенно её разбирать.
  В итоге вы получили асинхронную систему проверки доступности URL с доступом по REST API. Вы не создали ни одной виртуальной машины, но решили вопросы масштабирования и отказоустойчивости системы.
  Удаление триггера-таймера
  По завершении практической работы не забудьте удалить созданный вами триггер trigger-for-mq, иначе он будет работать, пока не исчерпает деньги на аккаунте:
  yc serverless trigger delete trigger-for-mq 
  Не забудьте удалить или остановить все созданные вами ресурсы: триггеры, очереди YMQ и кластер базы данных.
  Decision:
  $ yc iam access-key create --service-account-name service-account-for-cf
  $ echo "export VERBOSE_LOG=True" >> ~/.bashrc && . ~/.bashrc
  $ echo "export AWS_ACCESS_KEY_ID=<AWS_ACCESS_KEY_ID>" >> ~/.bashrc && . ~/.bashrc
  $ echo "export AWS_SECRET_ACCESS_KEY=<AWS_SECRET_ACCESS_KEY>" >> ~/.bashrc && . ~/.bashrc
  $ echo "export QUEUE_URL=<QUEUE_URL>" >> ~/.bashrc && . ~/.bashrc
  $ vim my-url-receiver-function.py
  $ cat my-url-receiver-function.py
  import logging
  import os
  import boto3
  logger = logging.getLogger()
  logger.setLevel(logging.INFO)
  verboseLogging = eval(os.environ['VERBOSE_LOG'])  ## Convert to bool
  queue_url = os.environ['QUEUE_URL']
  def log(logString):
      if verboseLogging:
          logger.info(logString)
  def handler(event, context):
      # Get url
      try:
          url = event['queryStringParameters']['url']
      except Exception as error:
          logger.error(error)
          statusCode = 400
          return {
              'statusCode': statusCode
          }
      # Create client
      client = boto3.client(
          service_name='sqs',
          endpoint_url='https://message-queue.api.cloud.yandex.net',
          region_name='ru-central1'
      )
      # Send message to queue
      client.send_message(
          QueueUrl=queue_url,
          MessageBody=url
      )
      log('Successfully sent test message to queue')
      statusCode = 200
      return {
          'statusCode': statusCode
      }
  $ touch requirements.txt
  $ pipreqs $PWD --force
  $ zip my-url-receiver-function my-url-receiver-function.py requirements.txt
  $ yc serverless function create \
      --name  my-url-receiver-function \
      --description "function for url"
  $ yc serverless function version create \
      --function-name=my-url-receiver-function \
      --memory=256m \
      --execution-timeout=5s \
      --runtime=python37 \
      --entrypoint=my-url-receiver-function.handler \
      --service-account-id $SERVICE_ACCOUNT_ID \
      --environment VERBOSE_LOG=$VERBOSE_LOG \
      --environment AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
      --environment AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
      --environment QUEUE_URL=$QUEUE_URL \
      --source-path my-url-receiver-function.zip
  $ yc serverless api-gateway get-spec \
      --name hello-world >> hello-world-new.yaml
  $ vim hello-world-new.yaml 
  $ cat hello-world-new.yaml 
  openapi: "3.0.0"
  info:
    version: 1.0.0
    title: Test API
  paths:
    /hello:
      get:
        summary: Say hello
        operationId: hello
        parameters:
          - name: user
            in: query
            description: User name to appear in greetings
            required: false
            schema:
              type: string
              default: 'world'
        responses:
          '200':
            description: Greeting
            content:
              'text/plain':
                schema:
                  type: "string"
        x-yc-apigateway-integration:
          type: dummy
          http_code: 200
          http_headers:
            'Content-Type': "text/plain"
          content:
            'text/plain': "Hello, {user}!\n"
      /check:
          get:
              x-yc-apigateway-integration:
                  type: cloud-functions
                  function_id: <идентификатор функции>
                  service_account_id: <идентификатор сервисного аккаунта>
              operationId: add-url
  $ yc serverless api-gateway update \
    --name hello-world \
    --spec=hello-world-new.yaml
  $ echo "export CONNECTION_ID=<CONNECTION_ID>" >> ~/.bashrc && . ~/.bashrc
  $ echo "export DB_USER=<DB_USER>" >> ~/.bashrc && . ~/.bashrc
  $ echo "export DB_HOST=<DB_HOST>" >> ~/.bashrc && . ~/.bashrc
  $ vim function-for-url-from-mq.py
  $ cat function-for-url-from-mq.py
  import logging
  import os
  import boto3
  import datetime
  import requests
  #Эти библиотеки нужны для работы с PostgreSQL
  import psycopg2
  import psycopg2.errors
  import psycopg2.extras
  CONNECTION_ID = os.getenv("CONNECTION_ID")
  DB_USER = os.getenv("DB_USER")
  DB_HOST = os.getenv("DB_HOST")
  QUEUE_URL = os.environ['QUEUE_URL']
  # Настраиваем функцию для записи информации в журнал функции
  # Получаем стандартный логер языка Python
  logger = logging.getLogger()
  logger.setLevel(logging.INFO)
  # Вычитываем переменную VERBOSE_LOG, которую мы указываем в переменных окружения 
  verboseLogging = eval(os.environ['VERBOSE_LOG'])  ## Convert to bool
  #Функция log, которая запишет текст в журнал выполнения функции, если в переменной окружения VERBOSE_LOG будет значение True
  def log(logString):
      if verboseLogging:
          logger.info(logString)
  #Получаем подключение
  def getConnString(context):
      """
      Extract env variables to connect to DB and return a db string
      Raise an error if the env variables are not set
      :return: string
      """
      connection = psycopg2.connect(
          database=CONNECTION_ID, # Идентификатор подключения
          user=DB_USER, # Пользователь БД
          password=context.token["access_token"],
          host=DB_HOST, # Точка входа
          port=6432,
          sslmode="require")
      return connection
  """
      Create SQL query with table creation
  """
  def makeCreateDataTableQuery(table_name):
      query = f"""CREATE TABLE public.{table_name} (
      url text,
      result integer,
      time float
      )"""
      return query
  def makeInsertDataQuery(table_name, url, result, time):
      query = f"""INSERT INTO {table_name} 
      (url, result,time)
      VALUES('{url}', {result}, {time})
      """
      return query
  def handler(event, context):
      # Create client
      client = boto3.client(
          service_name='sqs',
          endpoint_url='https://message-queue.api.cloud.yandex.net',
          region_name='ru-central1'
      )
      # Receive sent message
      messages = client.receive_message(
          QueueUrl=QUEUE_URL,
          MaxNumberOfMessages=1,
          VisibilityTimeout=60,
          WaitTimeSeconds=1
      ).get('Messages')
      if messages is None:
          return {
              'statusCode': 200
          }
      for msg in messages:
          log('Received message: "{}"'.format(msg.get('Body')))
      # Get url from message
      url = msg.get('Body');
      # Check url
      try:
          now = datetime.datetime.now()
          response = requests.get(url, timeout=(1.0000, 3.0000))
          timediff = datetime.datetime.now() - now
          result = response.status_code
      except requests.exceptions.ReadTimeout:
          result = 601
      except requests.exceptions.ConnectTimeout:
          result = 602
      except requests.exceptions.Timeout:
          result = 603
      log(f'Result: {result} Time: {timediff.total_seconds()}')
      connection = getConnString(context)
      log(f'Connecting: {connection}')    
      cursor = connection.cursor()
      table_name = 'custom_request_result'
      sql = makeInsertDataQuery(table_name, url, result, timediff.total_seconds())
      log(f'Exec: {sql}')
      try:
          cursor.execute(sql)
      except psycopg2.errors.UndefinedTable as error:
          log(f'Table not exist - create and repeate insert')
          connection.rollback()
          logger.error(error)
          createTable = makeCreateDataTableQuery(table_name)
          log(f'Exec: {createTable}')
          cursor.execute(createTable)
          connection.commit()
          log(f'Exec: {sql}')
          cursor.execute(sql)
      except Exception as error:
          logger.error( error)
      connection.commit()
      cursor.close()
      connection.close()
      # Delete processed messages
      for msg in messages:
          client.delete_message(
              QueueUrl=QUEUE_URL,
              ReceiptHandle=msg.get('ReceiptHandle')
          )
          print('Successfully deleted message by receipt handle "{}"'.format(msg.get('ReceiptHandle')))
      statusCode = 200
      return {
          'statusCode': statusCode
      }
  $ zip function-for-url-from-mq function-for-url-from-mq.py requirements.txt
  $ yc serverless function create \
    --name function-for-url-from-mq \
    --description "function for url from mq"
  $ yc serverless function version create \
    --function-name=function-for-url-from-mq \
    --memory=256m \
    --execution-timeout=5s \
    --runtime=python37 \
    --entrypoint=function-for-url-from-mq.handler \
    --service-account-id $SERVICE_ACCOUNT_ID \
    --environment VERBOSE_LOG=True \
    --environment CONNECTION_ID=$CONNECTION_ID \
    --environment DB_USER=$DB_USER \
    --environment DB_HOST=$DB_HOST \
    --environment AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
    --environment AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
    --environment QUEUE_URL=$QUEUE_URL \
    --source-path function-for-url-from-mq.zip
  $ yc serverless trigger create timer \
    --name trigger-for-mq \
    --invoke-function-name function-for-url-from-mq \
    --invoke-function-service-account-id $SERVICE_ACCOUNT_ID \
    --cron-expression '* * * * ? *'
  $ yc serverless trigger delete trigger-for-mq
  Task:
  Однократная отправка сообщений
  В этой практической работе мы реализуем проект, который позволит пользователям конвертировать видеофайлы в GIF. 
  Такая задача хорошо подходит для Cloud Functions, потому что конвертирование отнимает немало ресурсов процессора, и чем качественнее видео, тем больше ресурсов требуется на его обработку.
  Почему для решения этой задачи нужны очереди?
  Представим, что мы попытались решить эту задачу «в лоб». Пользователь заходит на страницу и вводит ссылку на видеофайл. Сервис скачивает его, конвертирует и отдает ссылку на GIF. Возникают две серьёзные проблемы:
  Синхронное соединение не всегда стабильно. Чем дольше вы его держите, тем выше вероятность, что оно разорвётся. В этом случае всё придётся сделать заново. А если соединение нестабильно, то пользователь может и не дождаться результата.
  Задача ресурсоёмкая: если сервисом одновременно воспользуются много пользователей с большими видеороликами, мощностей может не хватить.
  Чтобы избежать этих проблем, в архитектуру сервиса необходимо встроить очередь.
  Decision:
  Шаг 1. Сервисный аккаунт и Lockbox
  Создание сервисного аккаунта
  Создайте сервисный аккаунт с именем ffmpeg-account-for-cf:
  export SERVICE_ACCOUNT=$(yc iam service-account create --name ffmpeg-account-for-cf \
    --description "service account for serverless" \
    --format json | jq -r .) 
  Проверьте текущий список сервисных аккаунтов:
  yc iam service-account list 
  После проверки запишите ID созданного сервисного аккаунта в переменную SERVICE_ACCOUNT_ID:
  echo "export SERVICE_ACCOUNT_FFMPEG_ID=<ID>" >> ~/.bashrc && . ~/.bashrc
  echo $SERVICE_ACCOUNT_FFMPEG_ID 
  Назначение роли сервисному аккаунту
  Добавим вновь созданному сервисному аккаунту роли storage.viewer, storage.uploader, ymq.reader, ymq.writer, ydb.admin, serverless.functions.invoker, и lockbox.payloadViewer:
  echo "export FOLDER_ID=$(yc config get folder-id)" >> ~/.bashrc && . ~/.bashrc
  echo $FOLDER_ID
  yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --role storage.viewer
  yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --role storage.uploader
  yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --role ymq.reader
  yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --role ymq.writer
  yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --role ydb.admin
  yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --role serverless.functions.invoker
  yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --role lockbox.payloadViewer
  yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --role editor 
  Вы можете назначить несколько ролей и с помощью команды set-access-binding. Но эта команда полностью перезаписывает права доступа к ресурсу и все текущие роли на него будут удалены! Поэтому сначала убедитесь, что ресурсу не назначены роли, которые вы не хотите потерять:
  yc resource-manager folder list-access-bindings $FOLDER_ID
  yc resource-manager folder set-access-bindings $FOLDER_ID \
    --access-binding role=storage.viewer,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --access-binding role=storage.uploader,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --access-binding role=ymq.reader,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --access-binding role=ymq.writer,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --access-binding role=ydb.admin,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --access-binding role=serverless.functions.invoker,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --access-binding role=lockbox.payloadViewer,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --access-binding role=editor,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID 
  Создание ключа доступа для сервисного аккаунта
  Этот этап нужен для получения идентификатора ключа доступа и секретного ключа, которые будут использованы для загрузки файлов в Object Storage, работы с Yandex Message Queue и т. д. Для создания ключа доступа необходимо вызвать следующую команду:
  yc iam access-key create --service-account-name ffmpeg-account-for-cf 
  В результате вы получите примерно следующее:
      access_key:
          id: ajefraollq5puj2tir1o
          service_account_id: ajetdv28pl0a1a8r41f0
          created_at: "2021-08-23T21:13:05.677319393Z"
          key_id: BTPNvWthv0ZX2xVmlPIU
      secret: cWLQ0HrTM0k_qAac43cwMNJA8VV_rfTg_kd4xVPi 
  Здесь key_id — это идентификатор ключа доступа ACCESS_KEY_ID. А secret — это секретный ключ SECRET_ACCESS_KEY. Переменные ACCESS_KEY_ID и SECRET_ACCESS_KEY могут быть использованы для задания соответствующих значений aws_access_key_id и aws_secret_access_key при использовании библиотеки boto3.
  Создание элемента в сервисе Lockbox
  В сервисе Lockbox (находится на стадии Preview) создайте ваш первый секрет, состоящий из набора версий, в которых хранятся ваши данные. Версия содержит наборы ключей и значений:
      Ключ — несекретное название для значения, по которому вы будете его идентифицировать.
      Значение — это секретные данные.
  Версия не изменяется. Для любого изменения количества пар ключей-значений или их содержимого необходимо создать новую версию. Создадим секрет с именем ffmpeg-sa-key и парой ключей ACCESS_KEY_ID и SECRET_ACCESS_KEY:
  yc lockbox secret create --name ffmpeg-sa-key \
    --folder-id $FOLDER_ID \
    --description "keys for serverless" \
    --payload '[{"key": "ACCESS_KEY_ID", "text_value": <ACCESS_KEY_ID>}, {"key": "SECRET_ACCESS_KEY", "text_value": "<SECRET_ACCESS_KEY>"}]' 
  Получим и запишем значение SECRET_ID, оно нам потребуется при создании функции:
  yc lockbox secret list
  yc lockbox secret get --name ffmpeg-sa-key
  echo "export SECRET_ID=<SECRET_ID>" >> ~/.bashrc && . ~/.bashrc
  echo $SECRET_ID 
  Шаг 2. Создание очереди Yandex Message Queue
  Для создания очереди Yandex Message Queue вы можете использовать три разных способа:
      консоль управления;
      консольная утилита aws;
      Terraform.
  Создание очереди с помощью утилиты aws
  Воспользуемся AWS CLI. Для начала задайте конфигурацию с помощью команды aws configure. При этом от вас потребуется ввести:
      AWS Access Key ID — идентификатор ключа доступа key_id сервисного аккаунта, полученный на предыдущем шаге.
      AWS Secret Access Key — секретный ключ secret сервисного аккаунта, полученный на предыдущем шаге.
      Default region name — используйте значение ru-central1.
  По завершению конфигурации вы сможете создать очередь:
  aws configure
  aws sqs create-queue --queue-name ffmpeg --endpoint https://message-queue.api.cloud.yandex.net/ 
  В результате успешного выполнения предыдущей команды в ответ вы получите URL:
      {
          "QueueUrl": "https://message-queue.api.cloud.yandex.net/b1ga4gj7agij03ln6aov/dj6000000003kv2t02b3/ffmpeg"
      } 
  Запишем значения URL в переменную YMQ_QUEUE_URL. Она потребуется нам при создании функции:
  echo "export YMQ_QUEUE_URL=<YMQ_QUEUE_URL>" >> ~/.bashrc && . ~/.bashrc
  echo $YMQ_QUEUE_URL 
  Ещё вам потребует значение атрибута QueueArn, получим его:
  aws sqs get-queue-attributes \
    --endpoint https://message-queue.api.cloud.yandex.net \
    --queue-url $YMQ_QUEUE_URL \
    --attribute-names QueueArn 
  В результате вы получите ответ вида:
      {
          "Attributes": {
              "QueueArn": "yrn:yc:ymq:ru-central1:b1gl21bkgss4msekt08i:ffmpeg"
          }
      } 
  Сохраним значение QueueArn в переменную YMQ_QUEUE_ARN:
  echo "export YMQ_QUEUE_ARN=<YMQ_QUEUE_ARN>" >> ~/.bashrc && . ~/.bashrc
  echo $YMQ_QUEUE_ARN 
  Шаг 3. Создание базы данных в сервисе Yandex Database
  Создадим базу данных YDB с именем ffmpeg и типом serverless, используя для этого флаг --serverless:
  yc ydb database create ffmpeg \
    --serverless \
    --folder-id $FOLDER_ID
  yc ydb database list 
  Сразу получим и сохраним document_api_endpoint в значение переменной DOCAPI_ENDPOINT:
  yc ydb database get --name ffmpeg
  echo "export DOCAPI_ENDPOINT=<DOCAPI_ENDPOINT>" >> ~/.bashrc && . ~/.bashrc
  echo $DOCAPI_ENDPOINT 
  Как только база данных создана, воспользуемся ранее использованной утилитой AWS CLI для создания документной таблицы в этой базе данных. Всю конфигурацию возьмем из файла tasks.json:
  {
    "AttributeDefinitions": [
      {
        "AttributeName": "task_id",
        "AttributeType": "S"
      }
    ],
    "KeySchema": [
      {
        "AttributeName": "task_id",
        "KeyType": "HASH"
      }
    ],
    "TableName": "tasks"
  } 
  Находясь в одном каталоге с файлом tasks.json, вызовите следующую команду для создания таблицы:
  aws dynamodb create-table \
    --cli-input-json file://tasks.json \
    --endpoint-url $DOCAPI_ENDPOINT \
    --region ru-central1 
  В консоли управления убедитесь, что БД ffmpeg создана, и в ней есть пустая таблица tasks.
  Шаг 4. Создание бакета в сервисе Object Storage
  Самый простой способ создания бакета в Object Storage — это использование консоли управления.
  В консоли управления в вашем рабочем каталоге выберите сервис Object Storage. Нажмите кнопку Создать бакет. На странице создания бакета введите имя, в нашем примере это будет storage-for-ffmpeg, остальные параметры не меняйте.
  Нажмите кнопку Создать бакет для завершения операции. Далее вы всегда сможете поменять класс хранилища, его размер и настройки доступа.
  Сохраним название бакета для дальнейшего использования:
  echo "export S3_BUCKET=<имя бакета>" >> ~/.bashrc && . ~/.bashrc
  echo $S3_BUCKET 
  Шаг 5. Создание функций
  При создании функций нам потребуется ряд переменных:
      SECRET_ID — идентификатор секрета (можно получить из таблицы со списком секретов);
      YMQ_QUEUE_URL — URL очереди (можно получить на странице обзора);
      DOCAPI_ENDPOINT — его можно получить на странице обзора БД, нужен именно Document API;
      S3_BUCKET — имя бакета, в нашем случае это storage-for-ffmpeg.
  Проверим заданные ранее переменные:
  echo $SERVICE_ACCOUNT_FFMPEG_ID
  echo $SECRET_ID
  echo $YMQ_QUEUE_URL
  echo $DOCAPI_ENDPOINT
  echo $S3_BUCKET 
  Для обработки видео нам потребуется утилита FFmpeg. Скачайте статический релизный бинарный файл для Linux amd64 на сайте ffmpeg.org. Обычно он находится в разделе FFmpeg Static Builds и называется примерно так: ffmpeg-release-amd64-static.tar.xz. Распакуйте архив. Из него вам понадобится только файл ffmpeg. Поскольку есть ограничение на размер  файла, который можно приложить через консоль, загрузим код функций и ffmpeg в Object Storage.
  Исходный код в файле index.py содержит обе необходимые нам функции:
  import json
  import os
  import subprocess
  import uuid
  from urllib.parse import urlencode
  import boto3
  import requests
  import yandexcloud
  from yandex.cloud.lockbox.v1.payload_service_pb2 import GetPayloadRequest
  from yandex.cloud.lockbox.v1.payload_service_pb2_grpc import PayloadServiceStub
  boto_session = None
  storage_client = None
  docapi_table = None
  ymq_queue = None
  def get_boto_session():
      global boto_session
      if boto_session is not None:
          return boto_session
      # initialize lockbox and read secret value
      yc_sdk = yandexcloud.SDK()
      channel = yc_sdk._channels.channel("lockbox-payload")
      lockbox = PayloadServiceStub(channel)
      response = lockbox.Get(GetPayloadRequest(secret_id=os.environ['SECRET_ID']))
      # extract values from secret
      access_key = None
      secret_key = None
      for entry in response.entries:
          if entry.key == 'ACCESS_KEY_ID':
              access_key = entry.text_value
          elif entry.key == 'SECRET_ACCESS_KEY':
              secret_key = entry.text_value
      if access_key is None or secret_key is None:
          raise Exception("secrets required")
      print("Key id: " + access_key)
      # initialize boto session
      boto_session = boto3.session.Session(
          aws_access_key_id=access_key,
          aws_secret_access_key=secret_key
      )
      return boto_session
  def get_ymq_queue():
      global ymq_queue
      if ymq_queue is not None:
          return ymq_queue
      ymq_queue = get_boto_session().resource(
          service_name='sqs',
          endpoint_url='https://message-queue.api.cloud.yandex.net',
          region_name='ru-central1'
      ).Queue(os.environ['YMQ_QUEUE_URL'])
      return ymq_queue
  def get_docapi_table():
      global docapi_table
      if docapi_table is not None:
          return docapi_table
      docapi_table = get_boto_session().resource(
          'dynamodb',
          endpoint_url=os.environ['DOCAPI_ENDPOINT'],
          region_name='ru-central1'
      ).Table('tasks')
      return docapi_table
  def get_storage_client():
      global storage_client
      if storage_client is not None:
          return storage_client
      storage_client = get_boto_session().client(
          service_name='s3',
          endpoint_url='https://storage.yandexcloud.net',
          region_name='ru-central1'
      )
      return storage_client
  # API handler
  def create_task(src_url):
      task_id = str(uuid.uuid4())
      get_docapi_table().put_item(Item={
          'task_id': task_id,
          'ready': False
      })
      get_ymq_queue().send_message(MessageBody=json.dumps({'task_id': task_id, "src": src_url}))
      return {
          'task_id': task_id
      }
  def get_task_status(task_id):
      task = get_docapi_table().get_item(Key={
          "task_id": task_id
      })
      if task['Item']['ready']:
          return {
              'ready': True,
              'gif_url': task['Item']['gif_url']
          }
      return {'ready': False}
  def handle_api(event, context):
      action = event['action']
      if action == 'convert':
          return create_task(event['src_url'])
      elif action == 'get_task_status':
          return get_task_status(event['task_id'])
      else:
          return {"error": "unknown action: " + action}
  # Converter handler
  def download_from_ya_disk(public_key, dst):
      api_call_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?' + \
                    urlencode(dict(public_key=public_key))
      response = requests.get(api_call_url)
      download_url = response.json()['href']
      download_response = requests.get(download_url)
      with open(dst, 'wb') as video_file:
          video_file.write(download_response.content)
  def upload_and_presign(file_path, object_name):
      client = get_storage_client()
      bucket = os.environ['S3_BUCKET']
      client.upload_file(file_path, bucket, object_name)
      return client.generate_presigned_url('get_object', Params={'Bucket': bucket, 'Key': object_name}, ExpiresIn=3600)
  def handle_process_event(event, context):
      for message in event['messages']:
          task_json = json.loads(message['details']['message']['body'])
          task_id = task_json['task_id']
          # Download video
          download_from_ya_disk(task_json['src'], '/tmp/video.mp4')
          # Convert with ffmpeg
          subprocess.run(['ffmpeg', '-i', '/tmp/video.mp4', '-r', '10', '-s', '320x240', '/tmp/result.gif'])
          result_object = task_id + ".gif"
          # Upload to Object Storage and generate presigned url
          result_download_url = upload_and_presign('/tmp/result.gif', result_object)
          # Update task status in DocAPI
          get_docapi_table().update_item(
              Key={'task_id': task_id},
              AttributeUpdates={
                  'ready': {'Value': True, 'Action': 'PUT'},
                  'gif_url': {'Value': result_download_url, 'Action': 'PUT'},
              }
          )
      return "OK" 
  Сгенерируйте файл requirements.txt:
  pipreqs $PWD --force 
  Находясь в директории с исходными файлами, упакуем все нужные файлы в ZIP-архив.
  zip src.zip index.py requirements.txt ffmpeg 
  В Object Storage для простоты используем тот же бакет, куда далее будем складывать видео. На вкладке Объекты, вверху справа нажмите кнопку Загрузить и выберите созданный архив.
  Создадим функции ffmpeg-api и ffmpeg-converter, при этом сразу зададим все необходимые переменные и сервисный аккаунт:
  yc serverless function create \
    --name ffmpeg-api \
    --description "function for ffmpeg-api"
  yc serverless function create \
    --name ffmpeg-converter \
    --description "function for ffmpeg-converter"
  yc serverless function version create \
    --function-name ffmpeg-api \
    --memory=256m \
    --execution-timeout=5s \
    --runtime=python37 \
    --entrypoint=index.handle_api \
    --service-account-id $SERVICE_ACCOUNT_FFMPEG_ID \
    --environment SECRET_ID=$SECRET_ID \
    --environment YMQ_QUEUE_URL=$YMQ_QUEUE_URL \
    --environment DOCAPI_ENDPOINT=$DOCAPI_ENDPOINT \
    --package-bucket-name $S3_BUCKET \
    --package-object-name src.zip
  yc serverless function version create \
    --function-name ffmpeg-converter \
    --memory=2048m \
    --execution-timeout=600s \
    --runtime=python37 \
    --entrypoint=index.handle_process_event \
    --service-account-id $SERVICE_ACCOUNT_FFMPEG_ID \
    --environment SECRET_ID=$SECRET_ID \
    --environment YMQ_QUEUE_URL=$YMQ_QUEUE_URL \
    --environment DOCAPI_ENDPOINT=$DOCAPI_ENDPOINT \
    --environment S3_BUCKET=$S3_BUCKET \
    --package-bucket-name $S3_BUCKET \
    --package-object-name src.zip 
  Тестирование функции
  В консоли управления перейдите из рабочего каталога в раздел Cloud Functions и выберите ранее созданную функцию ffmpeg-api. Перейдите на вкладку Тестирование в боковом меню, выберите шаблон данных Без шаблона и добавьте во вводные данные JSON:
  {"action":"convert", "src_url":"https://disk.yandex.ru/i/38RbVC0spb_jQQ"} 
  Нажмите кнопку Запустить тест. Этим самым мы загрузим файл в хранилище и создадим задачу в БД. Если всё сделано правильно, то вы увидите такой результат:
      {
          "task_id": "133e05c2-1b98-41cc-9aab-b816d71af343"
      } 
  Воспользуемся полученным идентификатором задачи task_id для получения статуса из базы данных. Для этого внесите в вводные данные JSON следующие изменения:
  {"action":"get_task_status", "task_id":"<идентификатор задачи>"} 
  Нажмите кнопку Запустить тест. Так как мы ещё не обрабатывали задачи в очереди, результат очевиден:
      {
          "ready": false
      } 
  Шаг 6. Создание триггера
  Теперь создайте триггер, который будет вызывать функцию обработки сообщений из очереди. После создания триггер начинает работать через пять минут. Он будет брать по одному сообщению и раз в 10 секунд отправлять в функцию:
  yc serverless trigger create message-queue \
    --name ffmpeg \
    --queue $YMQ_QUEUE_ARN \
    --queue-service-account-id $SERVICE_ACCOUNT_FFMPEG_ID \
    --invoke-function-name ffmpeg-converter  \
    --invoke-function-service-account-id $SERVICE_ACCOUNT_FFMPEG_ID \
    --batch-size 1 \
    --batch-cutoff 10s 
  С этого момента очередь начнёт обрабатываться. Можно проверить, готова ли задача, и, если это так, запросить по URL результат обработки из Object Storage.
  Теперь у нас есть функция, которая выполняет функцию API, через которую мы можем ставить задачи в очередь на исполнение. Триггер раз в 10 секунд берет по одному сообщению в очереди и передает функции обработчику. Функция-обработчик формирует результат и обновляет данные в базе данных. При этом мы получаем сконвертированные GIF-файлы из видео.
  Протестируйте систему, используя полученный ранее идентификатор задачи task_id для получения статуса из базы данных. Для этого внесите изменения в вводные данные JSON:
  {"action":"get_task_status", "task_id":"133e05c2-1b98-41cc-9aab-b816d71af343"} 
  Нажмите кнопку Запустить тест. Если задача уже успела обработаться, то вы получите URL.
  Удаление триггера
  По завершении работы не забудьте удалить созданный триггер ffmpeg, иначе он будет продолжать работать:
  yc serverless trigger delete ffmpeg 
  Decision:
  $ export SERVICE_ACCOUNT=$(yc iam service-account create --name ffmpeg-account-for-cf \
    --description "service account for serverless" \
    --format json | jq -r .)
  $ yc iam service-account list
  $ echo "export SERVICE_ACCOUNT_FFMPEG_ID=<ID>" >> ~/.bashrc && . ~/.bashrc
  $ echo $SERVICE_ACCOUNT_FFMPEG_ID
  $ echo "export FOLDER_ID=$(yc config get folder-id)" >> ~/.bashrc && . ~/.bashrc
  $ echo $FOLDER_ID
  $ yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --role storage.viewer
  $ yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --role storage.uploader
  $ yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --role ymq.reader
  $ yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --role ymq.writer
  $ yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --role ydb.admin
  $ yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --role serverless.functions.invoker
  $ yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --role lockbox.payloadViewer
  $ yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --role editor
  $ yc resource-manager folder list-access-bindings $FOLDER_ID
  $ yc resource-manager folder set-access-bindings $FOLDER_ID \
    --access-binding role=storage.viewer,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --access-binding role=storage.uploader,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --access-binding role=ymq.reader,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --access-binding role=ymq.writer,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --access-binding role=ydb.admin,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --access-binding role=serverless.functions.invoker,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --access-binding role=lockbox.payloadViewer,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
    --access-binding role=editor,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID
  $ yc iam access-key create --service-account-name ffmpeg-account-for-cf
  $ yc lockbox secret create --name ffmpeg-sa-key \
    --folder-id $FOLDER_ID \
    --description "keys for serverless" \
    --payload '[{"key": "ACCESS_KEY_ID", "text_value": <ACCESS_KEY_ID>}, {"key": "SECRET_ACCESS_KEY", "text_value": "<SECRET_ACCESS_KEY>"}]'
  $ yc lockbox secret list
  $ yc lockbox secret get --name ffmpeg-sa-key
  $ echo "export SECRET_ID=<SECRET_ID>" >> ~/.bashrc && . ~/.bashrc
  $ echo $SECRET_ID
  $ aws configure
  $ aws sqs create-queue --queue-name ffmpeg --endpoint https://message-queue.api.cloud.yandex.net/
  $ echo "export YMQ_QUEUE_URL=<YMQ_QUEUE_URL>" >> ~/.bashrc && . ~/.bashrc
  $ echo $YMQ_QUEUE_URL
  $ aws sqs get-queue-attributes \
    --endpoint https://message-queue.api.cloud.yandex.net \
    --queue-url $YMQ_QUEUE_URL \
    --attribute-names QueueArn
  $ echo "export YMQ_QUEUE_ARN=<YMQ_QUEUE_ARN>" >> ~/.bashrc && . ~/.bashrc
  $ echo $YMQ_QUEUE_ARN
  $ yc ydb database create ffmpeg \
    --serverless \
    --folder-id $FOLDER_ID
  $ yc ydb database list
  $ yc ydb database get --name ffmpeg
  $ echo "export DOCAPI_ENDPOINT=<DOCAPI_ENDPOINT>" >> ~/.bashrc && . ~/.bashrc
  $ echo $DOCAPI_ENDPOINT
  $ vim tasks.json
  $ cat tasks.json
  {
    "AttributeDefinitions": [
      {
        "AttributeName": "task_id",
        "AttributeType": "S"
      }
    ],
    "KeySchema": [
      {
        "AttributeName": "task_id",
        "KeyType": "HASH"
      }
    ],
    "TableName": "tasks"
  }
  $ aws dynamodb create-table \
    --cli-input-json file://tasks.json \
    --endpoint-url $DOCAPI_ENDPOINT \
    --region ru-central1
  $ echo "export S3_BUCKET=<имя бакета>" >> ~/.bashrc && . ~/.bashrc
  $ echo $S3_BUCKET
  $ echo $SERVICE_ACCOUNT_FFMPEG_ID
  $ echo $SECRET_ID
  $ echo $YMQ_QUEUE_URL
  $ echo $DOCAPI_ENDPOINT
  $ echo $S3_BUCKET
  $ sudo apt install ffmpeg
  $ vim index.py
  $ cat index.py
  import json
  import os
  import subprocess
  import uuid
  from urllib.parse import urlencode
  import boto3
  import requests
  import yandexcloud
  from yandex.cloud.lockbox.v1.payload_service_pb2 import GetPayloadRequest
  from yandex.cloud.lockbox.v1.payload_service_pb2_grpc import PayloadServiceStub
  boto_session = None
  storage_client = None
  docapi_table = None
  ymq_queue = None
  def get_boto_session():
      global boto_session
      if boto_session is not None:
          return boto_session
      # initialize lockbox and read secret value
      yc_sdk = yandexcloud.SDK()
      channel = yc_sdk._channels.channel("lockbox-payload")
      lockbox = PayloadServiceStub(channel)
      response = lockbox.Get(GetPayloadRequest(secret_id=os.environ['SECRET_ID']))
      # extract values from secret
      access_key = None
      secret_key = None
      for entry in response.entries:
          if entry.key == 'ACCESS_KEY_ID':
              access_key = entry.text_value
          elif entry.key == 'SECRET_ACCESS_KEY':
              secret_key = entry.text_value
      if access_key is None or secret_key is None:
          raise Exception("secrets required")
      print("Key id: " + access_key)
      # initialize boto session
      boto_session = boto3.session.Session(
          aws_access_key_id=access_key,
          aws_secret_access_key=secret_key
      )
      return boto_session
  def get_ymq_queue():
      global ymq_queue
      if ymq_queue is not None:
          return ymq_queue
      ymq_queue = get_boto_session().resource(
          service_name='sqs',
          endpoint_url='https://message-queue.api.cloud.yandex.net',
          region_name='ru-central1'
      ).Queue(os.environ['YMQ_QUEUE_URL'])
      return ymq_queue
  def get_docapi_table():
      global docapi_table
      if docapi_table is not None:
          return docapi_table
      docapi_table = get_boto_session().resource(
          'dynamodb',
          endpoint_url=os.environ['DOCAPI_ENDPOINT'],
          region_name='ru-central1'
      ).Table('tasks')
      return docapi_table
  def get_storage_client():
      global storage_client
      if storage_client is not None:
          return storage_client
      storage_client = get_boto_session().client(
          service_name='s3',
          endpoint_url='https://storage.yandexcloud.net',
          region_name='ru-central1'
      )
      return storage_client
  # API handler
  def create_task(src_url):
      task_id = str(uuid.uuid4())
      get_docapi_table().put_item(Item={
          'task_id': task_id,
          'ready': False
      })
      get_ymq_queue().send_message(MessageBody=json.dumps({'task_id': task_id, "src": src_url}))
      return {
          'task_id': task_id
      }
  def get_task_status(task_id):
      task = get_docapi_table().get_item(Key={
          "task_id": task_id
      })
      if task['Item']['ready']:
          return {
              'ready': True,
              'gif_url': task['Item']['gif_url']
          }
      return {'ready': False}
  def handle_api(event, context):
      action = event['action']
      if action == 'convert':
          return create_task(event['src_url'])
      elif action == 'get_task_status':
          return get_task_status(event['task_id'])
      else:
          return {"error": "unknown action: " + action}
  # Converter handler
  def download_from_ya_disk(public_key, dst):
      api_call_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?' + \
                    urlencode(dict(public_key=public_key))
      response = requests.get(api_call_url)
      download_url = response.json()['href']
      download_response = requests.get(download_url)
      with open(dst, 'wb') as video_file:
          video_file.write(download_response.content)
  def upload_and_presign(file_path, object_name):
      client = get_storage_client()
      bucket = os.environ['S3_BUCKET']
      client.upload_file(file_path, bucket, object_name)
      return client.generate_presigned_url('get_object', Params={'Bucket': bucket, 'Key': object_name}, ExpiresIn=3600)
  def handle_process_event(event, context):
      for message in event['messages']:
          task_json = json.loads(message['details']['message']['body'])
          task_id = task_json['task_id']
          # Download video
          download_from_ya_disk(task_json['src'], '/tmp/video.mp4')
          # Convert with ffmpeg
          subprocess.run(['ffmpeg', '-i', '/tmp/video.mp4', '-r', '10', '-s', '320x240', '/tmp/result.gif'])
          result_object = task_id + ".gif"
          # Upload to Object Storage and generate presigned url
          result_download_url = upload_and_presign('/tmp/result.gif', result_object)
          # Update task status in DocAPI
          get_docapi_table().update_item(
              Key={'task_id': task_id},
              AttributeUpdates={
                  'ready': {'Value': True, 'Action': 'PUT'},
                  'gif_url': {'Value': result_download_url, 'Action': 'PUT'},
              }
          )
      return "OK"
  $ pipreqs $PWD --force
  $ zip src.zip index.py requirements.txt ffmpeg
  $ yc serverless function create \
    --name ffmpeg-api \
    --description "function for ffmpeg-api"
  $ yc serverless function create \
    --name ffmpeg-converter \
    --description "function for ffmpeg-converter"
  $ yc serverless function version create \
    --function-name ffmpeg-api \
    --memory=256m \
    --execution-timeout=5s \
    --runtime=python37 \
    --entrypoint=index.handle_api \
    --service-account-id $SERVICE_ACCOUNT_FFMPEG_ID \
    --environment SECRET_ID=$SECRET_ID \
    --environment YMQ_QUEUE_URL=$YMQ_QUEUE_URL \
    --environment DOCAPI_ENDPOINT=$DOCAPI_ENDPOINT \
    --package-bucket-name $S3_BUCKET \
    --package-object-name src.zip
  $ yc serverless function version create \
    --function-name ffmpeg-converter \
    --memory=2048m \
    --execution-timeout=600s \
    --runtime=python37 \
    --entrypoint=index.handle_process_event \
    --service-account-id $SERVICE_ACCOUNT_FFMPEG_ID \
    --environment SECRET_ID=$SECRET_ID \
    --environment YMQ_QUEUE_URL=$YMQ_QUEUE_URL \
    --environment DOCAPI_ENDPOINT=$DOCAPI_ENDPOINT \
    --environment S3_BUCKET=$S3_BUCKET \
    --package-bucket-name $S3_BUCKET \
    --package-object-name src.zip
  $ yc serverless trigger create message-queue \
    --name ffmpeg \
    --queue $YMQ_QUEUE_ARN \
    --queue-service-account-id $SERVICE_ACCOUNT_FFMPEG_ID \
    --invoke-function-name ffmpeg-converter  \
    --invoke-function-service-account-id $SERVICE_ACCOUNT_FFMPEG_ID \
    --batch-size 1 \
    --batch-cutoff 10s
  $ yc serverless trigger delete ffmpeg
  Task:
  Сокращатель ссылок. 
  В рамках этого курса вы изучили несколько ключевых сервисов Yandex Cloud, относящихся к группе Serverless. 
  Давайте объединим их для решения ещё одной практической задачи и создадим сервис, который конвертирует длинные ссылки в короткие.
  Decision:
  Шаг 1. Сервисный аккаунт
  Создание аккаунта
  Создайте сервисный аккаунт с именем serverless-shortener:
  export SERVICE_ACCOUNT_SHORTENER_ID=$(yc iam service-account create --name serverless-shortener \
    --description "service account for serverless" \
    --format json | jq -r .) 
  Проверьте текущий список сервисных аккаунтов:
  yc iam service-account list 
  После проверки запишите идентификатор созданного сервисного аккаунта в переменную SERVICE_ACCOUNT_SHORTENER_ID:
  echo "export SERVICE_ACCOUNT_SHORTENER_ID=<идентификатор сервисного аккаунта>" >> ~/.bashrc && . ~/.bashrc
  echo $SERVICE_ACCOUNT_SHORTENER_ID 
  Назначение ролей
  Добавьте созданному сервисному аккаунту роли editor, storage.viewer и ydb.admin:
  echo "export FOLDER_ID=$(yc config get folder-id)" >> ~/.bashrc && . ~/.bashrc
  echo $FOLDER_ID
  echo "export OAUTH_TOKEN=$(yc config get token)" >> ~/.bashrc && . ~/.bashrc
  echo $OAUTH_TOKEN
  echo "export CLOUD_ID=$(yc config get cloud-id)" >> ~/.bashrc && . ~/.bashrc
  echo $CLOUD_ID
  yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_SHORTENER_ID \
    --role editor
  yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_SHORTENER_ID \
    --role ydb.admin
  yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_SHORTENER_ID \
    --role storage.viewer 
  Шаг 2. Создание бакета в Object Storage
  Сделаем для нашего сервиса веб-интерфейс. Поскольку это будет статическая веб-страница, разместим её в объектном хранилище.
  В консоли управления в вашем рабочем каталоге выберите сервис Object Storage. Нажмите кнопку Создать бакет.
  На странице создания бакета:
      Введите имя бакета. В нашем примере это будет storage-for-serverless-shortener.
      Ограничьте максимальный размер бакета (например 1 ГБ).
      Выберите тип доступа Публичный во всех случаях.
      Выберите класс хранилища Стандартное.
  Нажмите кнопку Создать бакет для завершения операции.
  Создайте и загрузите файл index.html в созданный бакет — это будет стартовая страничка для нашего сокращателя:
  <!DOCTYPE html>
  <html lang="en">
  <head>
      <meta charset="UTF-8">
      <title>Сокращатель URL</title>
      <!-- предостережет от лишнего GET запроса на адрес /favicon.ico -->
      <link rel="icon" href="data:;base64,iVBORw0KGgo=">
  </head>
  <body>
  <h1>Добро пожаловать</h1>
  <form action="javascript:shorten()">
      <label for="url">Введите ссылку:</label><br>
      <input id="url" name="url" type="text"><br>
      <input type="submit" value="Сократить">
  </form>
  <p id="shortened"></p>
  </body>
  <script>
      function shorten() {
          const link = document.getElementById("url").value
          fetch("/shorten", {
              method: 'POST',
              headers: {
                  'Content-Type': 'application/json'
              },
              body: link
          })
              .then(response => response.json())
              .then(data => {
                  const url = data.url
                  document.getElementById("shortened").innerHTML = `<a href=${url}>${url}</a>`
              })
              .catch(error => {
                  document.getElementById("shortened").innerHTML = `<p>Произошла ошибка ${error}, попробуйте еще раз</p>`
              })
      }
  </script>
  </html> 
  Шаг 3. Создание базы данных
      Создадим бессерверную базу данных YDB с именем for-serverless-shortener. Чтобы не переключаться из терминала, снова воспользуемся CLI. Обязательно укажите флаг --serverless для выбора типа создаваемой базы данных:
  yc ydb database create for-serverless-shortener \
    --serverless \
    --folder-id $FOLDER_ID
  yc ydb database list 
      Выполните команду:
  yc ydb database get --name for-serverless-shortener 
  В выводе вы увидите значение endpoint. Оно состоит из двух частей: собственно эндпоинта (обычно это ydb.serverless.yandexcloud.net:2135) и пути базы данных (он указывается после ключевого слова database и начинается с символа /, например /ru-central1/...). Сохраним адрес эндпоинта в переменную YDB_ENDPOINT, а путь базы данных — в переменную YDB_DATABASE. Они пригодятся нам для подключения функции.
  yc ydb database get --name for-serverless-shortener
  echo "export YDB_ENDPOINT=<YDB_ENDPOINT>" >> ~/.bashrc && . ~/.bashrc
  echo $YDB_ENDPOINT
  echo "export YDB_DATABASE=<YDB_DATABASE>" >> ~/.bashrc && . ~/.bashrc
  echo $YDB_DATABASE 
      Для дальнейшей работы нам понадобится утилита ydb:
  curl https://storage.yandexcloud.net/yandexcloud-ydb/install.sh | bash 
      С помощью CLI создадим авторизованный ключ сервисного аккаунта serverless-shortener:
      yc iam key create \
      --service-account-name serverless-shortener \
      --output serverless-shortener.sa 
  Сохраним путь к файлу с ключом в переменную окружения:
  echo "export SA_KEY_FILE=$PWD/serverless-shortener.sa" >> ~/.bashrc && . ~/.bashrc
  echo $SA_KEY_FILE 
      Проверим работоспособность с помощью команды:
  ydb \
    --endpoint $YDB_ENDPOINT \
    --database $YDB_DATABASE \
    --sa-key-file $SA_KEY_FILE \
    discovery whoami \
    --groups 
      Сохраним в файл links.yql SQL-скрипт для создания таблицы:
  CREATE TABLE links
  (
      id Utf8,
      link Utf8,
      PRIMARY KEY (id)
  );
  COMMIT; 
      Запустите создание таблицы, а затем проверьте результат:
  ydb \
    --endpoint $YDB_ENDPOINT \
    --database $YDB_DATABASE \
    --sa-key-file $SA_KEY_FILE \
    scripting yql --file links.yql
  ydb \
    --endpoint $YDB_ENDPOINT \
    --database $YDB_DATABASE \
    --sa-key-file $SA_KEY_FILE \
    scheme describe links 
  Шаг 4. Создание функции
      В рабочем каталоге создайте файл index.py:
  from kikimr.public.sdk.python import client as ydb
  import urllib.parse
  import hashlib
  import base64
  import json
  import os
  def decode(event, body):
      # тело запроса может быть закодировано
      is_base64_encoded = event.get('isBase64Encoded')
      if is_base64_encoded:
          body = str(base64.b64decode(body), 'utf-8')
      return body
  def response(statusCode, headers, isBase64Encoded, body):
      return {
          'statusCode': statusCode,
          'headers': headers,
          'isBase64Encoded': isBase64Encoded,
          'body': body,
      }
  def get_config():
      endpoint = os.getenv("endpoint")
      database = os.getenv("database")
      if endpoint is None or database is None:
          raise AssertionError("Нужно указать обе переменные окружения")
      credentials = ydb.construct_credentials_from_environ()
      return ydb.DriverConfig(endpoint, database, credentials=credentials)
  def execute(config, query, params):
      with ydb.Driver(config) as driver:
          try:
              driver.wait(timeout=5)
          except TimeoutError:
              print("Connect failed to YDB")
              print("Last reported errors by discovery:")
              print(driver.discovery_debug_details())
              return None
          session = driver.table_client.session().create()
          prepared_query = session.prepare(query)
          return session.transaction(ydb.SerializableReadWrite()).execute(
              prepared_query,
              params,
              commit_tx=True
          )
  def insert_link(id, link):
      config = get_config()
      query = """
          DECLARE $id AS Utf8;
          DECLARE $link AS Utf8;
          UPSERT INTO links (id, link) VALUES ($id, $link);
          """
      params = {'$id': id, '$link': link}
      execute(config, query, params)
  def find_link(id):
      print(id)
      config = get_config()
      query = """
          DECLARE $id AS Utf8;
          SELECT link FROM links where id=$id;
          """
      params = {'$id': id}
      result_set = execute(config, query, params)
      if not result_set or not result_set[0].rows:
          return None
      return result_set[0].rows[0].link
  def shorten(event):
      body = event.get('body')
      if body:
          body = decode(event, body)
          original_host = event.get('headers').get('Origin')
          link_id = hashlib.sha256(body.encode('utf8')).hexdigest()[:6]
          # в ссылке могут быть закодированные символы, например, %. это помешает работе api-gateway при редиректе,
          # поэтому следует избавиться от них вызовом urllib.parse.unquote
          insert_link(link_id, urllib.parse.unquote(body))
          return response(200, {'Content-Type': 'application/json'}, False, json.dumps({'url': f'{original_host}/r/{link_id}'}))
      return response(400, {}, False, 'В теле запроса отсутствует параметр url')
  def redirect(event):
      link_id = event.get('pathParams').get('id')
      redirect_to = find_link(link_id)
      if redirect_to:
          return response(302, {'Location': redirect_to}, False, '')
      return response(404, {}, False, 'Данной ссылки не существует')
  # эти проверки нужны, поскольку функция у нас одна
  # в идеале сделать по функции на каждый путь в api-gw
  def get_result(url, event):
      if url == "/shorten":
          return shorten(event)
      if url.startswith("/r/"):
          return redirect(event)
      return response(404, {}, False, 'Данного пути не существует')
  def handler(event, context):
      url = event.get('url')
      if url:
          # из API-gateway url может прийти со знаком вопроса на конце
          if url[-1] == '?':
              url = url[:-1]
          return get_result(url, event)
      return response(404, {}, False, 'Эту функцию следует вызывать при помощи api-gateway') 
  Создайте файл requirements.txt:
  pipreqs $PWD --force 
  Находясь в директории с исходными файлами, упакуйте все нужные файлы в zip-архив:
  zip src.zip index.py requirements.txt 
      В переменные окружения функции необходимо добавить:
      endpoint — нужно указать протокол grpcs:// и добавить значение Эндпоинт из секции YDB эндпоинт, обычно получается grpcs://ydb.serverless.yandexcloud.net:2135.
      database — это значение поля База данных из секции YDB эндпоинт (начинается с /ru-central1/....).
      USE_METADATA_CREDENTIALS — выставите значение переменной в 1.
      Создадим нашу функцию for-serverless-shortener. При этом сразу зададим все необходимые переменные, сервисный аккаунт и сделаем ее публичной:
  yc serverless function create \
    --name for-serverless-shortener \
    --description "function for serverless-shortener"
  yc serverless function version create \
    --function-name for-serverless-shortener \
    --memory=256m \
    --execution-timeout=5s \
    --runtime=python37 \
    --entrypoint=index.handler \
    --service-account-id $SERVICE_ACCOUNT_SHORTENER_ID \
    --environment USE_METADATA_CREDENTIALS=1 \
    --environment endpoint=grpcs://ydb.serverless.yandexcloud.net:2135 \
    --environment database=$YDB_DATABASE \
    --source-path src.zip
  yc serverless function allow-unauthenticated-invoke for-serverless-shortener 
  Шаг 5. Конфигурирование Yandex API Gateway
      Создадим спецификацию for-serverless-shortener.yml со следующим содержанием:
  openapi: 3.0.0
  info:
    title: for-serverless-shortener
    version: 1.0.0
  paths:
    /:
      get:
        x-yc-apigateway-integration:
          type: object_storage
          bucket:             <bucket_name>        # <-- имя бакета
          object:             <html_file>          # <-- имя html-файла
          presigned_redirect: false
          service_account:    <service_account_id> # <-- идентификатор сервисного аккаунта
        operationId: static
    /shorten:
      post:
        x-yc-apigateway-integration:
          type: cloud_functions
          function_id:  <function_id>               # <-- идентификатор функции
        operationId: shorten
    /r/{id}:
      get:
        x-yc-apigateway-integration:
          type: cloud_functions
          function_id:  <function_id>               # <-- идентификатор функции
        operationId: redirect
        parameters:
          - description: id of the url
            explode: false
            in: path
            name: id
            required: true
            schema:
              type: string
            style: simple 
  Не забудьте подставить в спецификацию актуальные для вас значения переменных.
      Используем спецификацию для инициализации:
  yc serverless api-gateway create \
    --name for-serverless-shortener \
    --spec=for-serverless-shortener.yml \
    --description "for serverless shortener" 
  В результате успешного создания API-шлюза получим значение параметра domain:
  yc serverless api-gateway list
  yc serverless api-gateway get --name for-serverless-shortener 
      Чтобы проверить работоспособность API-шлюза и созданного приложения целиком, скопируйте служебный домен (вида https://<идентификатор API Gateway>.apigw.yandexcloud.net/) и вставьте адрес в браузер.
  Добавляйте адреса сайтов в форму, они будут сохранятся в базу данных. А вам будет доступна ссылка, за которой будет скрываться оригинальный адрес. Ваше приложение полностью работоспособно. Теперь вы умеете использовать serverless-стеком технологий Yandex Cloud.
  Итак, вы создали приложение с использованием Cloud Functions, API Gateway, Object Storage и Yandex Database. Конечно, вы можете развивать его и дальше, расширяя функциональность.
  Decision:
  $ export SERVICE_ACCOUNT_SHORTENER_ID=$(yc iam service-account create --name serverless-shortener \
    --description "service account for serverless" \
    --format json | jq -r .)
  $ yc iam service-account list
  $ echo "export SERVICE_ACCOUNT_SHORTENER_ID=<идентификатор сервисного аккаунта>" >> ~/.bashrc && . ~/.bashrc
  $ echo $SERVICE_ACCOUNT_SHORTENER_ID
  $ echo "export FOLDER_ID=$(yc config get folder-id)" >> ~/.bashrc && . ~/.bashrc
  $ echo $FOLDER_ID
  $ echo "export OAUTH_TOKEN=$(yc config get token)" >> ~/.bashrc && . ~/.bashrc
  $ echo $OAUTH_TOKEN
  $ echo "export CLOUD_ID=$(yc config get cloud-id)" >> ~/.bashrc && . ~/.bashrc
  $ echo $CLOUD_ID
  $ yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_SHORTENER_ID \
    --role editor
  $ yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_SHORTENER_ID \
    --role ydb.admin
  $ yc resource-manager folder add-access-binding $FOLDER_ID \
    --subject serviceAccount:$SERVICE_ACCOUNT_SHORTENER_ID \
    --role storage.viewer
  $ vim index.html
  $ cat index.html
  <!DOCTYPE html>
  <html lang="en">
  <head>
      <meta charset="UTF-8">
      <title>Сокращатель URL</title>
      <!-- предостережет от лишнего GET запроса на адрес /favicon.ico -->
      <link rel="icon" href="data:;base64,iVBORw0KGgo=">
  </head>
  <body>
  <h1>Добро пожаловать</h1>
  <form action="javascript:shorten()">
      <label for="url">Введите ссылку:</label><br>
      <input id="url" name="url" type="text"><br>
      <input type="submit" value="Сократить">
  </form>
  <p id="shortened"></p>
  </body>
  <script>
      function shorten() {
          const link = document.getElementById("url").value
          fetch("/shorten", {
              method: 'POST',
              headers: {
                  'Content-Type': 'application/json'
              },
              body: link
          })
              .then(response => response.json())
              .then(data => {
                  const url = data.url
                  document.getElementById("shortened").innerHTML = `<a href=${url}>${url}</a>`
              })
              .catch(error => {
                  document.getElementById("shortened").innerHTML = `<p>Произошла ошибка ${error}, попробуйте еще раз</p>`
              })
      }
  </script>
  </html>
  $ yc ydb database create for-serverless-shortener \
    --serverless \
    --folder-id $FOLDER_ID
  $ yc ydb database list
  $ yc ydb database get --name for-serverless-shortener
  $ echo "export YDB_ENDPOINT=<YDB_ENDPOINT>" >> ~/.bashrc && . ~/.bashrc
  $ echo $YDB_ENDPOINT
  $ echo "export YDB_DATABASE=<YDB_DATABASE>" >> ~/.bashrc && . ~/.bashrc
  $ echo $YDB_DATABASE
  $ curl https://storage.yandexcloud.net/yandexcloud-ydb/install.sh | bash
  $ yc iam key create \
  --service-account-name serverless-shortener \
  --output serverless-shortener.sa
  $ echo "export SA_KEY_FILE=$PWD/serverless-shortener.sa" >> ~/.bashrc && . ~/.bashrc
  $ echo $SA_KEY_FILE
  $ ydb \
    --endpoint $YDB_ENDPOINT \
    --database $YDB_DATABASE \
    --sa-key-file $SA_KEY_FILE \
    discovery whoami \
    --groups
  $ vim links.yql
  $ cat links.yql
  CREATE TABLE links
  (
      id Utf8,
      link Utf8,
      PRIMARY KEY (id)
  );
  COMMIT;
  $ ydb \
    --endpoint $YDB_ENDPOINT \
    --database $YDB_DATABASE \
    --sa-key-file $SA_KEY_FILE \
    scripting yql --file links.yql
  $ ydb \
    --endpoint $YDB_ENDPOINT \
    --database $YDB_DATABASE \
    --sa-key-file $SA_KEY_FILE \
    scheme describe links
  $ vim index1.py
  $ cat index1.py
  import ydb
  import urllib.parse
  import hashlib
  import base64
  import json
  import os
  def decode(event, body):
      # тело запроса может быть закодировано
      is_base64_encoded = event.get('isBase64Encoded')
      if is_base64_encoded:
          body = str(base64.b64decode(body), 'utf-8')
      return body
  def response(statusCode, headers, isBase64Encoded, body):
      return {
          'statusCode': statusCode,
          'headers': headers,
          'isBase64Encoded': isBase64Encoded,
          'body': body,
      }
  def get_config():
      endpoint = os.getenv("endpoint")
      database = os.getenv("database")
      if endpoint is None or database is None:
          raise AssertionError("Нужно указать обе переменные окружения")
      credentials = ydb.construct_credentials_from_environ()
      return ydb.DriverConfig(endpoint, database, credentials=credentials)
  def execute(config, query, params):
      with ydb.Driver(config) as driver:
          try:
              driver.wait(timeout=5)
          except TimeoutError:
              print("Connect failed to YDB")
              print("Last reported errors by discovery:")
              print(driver.discovery_debug_details())
              return None
          session = driver.table_client.session().create()
          prepared_query = session.prepare(query)
          return session.transaction(ydb.SerializableReadWrite()).execute(
              prepared_query,
              params,
              commit_tx=True
          )
  def insert_link(id, link):
      config = get_config()
      query = """
          DECLARE $id AS Utf8;
          DECLARE $link AS Utf8;
          UPSERT INTO links (id, link) VALUES ($id, $link);
          """
      params = {'$id': id, '$link': link}
      execute(config, query, params)
  def find_link(id):
      print(id)
      config = get_config()
      query = """
          DECLARE $id AS Utf8;
          SELECT link FROM links where id=$id;
          """
      params = {'$id': id}
      result_set = execute(config, query, params)
      if not result_set or not result_set[0].rows:
          return None
      return result_set[0].rows[0].link
  def shorten(event):
      body = event.get('body')
      if body:
          body = decode(event, body)
          original_host = event.get('headers').get('Origin')
          link_id = hashlib.sha256(body.encode('utf8')).hexdigest()[:6]
          # в ссылке могут быть закодированные символы, например, %. это помешает работе api-gateway при редиректе,
          # поэтому следует избавиться от них вызовом urllib.parse.unquote
          insert_link(link_id, urllib.parse.unquote(body))
          return response(200, {'Content-Type': 'application/json'}, False, json.dumps({'url': f'{original_host}/r/{link_id}'}))
      return response(400, {}, False, 'В теле запроса отсутствует параметр url')
  def redirect(event):
      link_id = event.get('pathParams').get('id')
      redirect_to = find_link(link_id)
      if redirect_to:
          return response(302, {'Location': redirect_to}, False, '')
      return response(404, {}, False, 'Данной ссылки не существует')
  # эти проверки нужны, поскольку функция у нас одна
  # в идеале сделать по функции на каждый путь в api-gw
  def get_result(url, event):
      if url == "/shorten":
          return shorten(event)
      if url.startswith("/r/"):
          return redirect(event)
      return response(404, {}, False, 'Данного пути не существует')
  def handler(event, context):
      url = event.get('url')
      if url:
          # из API-gateway url может прийти со знаком вопроса на конце
          if url[-1] == '?':
              url = url[:-1]
          return get_result(url, event)
      return response(404, {}, False, 'Эту функцию следует вызывать при помощи api-gateway')
  $ vim requirements.txt 
  $ cat requirements.txt 
  ...
  ydb==2.13.3
  $ zip src.zip index1.py requirements.txt
  $ yc serverless function create \
    --name for-serverless-shortener \
    --description "function for serverless-shortener"
  $ yc serverless function version create \
    --function-name for-serverless-shortener \
    --memory=256m \
    --execution-timeout=5s \
    --runtime=python39 \
    --entrypoint=index.handler \
    --service-account-id $SERVICE_ACCOUNT_SHORTENER_ID \
    --environment USE_METADATA_CREDENTIALS=1 \
    --environment endpoint=grpcs://ydb.serverless.yandexcloud.net:2135 \
    --environment database=$YDB_DATABASE \
    --source-path src.zip
  $ yc serverless function allow-unauthenticated-invoke for-serverless-shortener
  $ vim for-serverless-shortener.yml
  $ cat for-serverless-shortener.yml
  openapi: 3.0.0
  info:
    title: for-serverless-shortener
    version: 1.0.0
  paths:
    /:
      get:
        x-yc-apigateway-integration:
          type: object_storage
          bucket:             <bucket_name>        # <-- имя бакета
          object:             <html_file>          # <-- имя html-файла
          presigned_redirect: false
          service_account:    <service_account_id> # <-- идентификатор сервисного аккаунта
        operationId: static
    /shorten:
      post:
        x-yc-apigateway-integration:
          type: cloud_functions
          function_id:  <function_id>               # <-- идентификатор функции
        operationId: shorten
    /r/{id}:
      get:
        x-yc-apigateway-integration:
          type: cloud_functions
          function_id:  <function_id>               # <-- идентификатор функции
        operationId: redirect
        parameters:
          - description: id of the url
            explode: false
            in: path
            name: id
            required: true
            schema:
              type: string
            style: simple
  $ yc serverless api-gateway create \
    --name for-serverless-shortener \
    --spec=for-serverless-shortener.yml \
    --description "for serverless shortener"
  $ yc serverless api-gateway list
  $ yc serverless api-gateway get --name for-serverless-shortener

Безопасность в Yandex Cloud
  Task:
  В целях безопасности облачных ресурсов реализовал права на управление сервисным аккаунтом, организовал защищённый канал настроив IPSec VPN-туннель между двумя VPN-шлюзами в ВМ с помощью демона strongSwan, реализовал для домена втоматический выпуск сертификата с помощью Certificate Manager
  Task:
  Права доступа и роли для сервисного аккаунта.
  В этом уроке вы научитесь работать с сервисными аккаунтами и назначать для них роли и права доступа к объектам. 
  В качестве объекта будет выступать созданный в сервисе KMS ключ шифрования. 
  Предположим, что перед вами стоит задача использовать сервисный аккаунт для ротации ключей.
  Чтобы решить эту задачу, понадобится выполнить следующие шаги:
  - Создать сервисный аккаунт.
  - Получить права на управление этим сервисным аккаунтом.
  - Создать статические ключи доступа и привязать их к сервисному аккаунту, чтобы он мог пройти авторизацию в сервисе KMS.
  - Создать в сервисе KMS ключ шифрования и назначить сервисному аккаунту роль kms.admin для управления этим ключом.
  - И, наконец, ротировать ключ, то есть создать его новую версию с такими же параметрами, из-под сервисного аккаунта.
  - Нужно заметить, что через консоль управления сервисному аккаунту можно назначить роль только на каталог, в котором он был создан. Роли на все остальные ресурсы назначаются с помощью CLI или API. Поэтому для выполнения этой практической работы вам понадобится вспомнить, как пользоваться утилитой yc, чему вы уже научились в курсе «DevOps и автоматизация».
  Decision:
  ШАГ 1
  Для начала создадим сервисный аккаунт. В консоли управления войдите в каталог облака, в котором вы будете выполнять эту практическую работу, и перейдите на вкладку Сервисные аккаунты. Нажмите кнопку Создать сервисный аккаунт.
  В открывшемся окне задайте для нового сервисного аккаунта имя и, при желании, добавьте описание. Здесь аккаунту также можно добавить роли на каталог, в котором он создаётся.
  Оставьте поле с ролями пустым и нажмите Создать.
  ШАГ 2
  Настройте для вашего аккаунта на Яндексе доступ на авторизацию под созданным сервисным аккаунтом.
  Для начала убедитесь, что вы авторизованы в аккаунте с ролью admin. Чтобы это проверить, выполните команду
  yc iam role list 
  Вы увидите список ролей вашего аккаунта. Примерно такой (роль admin должна в нем присутствовать):
  Узнайте идентификатор своего аккаунта. Он понадобится, чтобы добавить вашему аккаунту роль editor на созданный сервисный аккаунт (сервисный аккаунт тоже является ресурсом, и для работы с ним нужна соответствующая роль). Воспользуйтесь для этого командой:
  yc iam user-account get <имя_вашего_аккаунта> 
  Кроме того, нужно узнать идентификатор созданного сервисного аккаунта. Это можно сделать в разделе Сервисные аккаунты консоли управления. Выбрав нужный аккаунт в списке, вы перейдёте на страницу с детальной информацией о нем.
  Теперь предоставьте вашему пользовательскому аккаунту права на управление созданным сервисным аккаунтом. Для этого нужно выполнить команду
  yc iam service-account add-access-binding <ID_сервисного_аккаунта> \
  --role editor --subject userAccount:<ID_пользовательского_аккаунта> 
  ШАГ 3
  Настройте аутентификацию под сервисным аккаунтом с вашей машины.
  Сначала нужно создать статические ключи доступа и сохранить их в json-файле (например key.json).
  Воспользуйтесь для этого командой
  yc --folder-name <имя_каталога> iam key create \
  --service-account-name <имя_сервисного_аккаунта> --output key.json 
  После выполнения команды вы получите идентификатор созданной ключевой пары. Используя статические ключи доступа, можно получить IAM-токен для авторизации в сервисах.
  Теперь нужно создать профиль, от имени которого будут выполняться операции из-под сервисного аккаунта. Для этого придумайте имя этого профиля (например yc-lab23-profile) и выполните команду:
  yc config profile create <имя_профиля> 
  Привяжите к этому профилю ранее созданный статический ключ доступа с помощью команды:
  yc config set service-account-key key.json 
  Чтобы убедиться, что всё сделано правильно, выведите информацию об авторизации и ключах доступа
  yc config list 
  Вы должны получить примерно такой результат:
  ШАГ 4
  Теперь нужно создать ключ шифрования, ротацией которого вы будете управлять с помощью сервисного аккаунта. Для этого перейдите в дашборд каталога в консоли управления, нажмите кнопку Создать ресурс и выберите Ключ шифрования.
  В открывшемся окне задайте для ключа имя и нажмите кнопку Создать. Новый ключ появится в списке в открывшемся разделе Ключи.
  Чтобы назначить сервисному аккаунту роль для какого-либо ресурса, нужно знать идентификатор этого ресурса. Нажмите строку с созданным ключом, чтобы перейти на страницу детальной информации о нём, и скопируйте ID ключа.
  Перейдем к назначению сервисному аккаунту роли kms.admin для управления созданным ключом шифрования. Перед этим нужно сначала вернуться в профиль вашего аккаунта.
  yc config profile activate <имя_профиля> 
  Выполните команду:
  yc --folder-name <имя_каталога> kms symmetric-key \
  add-access-binding <ID_ключа_шифрования> --role kms.admin \
  --subject serviceAccount:<ID_сервисного_аккаунта> 
  Теперь с помощью сервисного аккаунта вы можете управлять этим ключом шифрования.
  ШАГ 5
  В CLI переключитесь обратно в профиль сервисного аккаунта:
  yc config profile activate <имя_профиля_сервисного_аккаунта> 
  Ротируйте ключ шифрования:
  yc kms symmetric-key rotate <ID_ключа> 
  После выполнения команды перейдите на страницу детальной информации о ключе и откройте вкладку Операции.
  Вы увидите, что операция по ротации ключа выполнена под вашим сервисным аккаунтом. Значит, всё получилось и задача решена!
  Decision:
  $ yc iam service-account create --name <имя_сервисного_аккаунта>
  $ yc iam service-account create --name security-labs \
  --description "Service account for Security course labs"
  $ yc iam role list
  $ yc iam user-account get <ваш_логин>
  $ yc iam service-account list
  $ yc iam service-account get <имя_сервисного_аккаунта>
  $ yc iam service-account add-access-binding <id_сервисного_аккаунта> \
  --role editor \
  --subject userAccount:<id_пользовательского_аккаунта>
  $ vim key.json
  $ cat key.json
  {
    "id": "YOUR-ID1",
    "service_account_id": "YOUR-ID2",
    "created_at": "2023-12-09T01:19:35.625144946Z",
    "key_algorithm": "RSA_2048",
    "public_key": "-----BEGIN PUBLIC KEY-----\nYOUR-KEY1\n-----END PUBLIC KEY-----\n",
    "private_key": "PLEASE DO NOT REMOVE THIS LINE! Yandex.Cloud SA Key ID <YOUR-ID1>\n-----BEGIN PRIVATE KEY-----\nYOUR-KEY2\n-----END PRIVATE KEY-----\n"
  }
  $ yc iam key create \
  --service-account-name <имя_сервисного_аккаунта> \
  --output key.json
  $ yc config profile create <имя_профиля_сервисного_аккаунта>
  $ yc config set service-account-key key.json
  $ yc config set folder-id <идентификатор_рабочего_каталога>
  $ yc config list
  $ yc config profile list
  $ yc config profile activate <имя_основного_профиля>
  $ yc kms symmetric-key add-access-binding \
  --id <id_ключа_шифрования> \
  --role kms.admin \
  --subject serviceAccount:<id_сервисного_аккаунта>
  $ yc config profile activate <имя_профиля_сервисного_аккаунта>
  $ yc kms symmetric-key rotate --id <id_ключа_шифрования>
  Task:
  Организация защищённого канала. 
  Защита данных, передаваемых между вашей локальной инфраструктурой и облаком, — важный элемент информационной безопасности. А удалённая работа, которая получила распространение в период пандемии коронавируса и сейчас закрепилась в практиках многих компаний, сделала эту задачу ещё более актуальной.
  Чтобы защитить передаваемую информацию, используют VPN (Virtual Private Network) — технологию, позволяющую развернуть защищённое сетевое соединение «поверх» незащищённой сети (чаще всего это интернет). VPN-соединение представляет собой канал передачи данных между двумя узлами. Этот канал обычно называют VPN-туннелем. Если за одним из узлов находится целая сеть, то его называют VPN-шлюзом.
  Механизм работы VPN:
  - Перед созданием туннеля узлы идентифицируют друг друга, чтобы удостовериться, что шифрованные данные будут отправлены на нужный узел.
  - На обоих узлах нужно заранее определить, какие протоколы будут использоваться для шифрования и обеспечения целостности данных.
  - Узлы сверяют настройки, чтобы договориться об используемых алгоритмах. Если настройки разные, туннель не создаётся.
  - Если сверка прошла успешно, то создаётся ключ, который используется для симметричного шифрования.
  Этот механизм регламентируют несколько стандартов. Один из самых популярных — IPSec (Internet Protocol Security).
  На этом уроке вы научитесь настраивать IPSec VPN-туннель между двумя VPN-шлюзами с помощью демона strongSwan. Один шлюз вы настроите на виртуальной машине в Yandex Cloud, второй — на своей локальной машине или виртуальной машине в другой облачной сети.
  Decision:
  Шаг 1. Создание ресурсов
  Для практической работы вам понадобится сеть и подсеть в Yandex Cloud, а также созданная в этой подсети тестовая ВМ без публичного IP-адреса. Создайте эти ресурсы, если у вас их нет.
  Теперь создадим IPSec-инстанс — ВМ, которая будет служить шлюзом для IPSec-туннеля. Чтобы это сделать:
  Откройте ваш каталог, нажмите кнопку Создать ресурс и выберите пункт Виртуальная машина.
  В поле Имя задайте имя ВМ, например ipsec-instance.
  Выберите зону доступности, где находится подсеть, к которой будет подключён IPSec-инстанс, и тестовая ВМ.
  В разделе Выбор образа/загрузочного диска перейдите в блок Cloud Marketplace и выберите образ IPSec-инстанс.
  В блоке Сетевые настройки выберите нужную сеть, подсеть и назначьте ВМ публичный IP-адрес из списка или автоматически.
  Важно использовать только статические публичные IP-адреса из списка или сделать IP-адрес ВМ статическим после её создания. Динамический IP-адрес может измениться после перезагрузки ВМ, и туннель перестанет работать.
  В блоке Доступ укажите логин и SSH-ключ для доступа к ВМ.
  Нажмите кнопку Создать ВМ.
  Виртуальная машина готова.
  Шаг 2. Настраиваем IPSec
  Теперь настроим шлюз с публичным IP-адресом, который будет устанавливать IPSec-соединение с удалённым шлюзом (вашей локальной машиной или ВМ в другой облачной сети).
  Вы можете создать в своём каталоге ещё одну облачную сеть с подсетью, создать в ней IPSec-инстанс из образа и использовать его в качестве удалённого шлюза. Либо можно использовать в качестве шлюза машину в вашей локальной сети. Вам понадобится публичный IP-адрес удалённого шлюза и CIDR подсети.
  Допустим, публичный IP-адрес вашего шлюза — 130.193.32.25, а за ним находится подсеть c префиксом подсети CIDR 10.128.0.0/24. Шлюз будет устанавливать IPSec-соединение с удалённым шлюзом с IP-адресом, например, 1.1.1.1, за которым находится подсеть с префиксом подсети CIDR 192.168.0.0/24.
  Подключитесь к ВМ IPSec-инстанс по SSH:
  ssh <имя пользователя>@130.193.32.25 
  Откройте конфигурацию IPSec:
  sudo nano /etc/ipsec.conf 
  В разделе config setup файла конфигурации задайте следующие параметры:
  config setup
          charondebug="all"
          uniqueids=yes
          strictcrlpolicy=no 
  Добавьте новый раздел с описанием тестового соединения, например conn cloud-to-hq.
  Задайте параметры тестового соединения:
  leftid — публичный IP-адрес IPSec-инстанса.
  leftsubnet — CIDR подсети, к которой подключён IPSec-инстанс.
  right — публичный IP-адрес шлюза на другом конце VPN-туннеля.
  rightsubnet — CIDR подсети, к которой подключён VPN-шлюз на другом конце VPN-туннеля.
  Параметры ike и esp — это алгоритмы шифрования, которые поддерживаются на удалённом шлюзе. Перечень поддерживаемых алгоритмов можно посмотреть на сайте strongSwan: IKEv1 и IKEv2.
  Укажите остальные настройки, консультируясь с документацией strongSwan и учитывая настройки удалённого шлюза.
  У вас должна получиться примерно такая конфигурация:
  conn cloud-to-hq
          authby=secret
          left=%defaultroute
          leftid=130.193.32.25
          leftsubnet=10.128.0.0/24
          right=1.1.1.1
          rightsubnet=192.168.0.0/24
          ike=aes256-sha2_256-modp1024!
          esp=aes256-sha2_256!
          keyingtries=0
          ikelifetime=1h
          lifetime=8h
          dpddelay=30
          dpdtimeout=120
          dpdaction=restart
          auto=start 
  Сохраните изменения и закройте файл.
  Откройте файл /etc/ipsec.secrets и укажите в нём пароль для установки соединения:
  130.193.32.25 1.1.1.1 : PSK "<пароль>" 
  Перезапустите strongSwan:
  sudo systemctl restart strongswan-starter 
  Шаг 3. Настраиваем статическую маршрутизацию
  Теперь нужно настроить маршрутизацию между IPSec-инстансом и тестовой ВМ без публичного IP-адреса. Для этого создадим таблицу маршрутизации и добавим в неё статические маршруты.
  Откройте сервис Virtual Private Cloud в каталоге, где требуется создать статический маршрут.
  Выберите раздел Таблицы маршрутизации в панели слева и нажмите кнопку Создать таблицу маршрутизации.
  Задайте имя таблицы маршрутизации, выберите сеть, в которой требуется её создать, и нажмите кнопку Добавить маршрут.
  В открывшемся окне введите префикс подсети назначения на удалённой стороне, в примере это 192.168.0.0/24.
  В поле Next hop укажите внутренний IP-адрес IPSec-шлюза и нажмите кнопку Добавить.
  Нажмите кнопку Создать таблицу маршрутизации.
  Чтобы использовать статические маршруты, нужно привязать таблицу маршрутизации к подсети. Для этого в разделе Подсети, в строке нужной подсети, нажмите кнопку … и в открывшемся меню выберите пункт Привязать таблицу маршрутизации.
  В открывшемся окне выберите созданную таблицу и нажмите кнопку Привязать. Созданный маршрут можно применять и к другим подсетям этой сети.
  Шаг 4. Настраиваем IPSec на другом шлюзе
  Для работы VPN-туннеля нужно настроить второй шлюз.
  Настройте strongSwan аналогично первому IPSec-шлюзу, но с зеркальными настройками IP-адресов и подсетей в файле /etc/ipsec.conf. Должна получиться такая конфигурация:
  conn hq-to-cloud
          authby=secret
          left=%defaultroute
          leftid=1.1.1.1
          leftsubnet=192.168.0.0/24
          right=130.193.32.25
          rightsubnet=10.128.0.0/24
          ike=aes256-sha2_256-modp1024!
          esp=aes256-sha2_256!
          keyingtries=0
          ikelifetime=1h
          lifetime=8h
          dpddelay=30
          dpdtimeout=120
          dpdaction=restart
          auto=start 
  Укажите пароль для соединения в файле /etc/ipsec.secrets, указав IP-адреса шлюзов в обратном порядке:
  1.1.1.1 130.193.32.25 : PSK "<пароль>" 
  Перезапустите strongSwan:
  sudo systemctl restart strongswan-starter 
  Шаг 5. Проверяем, что всё работает
  Чтобы убедиться, что туннель между шлюзами установлен, выполните на любом из шлюзов команду:
  sudo ipsec status 
  Если всё в порядке, то у вас должно появиться примерно такое сообщение:
  Security Associations (1 up, 0 connecting):
  hq-to-cloud[3]: ESTABLISHED 29 minutes ago, 10.128.0.26[130.193.33.12]...192.168.0.23[1.1.1.1]
  hq-to-cloud{3}:  INSTALLED, TUNNEL, reqid 3, ESP in UDP SPIs: c7fa371d_i ce8b91ad_o
  hq-to-cloud{3}:   10.128.0.0/24 === 192.168.0.0/24 
  Статус ESTABLISHED означает, что туннель между шлюзами создан.
  Сведения об установке и работе соединения находятся в логах strongSwan. Просмотреть логи можно с помощью команды:
  sudo journalctl -u strongswan-starter 
  Проверить статус демона strongSwan можно командой:
  systemctl status strongswan-starter 
  Осталось проверить связность соединения. Для этого создайте ещё одну тестовую виртуальную машину за вторым шлюзом, а затем пропингуйте одну тестовую машину с другой.
  Decision:
  $ ssh <имя пользователя>@130.193.32.25
  $ sudo vim /etc/ipsec.conf
  $ sudo cat /etc/ipsec.conf
  ...
  config setup
          charondebug="all"
          uniqueids=yes
          strictcrlpolicy=no
  ...
  conn cloud-to-hq
          authby=secret
          left=%defaultroute
          leftid=130.193.32.25
          leftsubnet=10.128.0.0/24
          right=1.1.1.1
          rightsubnet=192.168.0.0/24
          ike=aes256-sha2_256-modp1024!
          esp=aes256-sha2_256!
          keyingtries=0
          ikelifetime=1h
          lifetime=8h
          dpddelay=30
          dpdtimeout=120
          dpdaction=restart
          auto=start
  $ sudo vim /etc/ipsec.secrets
  $ sudo cat /etc/ipsec.secrets
  130.193.32.25 1.1.1.1 : PSK "<пароль>"
  $ sudo systemctl restart strongswan-starter
  $ exit 
  $ ssh <имя пользователя>@130.193.32.26
  $ sudo vim /etc/ipsec.conf
  $ sudo cat /etc/ipsec.conf
  ...
  config setup
          charondebug="all"
          uniqueids=yes
          strictcrlpolicy=no
  ...
  conn hq-to-cloud
          authby=secret
          left=%defaultroute
          leftid=1.1.1.1
          leftsubnet=192.168.0.0/24
          right=130.193.32.25
          rightsubnet=10.128.0.0/24
          ike=aes256-sha2_256-modp1024!
          esp=aes256-sha2_256!
          keyingtries=0
          ikelifetime=1h
          lifetime=8h
          dpddelay=30
          dpdtimeout=120
          dpdaction=restart
          auto=start
  $ sudo vim /etc/ipsec.secrets
  $ sudo cat /etc/ipsec.secrets
  1.1.1.1 130.193.32.25 : PSK "<пароль>"
  $ sudo systemctl restart strongswan-starter
  $ ssh <имя пользователя>@130.193.32.25
  $ sudo ipsec status 
  $ sudo journalctl -u strongswan-starter
  $ systemctl status strongswan-starter
  Task:
  Выпуск сертификата для сайта.
  В этой практической работе мы зарегистрируем домен, привяжем его к бакету в объектном хранилище и настроим для этого домена автоматический выпуск сертификата с помощью Certificate Manager.
  Decision:
  Шаг 1
  Если у вас нет своего домена, зарегистрируйте временный домен, например, на сайте freenom.com:
  Проверьте на сайте доступность имени, которое вы придумали для своего домена.
  Введите имя вместе с доменом верхнего уровня, например testpracticum2022.ml, иначе при попытке зарезервировать домен сервис будет сообщать, что домен занят.
  Если это имя доступно, добавьте домен в корзину и укажите свой email для подтверждения.
  Проверьте почту и подтвердите регистрацию домена.
  Обновите страницу с заказом.
  После подтверждения регистрации домена зайдите в объектное хранилище (Object Storage) и создайте новый публичный бакет. Его название должно совпадать с полным названием домена.
  Переключите доступ на чтение объектов в Публичный. Загрузите в бакет файлы статического сайта (вы можете воспользоваться файлами из практической работы курса «Хранение и анализ данных».
  Выберите на панели управления раздел Веб-сайт, переключите бакет в режим Хостинг и нажмите Сохранить.
  Шаг 2
  Настроить защищённый доступ к бакету можно двумя способами: загрузить сертификат прямо в бакет или с помощью Certificate Manager. Воспользуемся вторым способом.
  В консоли управления перейдите в сервис Certificate Manager. Для выпуска сертификата с помощью этого сервиса подтвердите владение доменом: в разделе Сертификаты нажмите кнопку Добавить сертификат и выберите Сертификат Let’s Encrypt.
  В открывшемся окне задайте имя создаваемого сертификата и заполните поле с именем вашего домена. Нажмите кнопку Создать.
  Сервис автоматически направит запрос на создание сертификата, а домен перейдёт в статус проверки.
  Для выпуска сертификата необходимо подтвердить статус владения доменом. Откройте страницу с деталями запроса на сертификат:
  На этой странице для нас важны два поля: имя DNS-записи и её значение. Если вы создавали домен на freenom.com, то перейдите в личный кабинет на этом сайте, выберите раздел Services → My Domains и нажмите кнопку Manage Domains:
  Выберите Manage Freenom DNS:
  В открывшемся редакторе добавьте TXT-запись для подтверждения владения доменом. В качестве названия записи задайте _acme-challenge без полного названия домена. В качестве значения TXT-записи — значение со страницы проверки прав на домен в консоли управления Yandex Cloud.
  Аналогично внесите значение CNAME-записи со страницы проверки прав на домен в консоли управления Yandex Cloud.
  Добавьте также запись CNAME для привязки поддомена WWW к вашему бакету:
  В поле Target укажите полное имя бакета, включая .website.yandexcloud.net. Сохраните сделанные изменения.
  Если вы используете собственный домен, задайте параметры DNS в настройках вашего DNS-сервера. Для применения настроек DNS потребуется некоторое время — обычно до 15 минут.
  После окончания проверки домена Certificate Manager автоматически выпустит сертификат.
  Шаг 3
  Теперь настроим доступ к сайту, то есть к созданному бакету, по протоколу HTTPS с помощью сертификата. Для этого перейдите в раздел HTTPS и нажмите кнопку Настроить.
  В поле Источник выберите Certificate Manager, в поле Сертификат — ранее выпущенный сертификат. Нажмите кнопку Сохранить.
  Теперь ваш сайт доступен по протоколу HTTPS. Чтобы проверить это, откройте его в браузере. В адресной строке браузера должен отображаться значок защищённого соединения.
  Task:
  Создание и ротация ключей шифрования
  На прошлом уроке вы познакомились с возможностями сервиса управления ключами шифрования KMS. В этой практической работе вы научитесь создавать ключи шифрования и управлять ими, а также использовать эти ключи для шифрования и расшифрования данных.
  Decision:
  Шаг 1
  Перейдите в панель управления Yandex Cloud, нажмите кнопку Создать ресурс и выберите из выпадающего списка пункт Ключ шифрования.
  Задайте для создаваемого ключа имя (например yc-lab-key1), заполните поле Описание (это необязательно) и выберите алгоритм шифрования. Предположим, что ключ нужно ротировать каждый день. Для этого в поле Период ротации, дни выберите вариант Своё значение и введите число 1 в поле справа.
  Нажмите кнопку Создать. Когда операция создания ключа завершится, новый ключ появится в списке.
  Нажав на строку с ключом, вы перейдёте на страницу детальной информации. На ней приведены все параметры ключа, а также список его версий. Обратите внимание, что ID (идентификатор) ключа и ID конкретной версии ключа отличаются. Важно их не путать.
  Шаг 2
  Давайте используем созданный ключ для шифрования и расшифрования данных. Создайте у себя на диске файл (например, текстовый файл с именем plain.txt). Добавьте в него любой текст и сохраните содержимое. Напомним, что размер файла не должен превышать 32 килобайта.
  Запустите утилиту командной строки (bash или cmd) и перейдите в каталог с файлом plain.txt. Зашифруйте этот файл с помощью утилиты yc, а результат операции шифрования выведите в файл encrypted.txt. Для этого выполните команду:
  yc kms symmetric-crypto encrypt --id <ID ключа> --plaintext-file plain.txt --ciphertext-file encrypted.txt 
  После выполнения команды будет создан файл encrypted.txt, который содержит зашифрованный текст. Утилита yc также выведет информацию о том, каким ключом и какой его версией файл был зашифрован.
  Шаг 3
  Теперь расшифруйте этот файл, а результат операции выведите в файл decrypted.txt. Для этого выполните команду:
  yc kms symmetric-crypto decrypt --id <ID ключа> --ciphertext-file encrypted.txt --plaintext-file decrypted.txt 
  В результате выполнения команды будет создан файл decrypted.txt с идентичным исходному файлу (plain.txt) содержимым.
  Если расшифровать файл не удалось, утилита выдаст сообщение об ошибке.
  Шаг 4
  Создайте новую версию ключа. Для этого перейдите на страницу детальной информации о ключе и нажмите кнопку Ротировать. Новая версия ключа появится в списке версий и станет основной (Primary). Обратите внимание, что идентификаторы версий отличаются друг от друга.
  Шаг 5
  Запланируйте удаление первой версии ключа. Для этого в списке версий нажмите на значок … в строке с этой версией, а затем выберите пункт Запланировать удаление.
  В появившемся окне установите время, по истечении которого ключ будет удалён, и нажмите Запланировать. Версия ключа не может быть удалена моментально, минимальный период времени для её удаления составляет один день.
  После этого в списке версий удаляемый ключ будет помечен как запланированный на удаление (Scheduled For Destruction). Теперь этой версией ключа невозможно расшифровать файлы, которые были зашифрованы с её помощью.
  Провести ротацию ключа можно и из командной строки. Для этого используется команда:
  yc kms symmetric-key rotate <ID ключа> 
  Шаг 6
  Зашифруйте исходный файл plain.txt с помощью новой версии ключа. Результат запишите в файл encrypted_with_new_key.txt.
  yc kms symmetric-crypto encrypt --id <ID ключа> --plaintext-file plain.txt --ciphertext-file encrypted_with_new_key.txt 
  Теперь у вас есть два файла:
  - encrypted.txt, зашифрованный версией ключа, которая помечена на удаление;
  - encrypted_with_new_version.txt, зашифрованный новой версией ключа.
  Попробуйте расшифровать данные из обоих файлов. Вы увидите, что расшифровать первый файл не получилось, а файл, который зашифрован второй версией ключа, расшифрован.
  Запланированное удаление первой версии ключа можно отменить. Это позволит расшифровать данные из первого файла.
  В строке версии ключа, которая запланирована на удаление, нажмите значок …, а затем кнопку кнопку Отменить удаление. Эта версия снова получит статус активной. Проверьте, что она работает, расшифровав файл encrypted.txt.
  Decision:
  $ vim plain.txt
  $ cat plain.txt
  test text
  $ yc kms symmetric-crypto encrypt --id <ID ключа> --plaintext-file plain.txt --ciphertext-file encrypted.txt
  $ yc kms symmetric-crypto encrypt --id abjkh5a8k2uao3f8qi8k --plaintext-file plain.txt --ciphertext-file encrypted.txt
  $ cat encrypted.txt 
  abj2vjcrv89d83cef26in#���k��|�V�X�
                                    ����:�F���*Y��l�(���у��֜�l�S
  $ yc kms symmetric-crypto decrypt --id <ID ключа> --ciphertext-file encrypted.txt --plaintext-file decrypted.txt
  $ cat decrypted.txt 
  test text
  $ yc kms symmetric-key rotate <ID ключа>
  $ yc kms symmetric-crypto encrypt --id <ID ключа> --plaintext-file plain.txt --ciphertext-file encrypted_with_new_key.txt
  $ cat encrypted_with_new_key.txt
  abjofv85g302tc8mjhqq��n�nߊz6�5�A�
                                  �)ڀi�R0����o��s        �^
  �m�C zci�b�'

Прогнозирование затрат и оптимизация расходов в Yandex Cloud
  Task:
  Для оптимизации своих расходов в Yandex Cloud настроил дашборды в DataLens и использовал их.
  Task:
  Калькулятор расходов
  Любая стройка начинается со сметы. Облачная архитектура не исключение — прежде чем разворачивать систему, нужно просчитать её стоимость.
  Чтобы сделать это в Yandex Cloud, откройте калькулятор тарифов.
  Decision:
  Задание 1. Работа с калькулятором
  Давайте для начала посчитаем, во сколько рублей в месяц обойдётся система со следующими параметрами:
  Compute Cloud: Ubuntu 20.04 LTS, Intel Broadwell, 4 vCPU 100%, 16 ГБ RAM, SSD 100 ГБ, без публичного IP-адреса.
  Managed Service for PostgreSQL: Intel Cascade Lake, standard, s2.micro, network-ssd, 200 ГБ, два хоста, публичный IP-адрес.
  Техподдержка: тариф «Бизнес» при потреблении 50 тыс. ₽ в месяц.
  Сервисы Yandex.Cloud, стоимость которых можно рассчитать, указаны на верхней панели калькулятора:
  Выберите Compute Cloud и задайте параметры виртуальной машины. Должно получиться вот так:
  Как видите, справа указана стоимость сервиса с детализацией. Если выбрать ещё один сервис на панели вверху — его стоимость добавится в общий лист, а внизу отобразится общая сумма за все сервисы.
  Давайте это проверим. Выберите PostgreSQL и укажите нужную конфигурацию: один кластер Intel Cascade Lake, standard, s2.micro, network-ssd, 200 ГБ, два хоста, публичный IP-адрес.
  Остался последний сервис — техническая поддержка.
  Плата за тарифы «Стандарт» и «Бизнес» зависит от количества потребляемых ресурсов. Тариф «Премиум» может быть дополнен различными услугами и рассчитывается индивидуально.
  Узнаем стоимость тарифа «Бизнес» при потреблении ресурсов на 50 тыс. ₽ в месяц:
  Теперь вы знаете, сколько стоит система из трёх сервисов, и можете запустить её — или поэкспериментировать с конфигурациями, чтобы добиться приемлемой стоимости.
  Задание 2. Расчёт вручную
  А теперь задача со звёздочкой. В калькулятор тарифов пока добавлены не все сервисы Yandex Cloud. Стоимость некоторых придётся рассчитывать вручную. Потренируемся это делать.
  Узнаем, например, сколько стоит сервис Monitoring. Прокрутите страницу калькулятора вниз и в блоке Инфраструктура и сеть выберите Monitoring.
  Да, вы попадёте в документацию, но такова жизнь. Здесь указаны цены ресурсов. Со временем они могут меняться, поэтому для решения задачи ниже берите цены за август 2022 года:
  Task:
  рассчитать стоимость месячного использования сервиса при записи 35 метрик с частотой два значения в минуту.
  Подсказка: используйте раздел «Пример расчёта стоимости» как шпаргалку.
  Decision:
  -16,93 ₽
  -21,17 ₽
  +29,64 ₽
  -36,02 ₽
  Task:
  Поиск самой затратной виртуальной машины
  В боевых проектах вам часто придётся искать самую большую статью расходов, а потом придумывать, что с ней делать. Сейчас вы научитесь маркировать ВМ метками, чтобы понять, какая из них обходится дороже всего. Такими же метками можно размечать ресурсы и других сервисов Yandex Cloud.
  Decision:
  Перед началом этого урока создайте четыре виртуальные машины — с 5, 20, 50 и 100% vCPU — и сделайте небольшой перерыв. Нужно как минимум 15 минут, чтобы вы увидели их потребление на дашбордах.
  Если у вас ещё нет интерфейса командной строки Yandex Cloud Воспользуйтесь инструкцией, чтобы установить и инициализировать интерфейс командной строки (CLI) Yandex Cloud.
  Универсальный шаблон команды добавления метки выглядит следующим образом:
  yc <имя сервиса> <тип ресурса> add-labels <имя или идентификатор ресурса> --labels <имя метки>=<значение метки>
  Давайте уточним его для нашей задачи. Имя сервиса — compute, тип ресурса — instance. Идентификатор ресурса вы найдёте в консоли на странице обзора ВМ:
  Имя метки и значение метки задайте на свой вкус: можно использовать латиницу, цифры, дефисы и подчёркивания (не больше 63 символов).
  Если вы сделали всё правильно, то увидите результат выполнения команды:
  Повторите те же действия для трёх оставшихся ВМ.
  Когда закончите с метками, сделайте паузу. Системе понадобится несколько минут, чтобы отобразить данные размеченных машин в Yandex DataLens.
  В DataLens откройте дашборд Yandex Cloud Billing Dashboard и перейдите на вкладку Labels. Найти самую дорогую ВМ будет несложно:
  Устанавливайте метки на те ресурсы, которые требуют пристального внимания, и тогда вам будет легче за ними следить.
  Однако самостоятельно следить за потреблением в режиме нон-стоп просто невозможно. Чтобы всегда оставаться в курсе потребления и не уйти в минус при его резком всплеске, нужен автоматический помощник.
  Decision:
  $ yc <имя сервиса> <тип ресурса> add-labels <имя или идентификатор ресурса> --labels <имя метки>=<значение метки>

Резкое переполнение базы в Firebird
  Task:
  Резкое переполнение базы в Firebird
  Ошибка после ввода пароля в ПИ СДП у всех пользователей - "Не отвечает программа"
     ВНЕШНЯЯБАЗА2.exe не работает
  На одном из объектов автоматизации второй раз за месяц происходит процесс переполнения внешней базы документооборота C:\ПУТЬКБАЗЕ\ВНЕШНЯЯБАЗА1.GDB. База данных увеличивается в размерах с 1.8 Гб до 115Гб за три дня. После проведения инженером филиала процедуры Бэкап/Рестор внешняя база возвращается к прежнему размеру. Данная проблема наблюдается второй раз за месяц с периодичностью в две недели.
  Task:
  Отключить службы Firebird, Сделать backup/restore базы ВНЕШНЯЯБАЗА1.GDB, запустив скрипты 
  1. backup.bat 
  2. restore.bat
  после чего сохранится файл ВНЕШНЯЯБАЗА1.GDB в КУДАСОХРАНЯТЬИЗМЕНЕНИЯ. Файл заменить на файл в сервере, который занимал 120 гб. Посмотреть лог файлы:
  1. retranslator.log
  2. retranslator_2022-01-11_15-23-40-694.log
  3. retranslator_2022-01-24_16-36-09-627.log
  4. retranslator_2022-01-24_16-36-16-806.log
  Decision:
  $ cat backup.bat
  @ echo on
  SET ISC_USER=ИМЯПОЛЬЗОВАТЕЛЯБАЗЫ
  SET ISC_PASSWORD=ПАРОЛЬКБАЗЕ
  SET dbpath=localhost:C:\ПУТЬКБАЗЕ\ВНЕШНЯЯБАЗА1.GDB
  SET fbpath=C:\ПУТЬFIREBIRD\Firebird_1_5\bin\
  SET bbpath=C:\КУДАСОХРАНЯТЬИЗМЕНЕНИЯ\
  "%fbpath%gfix" -shut -force 5 "%dbpath%"
  "%fbpath%gbak" -b -v -g -y "%bbpath%%date%.log" "%dbpath%" "%bbpath%ВНЕШНЯЯБАЗА1_%date%.gbk"
  @ pause
  $ cat restore.bat
  @ echo on
  SET ISC_USER=ИМЯПОЛЬЗОВАТЕЛЯБАЗЫ
  SET ISC_PASSWORD=ПАРОЛЬКБАЗЕ
  SET fbpath=C:\ПУТЬFIREBIRD\Firebird_1_5\bin\
  SET bbpath=localhost:C:\КУДАСОХРАНЯТЬИЗМЕНЕНИЯ\
  "%fbpath%gbak.exe" -c -v -r -y "c:\КУДАСОХРАНЯТЬИЗМЕНЕНИЯ\%date%_fix.log" "c:\КУДАСОХРАНЯТЬИЗМЕНЕНИЯ\ВНЕШНЯЯБАЗА1_backup.gbk" "%bbpath%ВНЕШНЯЯБАЗА1_fix.GDB" 
  @ pause
  $ cat retranslator.log
  ...
  INF|09.12.2022 16:27:36 Сообщения в очереди "viv.client.КОДОА.1" отсутствуют
  INF|09.12.2022 16:32:39 Отправка запроса id="ИДЕНТИФИКАТОРПРОЦЕССА1", type_id=ИДЕНТИФИКАТОРТИП ...
  ERR|09.12.2022 16:33:37 Out of memory.
      E.ClassName=EOutOfMemory
      Sender.ClassName=TWorkThread
  ...
  INF|11.12.2022 22:28:27 Отправка запроса id="ИДЕНТИФИКАТОРПРОЦЕССА2", type_id=ИДЕНТИФИКАТОРТИП ...
  ERR|11.12.2022 22:28:53 При отправке запроса id="ИДЕНТИФИКАТОРПРОЦЕССА2" произошла неустранимая ошибка чтения из БД, запрос отклонён. Текст ошибки:
      Unsuccessful execution caused by a system error that precludes
      successful execution of subsequent statements.
      I/O error for file "C:\ПУТЬКБАЗЕ\ВНЕШНЯЯБАЗА1.GDB".
      Error while trying to write to file.
      Недостаточно места на диске.
  ...
  Decision:
  при обработке одного запроса id="ИДЕНТИФИКАТОРПРОЦЕССА1", type_id=ИДЕНТИФИКАТОРТИП происходит нехватка памяти, что далее приводит к сбою функционирования. Вероятно в запросе большое вложение. Для дальнейшего анализа смотрим БД ВНЕШНЯЯБАЗА2.gdb, в которой покажет технические ошибки состояния базы ВНЕШНЯЯБАЗА2.gdb.
  Для их устранения также сделать бэкап/ресторе БД ВНЕШНЯЯБАЗА2.gdb. Далее понаблюдать за ситуацией с размером БД ВНЕШНЯЯБАЗА2.GDB, чтобы он весил не больше 19 Гб.

Банковское хранилище данных с функцией обнаружения мошенничества
	Task:
	Для защиты дипломного проекта по теме "Банковское хранилище данных с функцией обнаружения мошенничества" при работе с транзакционными банковскими данными с помощью Python и SQL реализовал собственное хранилище данных - DWH, процесс сбора, очистки, трансформации и хранения данных, систему автоматического поиска мошеннических операций (AntiFraud-система)
	Task:
	Docker установка
	Decision:
	$ sudo apt update
	$ sudo apt install apt-transport-https ca-certificates curl software-properties-common
	$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
	$ echo"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"| sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
	$ sudo apt update
	$ apt-cache policy docker-ce
	$ sudo apt install docker-ce
	$ systemctl status docker
	$ sudo docker run hello-world
	$ sudo usermod -aG docker tuser
	$ docker run hello-world
	$ curl -SL https://github.com/docker/compose/releases/download/v2.6.0/docker-compose-linux-x86_64 -o ~/.docker/cli-plugins/docker-compose
	$ chmod +x ~/.docker/cli-plugins/docker-compose
	$ docker compose version
	Task:
	Create Postgres Docker Image
	Decision:
	$ mkdir docks
	$ mkdir docks/postgres
	$ touch docks/postgres/Dockerfile
	$ vim docks/postgres/Dockerfile
	$ cat docks/postgres/Dockerfile
	FROM postgres:latest
	ENV POSTGRES_USER YOUR-USERNAME
	ENV POSTGRES_PASSWORD YOUR-PASSWORD
	ENV POSTGRES_DB YOUR-DB
	EXPOSE 5432 
	$ cd docks/postgres/
	$ docker build -t YOUR-DB/postgres .
	$ docker run -d --name postg -p 5432:5432 YOUR-DB/postgres
	FULLYOUR-ID1
	$ sudo netstat -tulpn | grep 5432
	$ docker logs -f postg
	$ docker ps
	$ docker ps -a
	CONTAINER ID   IMAGE                          COMMAND                  CREATED        STATUS                       PORTS                                       NAMES
	YOUR-ID1   YOUR-DB/postgres                 "docker-entrypoint.s…"   10 hours ago   Exited (255) 5 minutes ago   0.0.0.0:5432->5432/tcp, :::5432->5432/tcp   postg
	$ docker start postg
	Task:
	Pgadmin Postgres Docker container
	Decision:
	$ google-chrome https://www.pgadmin.org/download/ &
	$ curl -fsS https://www.pgadmin.org/static/packages_pgadmin_org.pub | sudo gpg --dearmor -o /usr/share/keyrings/packages-pgadmin-org.gpg
	$ sudo sh -c 'echo "deb [signed-by=/usr/share/keyrings/packages-pgadmin-org.gpg] https://ftp.postgresql.org/pub/pgadmin/pgadmin4/apt/$(lsb_release -cs) pgadmin4 main" > /etc/apt/sources.list.d/pgadmin4.list && apt update'
	$ sudo apt install pgadmin4
	$ sudo /usr/pgadmin4/bin/setup-web.sh
	$ docker ps -a
	CONTAINER ID   IMAGE                          COMMAND                  CREATED        STATUS                        PORTS                                       NAMES
	YOUR-ID   YOUR-DB/postgres                 "docker-entrypoint.s…"   45 hours ago   Exited (255) 37 seconds ago   0.0.0.0:5432->5432/tcp, :::5432->5432/tcp   postg
	$ docker start postg
	$ google-chrome http://YOUR-IP/pgadmin4 &
	Task:
	Stop, remove postgres container
	Decision:
	$ docker stop postg
	$ docker rm postg
	$ docker push YOUR-DB/postgres
	Task:
	Разработать ETL процесс, получающий ежедневную выгрузку данных (предоставляется за 3 дня), загружающий ее в хранилище данных и ежедневно строящий отчет.
	Ежедневно некие информационные системы выгружают три следующих файла:
	1. Список транзакций за текущий день. Формат – CSV.
	2. Список терминалов полным срезом. Формат – XLSX.
	3. Список паспортов, включенных в «черный список» - с накоплением с начала месяца. Формат – XLSX.
	Сведения о картах, счетах и клиентах хранятся в СУБД PostgreSQL. Во вложении реквизиты для подключения. 
	Вам предоставляется выгрузка за последние три дня, ее надо обработать. В качестве хранилища выступает ваша учебная база (edu). Данные должны быть загружены в хранилище со следующей структурой (имена сущностей указаны по существу, без особенностей правил нейминга, указанных далее).
	Типы данных в полях можно изменять на однородные если для этого есть необходимость. Имена полей менять нельзя. Ко всем таблицам SCD1 должны
	быть добавлены технические поля create_dt, update_dt; ко всем таблицам SCD2 должны быть добавлены технические поля effective_from, effective_to, deleted_flg.
	По результатам загрузки ежедневно необходимо строить витрину отчетности по мошенническим операциям. Витрина строится накоплением, каждый новый отчет укладывается в эту же таблицу с новым report_dt.
	В витрине должны содержаться следующие поля:
	1. event_dt - Время наступления события. Если событие наступило по результату нескольких действий – указывается время действия, по которому установлен факт мошенничества.
	2. passport - Номер паспорта клиента, совершившего мошенническую операцию.
	3. fio - ФИО клиента, совершившего мошенническую операцию.
	4. phone - Номер телефона клиента, совершившего мошенническую операцию.
	5. event_type - Описание типа мошенничества.
	6. report_dt - Время построения отчета.
	Признаки мошеннических операций:
	1. Совершение операции при просроченном или заблокированном паспорте.
	2. Совершение операции при недействующем договоре.
	3. Совершение операций в разных городах в течение одного часа.
	4. Попытка подбора суммы. В течение 20 минут проходит более 3х операций соследующим шаблоном – каждая последующая меньше предыдущей, при этом отклонены все кроме последней. Последняя операция (успешная) в такой цепочке считается мошеннической.
	При именовании таблиц необходимо придерживаться следующих правил (для автоматизации проверки):
	1. YOUR-USERNAME.<CODE>_STG_<TABLE_NAME> - Таблицы для размещения стейджинговых таблиц (первоначальная загрузка), промежуточное выделение инкремента, если требуется. Временные таблицы, если такие потребуются в расчете, можно также складывать с таким именованием. Имя таблиц можете выбирать произвольное, но смысловое.
	2. YOUR-USERNAME.<CODE>_DWH_FACT_<TABLE_NAME> - Таблицы фактов, загруженных в хранилище. В качестве фактов выступают сами транзакции и «черный список» паспортов. Имя таблиц – как в ER диаграмме.
	3. YOUR-USERNAME.<CODE>_DWH_DIM_<TABLE_NAME> - Таблицы измерений, хранящиеся в формате SCD1. Имя таблиц – как в ER диаграмме.
	4. YOUR-USERNAME.<CODE>_DWH_DIM_<TABLE_NAME>_HIST - Таблицы измерений, хранящиеся в SCD2 формате (только для тех, кто выполняет усложненное задание). Имя таблиц – как в ER диаграмме.
	5. YOUR-USERNAME.<CODE>_REP_FRAUD - Таблица с отчетом.
	6. YOUR-USERNAME.<CODE>_META_<TABLE_NAME> - Таблицы для хранения метаданных. Имя таблиц можете выбирать произвольное, но
	смысловое.
	7. <CODE> - 4 буквы вашего персонального кода.
	Выгружаемые файлы именуются согласно следующему шаблону:
	1. transactions_DDMMYYYY.txt
	2. passport_blacklist_DDMMYYYY.xlsx
	3. terminals_DDMMYYYY.xlsx
	Предполагается что в один день приходит по одному такому файлу. После загрузки соответствующего файла он должен быть переименован в файл с расширением .backup чтобы при следующем запуске файл не искался и перемещен в каталог archive:
	1. transactions_DDMMYYYY.txt.backup
	2. passport_blacklist_DDMMYYYY.xlsx.back
	3. upterminals_DDMMYYYY.xlsx.backup
	Желающие могут придумать, обосновать и реализовать более технологичные и учитывающие сбои способы обработки (за это будет повышен балл).
	В classroom выкладывается zip-архив, содержащий следующие файлы и каталоги:
	1. main.py - Файл, обязательный. Основной процесс обработки.
	2. файлы с данными - Файл, обязательный. Те файлы, которые вы получили в качестве задания. Просто скопируйте все 9 файлов.
	3. main.ddl - Файл, обязательный. Файл с SQL кодом для создания всех необходимых объектов в базе edu.
	4. main.cron - Файл, обязательный. Файл для постановки вашего процесса на расписание, в формате crontab
	5. archive - Каталог, обязательный. Пустой, сюда должны перемещаться отработанные файлы
	6. sql_scripts - Каталог, необязательный. Если вы включаете в main.py какие-то SQL скрипты, вынесенные в отдельные файлы – помещайте их сюда.
	7. py_scripts - Каталог, необязательный. Если вы включаете в main.py какие-то python скрипты, вынесенные в отдельные файлы – помещайте их сюда.
	Decision:
	$ mkdir archive
	$ touch main.py
	$ chmod +x main.py
	$ touch main.ddl
	$ touch main.cron
	$ cat snippet_pg.py
	import psycopg2
	import pandas as pd
	# Создание подключения к PostgreSQL
	conn = psycopg2.connect (
			database = "YOUR-DB",
			host =     "YOUR-IP",
			user =     "YOUR-USERNAME",
			password = "YOUR-PASSWORD",
			port =     "5432"
		)
	# Отключение автокоммита
	conn.autocommit = False
	# Создание курсора
	cursor = conn.cursor()
	####################################################
	# Выполнение SQL кода в базе данных без возврата результата
	cursor.execute( "INSERT INTO YOUR-USERNAME.testtable( id, val ) VALUES ( 1, 'ABC' )" )
	conn.commit()
	# Выполнение SQL кода в базе данных с возвратом результата
	cursor.execute( "SELECT * FROM YOUR-USERNAME.testtable" )
	records = cursor.fetchall()
	for row in records:
		print( row )
	####################################################
	# Формирование DataFrame
	names = [ x[0] for x in cursor.description ]
	df = pd.DataFrame( records, columns = names )
	# Запись в файл
	df.to_excel( 'pandas_out.xlsx', sheet_name='sheet1', header=True, index=False )
	####################################################
	# Чтение из файла
	df = pd.read_excel( 'pandas.xlsx', sheet_name='sheet1', header=0, index_col=None )
	# Запись DataFrame в таблицу базы данных
	cursor.executemany( "INSERT INTO YOUR-USERNAME.testtable( id, val ) VALUES( %s, %s )", df.values.tolist() )
	# Закрываем соединение
	cursor.close()
	conn.close()
	$ /SCD1_incremental_full_script.sql 
	----------------------------------------------------------------------------
	-- Подготока данных

	create table YOUR-USERNAME.XXXX_source( 
			id integer,
			val varchar(50),
			update_dt timestamp(0)
	);
	insert into YOUR-USERNAME.XXXX_source ( id, val, update_dt ) values ( 1, 'A', now() );
	insert into YOUR-USERNAME.XXXX_source ( id, val, update_dt ) values ( 2, 'B', now() );
	insert into YOUR-USERNAME.XXXX_source ( id, val, update_dt ) values ( 3, 'C', now() );
	update YOUR-USERNAME.XXXX_source set val = 'X', update_dt = now() where id = 3;
	delete from YOUR-USERNAME.XXXX_source where id = 3;
	create table YOUR-USERNAME.XXXX_stg( 
			id integer,
			val varchar(50),
			update_dt timestamp(0)
	);
	create table YOUR-USERNAME.XXXX_target (
			id integer,
			val varchar(50),
			create_dt timestamp(0),
			update_dt timestamp(0)
	);
	create table YOUR-USERNAME.XXXX_meta(
		schema_name varchar(30),
		table_name varchar(30),
		max_update_dt timestamp(0)
	);
	insert into YOUR-USERNAME.XXXX_meta( schema_name, table_name, max_update_dt )
	values( 'YOUR-USERNAME','XXXX_SOURCE', to_timestamp('1900-01-01','YYYY-MM-DD') );
	create table YOUR-USERNAME.XXXX_stg_del( 
			id integer
	);
	----------------------------------------------------------------------------
	-- Инкрементальная загрузка
	-- 1. Очистка стейджинговых таблиц
	delete from YOUR-USERNAME.XXXX_stg;
	delete from YOUR-USERNAME.XXXX_stg_del;
	-- 2. Захват данных из источника (измененных с момента последней загрузки) в стейджинг
	insert into YOUR-USERNAME.XXXX_stg( id, val, update_dt )
	select id, val, update_dt from YOUR-USERNAME.XXXX_source
	where update_dt > ( select max_update_dt from YOUR-USERNAME.XXXX_meta where schema_name='YOUR-USERNAME' and table_name='XXXX_SOURCE' );
	-- 3. Захват в стейджинг ключей из источника полным срезом для вычисления удалений.
	insert into YOUR-USERNAME.XXXX_stg_del( id )
	select id from YOUR-USERNAME.XXXX_source;
	-- 4. Загрузка в приемник "вставок" на источнике (формат SCD1).
	insert into YOUR-USERNAME.XXXX_target( id, val, create_dt, update_dt )
	select 
			stg.id, 
			stg.val, 
			stg.update_dt, 
			null 
	from YOUR-USERNAME.XXXX_stg stg
	left join YOUR-USERNAME.XXXX_target trg
	on stg.id = trg.id
	where trg.id is null;
	-- 5. Обновление в приемнике "обновлений" на источнике (формат SCD1).
	update YOUR-USERNAME.XXXX_target
	set 
			val = tmp.val,
			update_dt = tmp.update_dt
	from (
			select 
					stg.id, 
					stg.val, 
					stg.update_dt, 
					null 
			from YOUR-USERNAME.XXXX_stg stg
					inner join YOUR-USERNAME.XXXX_target trg on stg.id = trg.id
			where stg.val <> trg.val 
					or ( stg.val is null and trg.val is not null ) or ( stg.val is not null and trg.val is null )
	) tmp
	where XXXX_target.id = tmp.id; 
	-- 6. Удаление в приемнике удаленных в источнике записей (формат SCD1).
	delete from YOUR-USERNAME.XXXX_target
	where id in (
			select trg.id
			from YOUR-USERNAME.XXXX_target trg
			left join YOUR-USERNAME.XXXX_stg_del stg
			on stg.id = trg.id
			where stg.id is null
	);
	-- 7. Обновление метаданных.
	update YOUR-USERNAME.XXXX_meta
	set max_update_dt = coalesce( (select max( update_dt ) from YOUR-USERNAME.XXXX_stg ), ( select max_update_dt from YOUR-USERNAME.XXXX_meta where schema_name='YOUR-USERNAME' and table_name='XXXX_SOURCE' ) )
	where schema_name='YOUR-USERNAME' and table_name = 'XXXX_SOURCE';
	-- 8. Фиксация транзакции.
	commit;
	SCD1_incremental_full_script.sql
	SCD1_incremental_full_script.sql. На экране
	$ wget https://drive.google.com/file/d/13WSK0C1Z36hhsopebgEv2bGLAoxEsLUp/view?usp=drive_web&authuser=0
	$ unzip data.zip
	Task:
	Создадите таблицы в базе
	Decision:
	$ vim main.ddl
	$ cat main.ddl
	#!/usr/bin/python3
	----------------------------------
	create table YOUR-USERNAME.XXXX_stg_transactions(
		trans_id varchar(15),
		trans_date timestamp(0),
		--amt decimal(10,2),
		amt numeric(10,2),    
		card_num varchar(20),
		oper_type varchar(10),
		oper_result varchar(10),
		terminal varchar(10),
		create_dt timestamp(0), 
		update_dt timestamp(0));
	create table YOUR-USERNAME.XXXX_dwh_fact_transactions(
		trans_id varchar(15),
		trans_date timestamp(0),
		--amt numeric(10,2),
		amt decimal(10,2),
		card_num varchar(20),
		oper_type varchar(10),
		oper_result varchar(10),
		terminal varchar(10),
		create_dt timestamp(0), 
		update_dt timestamp(0));
	----------------------------------
	create table YOUR-USERNAME.XXXX_stg_terminals(
		terminal_id varchar(10),
		terminal_type varchar(10),
		terminal_city varchar(30),
		terminal_address varchar(70),
		create_dt timestamp(0), 
		update_dt timestamp(0));
	create table YOUR-USERNAME.XXXX_dwh_dim_terminals(
		terminal_id varchar(10),
		terminal_type varchar(10),
		terminal_city varchar(30),
		terminal_address varchar(70),
		create_dt timestamp(0),
		update_dt timestamp(0),
		effective_from timestamp(0),
		effective_to timestamp(0),
		deleted_flg char(1));
	create table YOUR-USERNAME.XXXX_stg_del_terminals(
		terminal_id varchar(10));
	----------------------------------
	create table YOUR-USERNAME.XXXX_stg_blacklist(
		passport_num varchar(15),
		--entry_dt date
		entry_dt timestamp(0),
		create_dt timestamp(0), 
		update_dt timestamp(0));
	create table YOUR-USERNAME.XXXX_dwh_fact_passport_blacklist(
		passport_num varchar(30),
		--entry_dt date
		--date timestamp(0)
		entry_dt timestamp(0),
		create_dt timestamp(0), 
		update_dt timestamp(0));
	----------------------------------
	create table YOUR-USERNAME.XXXX_stg_cards(
		card_num varchar(20),
		account_num varchar(20),
		create_dt timestamp(0),
		update_dt timestamp(0));
	create table YOUR-USERNAME.XXXX_dwh_dim_cards(
		card_num varchar(20),
		account_num varchar(20),
		--create_dt date,
		--update_dt date
		create_dt timestamp(0), 
		update_dt timestamp(0),
		effective_from timestamp(0),
		effective_to timestamp(0),
		deleted_flg char(1));
	create table YOUR-USERNAME.XXXX_stg_del_cards(
		card_num varchar(20));
	----------------------------------
	create table YOUR-USERNAME.XXXX_stg_accounts(
		account_num varchar(20),
		--valid_to date,
		valid_to timestamp(0), 
		client varchar(10),
		create_dt timestamp(0),
		update_dt timestamp(0));
	create table YOUR-USERNAME.XXXX_dwh_dim_accounts(
		account_num varchar(20),
		--valid_to date,
		valid_to timestamp(0), 
		client varchar(10),
		--create_dt date,
		--update_dt date
		create_dt timestamp(0), 
		update_dt timestamp(0),
		effective_from timestamp(0),
		effective_to timestamp(0),
		deleted_flg char(1)); 
	create table YOUR-USERNAME.XXXX_stg_del_accounts(
		account_num varchar(20));
	----------------------------------
	create table YOUR-USERNAME.XXXX_stg_clients(
		client_id varchar(10),
		last_name varchar(20),
		first_name varchar(20),
		patronymic varchar(20),
		--date_of_birth date,
		date_of_birth timestamp(0), 
		passport_num varchar(15),
		--passport_valid_to date,
		passport_valid_to timestamp(0),
		phone varchar(16),
		create_dt timestamp(0),
		update_dt timestamp(0));
	create table YOUR-USERNAME.XXXX_dwh_dim_clients(
		client_id varchar(10),
		last_name varchar(20),
		first_name varchar(20),
		patronymic varchar(20),
		--date_of_birth date,
		date_of_birth timestamp(0),
		passport_num varchar(15),
		--passport_valid_to date,
		passport_valid_to timestamp(0),
		phone varchar(16),
		--create_dt date,
		--update_dt date
		create_dt timestamp(0), 
		update_dt timestamp(0),
		effective_from timestamp(0),
		effective_to timestamp(0),
		deleted_flg char(1));
	create table YOUR-USERNAME.XXXX_stg_del_clients(
		client_id varchar(10));
	----------------------------------
	create table YOUR-USERNAME.XXXX_rep_fraud(
		event_dt timestamp(0), 
		passport varchar(20), 
		fio varchar(50),
		phone varchar(16), 
		event_type varchar(120), 
		--report_dt date
		report_dt timestamp(0));
	----------------------------------
	create table YOUR-USERNAME.XXXX_meta(
		schema_name varchar(30),
		table_name varchar(30),
		max_update_dt timestamp(0));
	----------------------------------
	Task:
	Алгоритм для файла main.py: Подключитесь к двум базам, Очистите весь стейджинг. Загрузите файлы transactions_01032021.txt, terminals_01032021.xlsx, passport_blacklist_01032021.xlsx в стейджинг. Загрузите таблицы clients, accounts, cards в стейджинг. Используйте следующий подход: Выполните запрос к базе 1, Сохраните полученный результат в DataFrame, Загрузите DataFrame в базу 2. Загрузите данные из стейджинга в целевую таблицу xxxx_dwh_dim_terminals, xxxx_dwh_dim_cards, xxxx_dwh_dim_accounts, xxxx_dwh_dim_clients. Загрузите данные из стейджинга в целевую таблицу xxxx_dwh_fact_passport_blacklist, xxxx_dwh_fact_transactions.. Фактовые таблицы данные перекладываются «простым инсертом», то есть необходимо выполнить один INSERT INTO ... SELECT .... Напишите скрипт, соединяющий нужные таблицы для поиска операций, совершенных при недействующем договоре. Отладьте ваш скрипт для одной даты PgAdmin, он должен выдавать результат. Результат выполнения скрипта загружайте в таблицу xxxx_rep_fraud. Незабывайте сформировать поле report_dt. Зафиксируйте изменения. Отключитесь от баз. Переименуйте обработанные файлы и перенесите их в другой каталог. отладить его работоспособность навсех трех днях загрузки.
	Decision:
	$ vim main.py
	$ cat main.py
	#!/usr/bin/python3
	import pandas as pd
	import psycopg2
	import os
	###################Подключение к базам
	#conn_YOUR-DB1 = psycopg2.connect(database = "YOUR-DB",host = "YOUR-IP",user = "YOUR-USERNAME",password = "YOUR-PASSWORD",port = "5432")
	conn_YOUR-DB1 = psycopg2.connect(database = "YOUR-DB1",host = "YOUR-HOST",user = "YOUR-USERNAME1",password = "YOUR-PASSWORD1",port = "5432")
	conn_YOUR-DB2 = psycopg2.connect(database = "bank",host = "YOUR-HOST",user = "YOUR-USERNAME2",password = "YOUR-PASSWORD2",port = "5432")
	conn_YOUR-DB1.autocommit = False
	conn_YOUR-DB2.autocommit = False
	cursor_YOUR-DB1 = conn_YOUR-DB1.cursor()
	cursor_YOUR-DB2 = conn_YOUR-DB2.cursor()
	######################################
	###################Очистка стейджинговых таблиц
	cursor_YOUR-DB1.execute("""DELETE FROM YOUR-USERNAME1.XXXX_stg_transactions;""")
	###################Загрузка данных в стейджинг
	#df = pd.read_csv(f'/home/YOUR-USERNAME1/XXXX/project/data/transactions_01032021.txt', sep=';', decimal=',', header=0, index_col=None)
	dirpath = "/home/YOUR-USERNAME1/XXXX/project"
	project_files = os.listdir(dirpath)
	for transactions in project_files:
			if transactions.endswith('.txt'):
					df = pd.read_csv(transactions, sep=";")
					datenow_w_txt = transactions.rsplit('_')[1]
					datenow_str = datenow_w_txt.split('.')[0]
	df['amount'] = df['amount'].map(lambda z: z.strip().replace(',', '.')).astype('float')
	df['create_dt'] = datenow_str
	df['update_dt'] = datenow_str
	###################
	cursor_YOUR-DB1.executemany("""
					INSERT INTO YOUR-USERNAME1.XXXX_stg_transactions(trans_id, trans_date, amt, card_num, oper_type, oper_result, terminal , create_dt, update_dt) 
					VALUES(%s, %s , %s , %s , %s , %s , %s, to_timestamp(%s,'DDMMYYYY'), to_timestamp(%s,'DDMMYYYY'))
			""", df.values.tolist())
	###################Загрузка данных в целевые таблицы фактов
	cursor_YOUR-DB1.execute("""
					INSERT INTO YOUR-USERNAME1.XXXX_dwh_fact_transactions (trans_id,trans_date,card_num,oper_type,amt,oper_result,terminal,create_dt,update_dt)
					SELECT stg.trans_id,stg.trans_date,stg.card_num,stg.oper_type,stg.amt,stg.oper_result,stg.terminal,stg.create_dt,stg.update_dt 
					FROM YOUR-USERNAME1.XXXX_stg_transactions as stg;
			""")
	###################
	######################################
	###################Очистка стейджинговых таблиц
	cursor_YOUR-DB1.execute("""DELETE FROM YOUR-USERNAME1.XXXX_stg_terminals;""")
	###################Загрузка данных в стейджинг
	#df = pd.read_excel(f'/home/YOUR-USERNAME1/XXXX/project/data/terminals_01032021.xlsx', sheet_name='terminals', header=0, index_col=None)
	for terminals in project_files:
			if terminals.startswith('terminals'):
					df = pd.read_excel(terminals, sheet_name='terminals', header=0, index_col=None )
					datenow_w_txt = terminals.rsplit('_')[1]
					datenow_str = datenow_w_txt.split('.')[0]
	df['create_dt'] = datenow_str
	df['update_dt'] = datenow_str
	#df.insert(4, 'update_dt','2021-03-01')
	###################
	cursor_YOUR-DB1.executemany("""
					INSERT INTO YOUR-USERNAME1.XXXX_stg_terminals(terminal_id,terminal_type,terminal_city,terminal_address,create_dt,update_dt)
					VALUES(%s,%s,%s,%s,to_timestamp(%s,'DDMMYYYY'),to_timestamp(%s,'DDMMYYYY'));
			""", df.values.tolist())
	###################Загрузка данных в целевые таблицы измерений
	cursor_YOUR-DB1.execute("""
					INSERT INTO YOUR-USERNAME1.XXXX_dwh_dim_terminals (terminal_id,terminal_type,terminal_city,terminal_address,create_dt,update_dt)
					SELECT stg.terminal_id, stg.terminal_type, stg.terminal_city, stg.terminal_address, stg.update_dt,to_timestamp('9999-12-31', 'YYYY-MM-DD') 
					from YOUR-USERNAME1.XXXX_stg_terminals as stg  
					left join YOUR-USERNAME1.XXXX_dwh_dim_terminals as trg
					on stg.terminal_id = trg.terminal_id 
					where trg.terminal_id is null;
			""")
	###################
	######################################
	###################Очистка стейджинговых таблиц
	cursor_YOUR-DB1.execute("""DELETE FROM YOUR-USERNAME1.XXXX_stg_blacklist;""")
	###################Загрузка данных в стейджинг
	#df = pd.read_excel(f'/home/YOUR-USERNAME1/XXXX/project/data/passport_blacklist_01032021.xlsx', sheet_name='blacklist', header=0, index_col=None)
	for blacklist in project_files:
			if blacklist.startswith('passport_blacklist'):
					df = pd.read_excel(blacklist, sheet_name='blacklist', header=0, index_col=None )
					datenow_w_ext = blacklist.rsplit('blacklist_')[1]
					datenow_str = datenow_w_ext.split('.')[0]
	df['create_dt'] = datenow_str
	df['update_dt'] = datenow_str
	###################
	cursor_YOUR-DB1.executemany("""
					INSERT INTO YOUR-USERNAME1.XXXX_stg_blacklist(entry_dt,passport_num,create_dt,update_dt)
					VALUES(%s, %s,to_timestamp(%s,'DDMMYYYY'),to_timestamp(%s,'DDMMYYYY'));
			""", df.values.tolist())
	###################Загрузка данных в целевые таблицы фактов
	cursor_YOUR-DB1.execute("""
					INSERT INTO YOUR-USERNAME1.XXXX_dwh_fact_passport_blacklist 
					SELECT stg.passport_num,stg.entry_dt,stg.create_dt,stg.update_dt
					from YOUR-USERNAME1.XXXX_stg_blacklist as stg 
					left join YOUR-USERNAME1.XXXX_dwh_fact_passport_blacklist as trg 
					on stg.passport_num = trg.passport_num 
					where trg.passport_num is null;
			""")
	######################################
	###################Очистка стейджинговых таблиц
	cursor_YOUR-DB1.execute("""DELETE FROM YOUR-USERNAME1.XXXX_stg_cards;""")
	###################Загрузка данных в стейджинг
	cursor_YOUR-DB2.execute("""
					--SELECT regexp_replace(card_num, '\s+$', '') as card_num,account,create_dt,update_dt 
					SELECT card_num,account,create_dt,update_dt 
					FROM info.cards
			""")
	records = cursor_YOUR-DB2.fetchall()
	#for row in records:
	#    print(row)
	names = [ x[0] for x in cursor_YOUR-DB2.description ]
	df = pd.DataFrame( records, columns = names )
	cursor_YOUR-DB1.executemany("""
					INSERT INTO YOUR-USERNAME1.XXXX_stg_cards(card_num,account_num,create_dt,update_dt) 
					VALUES(%s,%s,%s,%s)
			""", df.values.tolist())
	###################Загрузка данных в целевые таблицы измерений
	cursor_YOUR-DB1.execute("""
					INSERT INTO YOUR-USERNAME1.XXXX_dwh_dim_cards (card_num,account_num,create_dt,update_dt)
					SELECT stg.card_num, stg.account_num, stg.create_dt, to_timestamp('9999-12-31', 'YYYY-MM-DD')
					from YOUR-USERNAME1.XXXX_stg_cards as stg 
					left join YOUR-USERNAME1.XXXX_dwh_dim_cards as trg 
					on stg.card_num = trg.card_num 
					where trg.card_num is null;
			""")
	###################
	######################################
	###################Очистка стейджинговых таблиц
	cursor_YOUR-DB1.execute("""DELETE FROM YOUR-USERNAME1.XXXX_stg_accounts;""")
	###################Загрузка данных в стейджинг
	cursor_YOUR-DB2.execute("""
					SELECT account,valid_to,client,create_dt,update_dt 
					FROM info.accounts
			""")
	records = cursor_YOUR-DB2.fetchall()
	#for row in records:
	#        print(row)
	names = [ x[0] for x in cursor_YOUR-DB2.description ]
	df = pd.DataFrame( records, columns = names )
	cursor_YOUR-DB1.executemany("""
					INSERT INTO YOUR-USERNAME1.XXXX_stg_accounts(account_num,valid_to,client,create_dt,update_dt)
					VALUES( %s, %s, %s, %s, %s)
			""", df.values.tolist())
	###################Загрузка данных в целевые таблицы измерений
	cursor_YOUR-DB1.execute("""
					INSERT INTO YOUR-USERNAME1.XXXX_dwh_dim_accounts(account_num,valid_to,client,create_dt,update_dt) 
					SELECT stg.account_num,stg.valid_to,stg.client,stg.create_dt,to_timestamp('9999-12-31', 'YYYY-MM-DD')
							from YOUR-USERNAME1.XXXX_stg_accounts as stg 
					left join YOUR-USERNAME1.XXXX_dwh_dim_accounts as trg
					on stg.account_num = trg.account_num 
					where trg.account_num is null;
			""")
	###################
	######################################
	###################Очистка стейджинговых таблиц
	cursor_YOUR-DB1.execute("""DELETE FROM YOUR-USERNAME1.XXXX_stg_clients;""")
	###################Загрузка данных в стейджинг
	cursor_YOUR-DB2.execute("""
					SELECT client_id,last_name,first_name,patronymic,date_of_birth,passport_num,passport_valid_to,phone,create_dt,update_dt
					FROM info.clients; 
			""")
	records = cursor_YOUR-DB2.fetchall()
	#for row in records:
	#        print(row)
	names = [ x[0] for x in cursor_YOUR-DB2.description ]
	df = pd.DataFrame(records, columns = names) 
	cursor_YOUR-DB1.executemany(""" 
					INSERT INTO YOUR-USERNAME1.XXXX_stg_clients(client_id,last_name,first_name,patronymic,date_of_birth,passport_num,passport_valid_to,phone,create_dt,update_dt)
					VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s);
			""", df.values.tolist()) 
	###################Загрузка данных в целевые таблицы измерений
	cursor_YOUR-DB1.execute("""
					INSERT INTO YOUR-USERNAME1.XXXX_dwh_dim_clients(client_id,last_name,first_name,patronymic,date_of_birth,passport_num,passport_valid_to,phone,create_dt,update_dt)
					SELECT stg.client_id,stg.last_name,stg.first_name,stg.patronymic,stg.date_of_birth,stg.passport_num,stg.passport_valid_to,stg.phone,stg.create_dt,now()
					from YOUR-USERNAME1.XXXX_stg_clients as stg
					left join YOUR-USERNAME1.XXXX_dwh_dim_clients as trg
					on stg.client_id = trg.client_id 
					where trg.client_id is null;
			""")
	###################
	######################################
	###################Выявление мошеннических операций и построение отчёта
	cursor_YOUR-DB1.execute("""
					INSERT INTO YOUR-USERNAME1.XXXX_rep_fraud 
					select 
							min(t2.trans_date) as trans_date, 
							tgddcl.passport_num as passport_num, 
							(tgddcl.last_name || ' ' || tgddcl.first_name || ' ' || tgddcl.patronymic ) as fio, 
							tgddcl.phone as phone, 
							'1' as event_type,
							now() as report_dt
					from (
							select *
							from (
									with current_dt as ( 
											select trans_date 
											from YOUR-USERNAME1.XXXX_stg_transactions) 
									select  tgdft.*, tgddca.account_num 
									from YOUR-USERNAME1.XXXX_dwh_fact_transactions as tgdft 
									left join YOUR-USERNAME1.XXXX_dwh_dim_cards as tgddca
									on trim(tgdft.card_num) = trim(tgddca.card_num ) 
									where tgdft.oper_result = 'SUCCESS' 
											and tgdft.trans_date in (
													select trans_date 
													from current_dt)) as t 
							left join YOUR-USERNAME1.XXXX_dwh_dim_accounts as gdda 
							on t.account_num = gdda.account_num ) as t2 
					left join YOUR-USERNAME1.XXXX_dwh_dim_clients as tgddcl 
					on t2.client = tgddcl.client_id 
					where (tgddcl.passport_valid_to < t2.trans_date 
							or tgddcl.passport_num in (
									select passport_num
									from YOUR-USERNAME1.XXXX_dwh_fact_passport_blacklist))  
					group by tgddcl.passport_num, (tgddcl.last_name || ' ' || tgddcl.first_name || ' ' || tgddcl.patronymic ), tgddcl.phone;
			""")
	###################Совершение операции при недействующем договоре.
	cursor_YOUR-DB1.execute("""
					INSERT INTO YOUR-USERNAME1.XXXX_rep_fraud
					select 
							t2.trans_date as trans_date, 
							gddcl.passport_num as passport_num, 
							(gddcl.last_name || ' ' || gddcl.first_name || ' ' || gddcl.patronymic ) as fio, 
							gddcl.phone as phone, 
							'2' as event_type,
							now() as report_dt
					from (
							select min(t.trans_date) trans_date, t.account_num, gdda.client
							from (
									with current_dt as ( 
											select trans_date 
											from YOUR-USERNAME1.XXXX_stg_transactions) 
									select gdft.trans_date, gdft.card_num, gddca.account_num
									from YOUR-USERNAME1.XXXX_dwh_fact_transactions as gdft 
									left join YOUR-USERNAME1.XXXX_dwh_dim_cards as gddca
									on trim(gdft.card_num) = trim(gddca.card_num ) 
									where gdft.oper_result = 'SUCCESS'
											and gdft.trans_date in (
													select trans_date 
													from current_dt)) as t 
							left join YOUR-USERNAME1.XXXX_dwh_dim_accounts as gdda 
							on t.account_num = gdda.account_num 
							where t.trans_date > gdda.valid_to
							group by t.account_num, gdda.client ) as t2 
					left join YOUR-USERNAME1.XXXX_dwh_dim_clients as gddcl 
					on t2.client = gddcl.client_id;
			""")
	###################Совершение операций в разных городах в течение одного часа
	cursor_YOUR-DB1.execute("""
					INSERT INTO YOUR-USERNAME1.XXXX_rep_fraud
					select
							t3.trans_date,
							t3.passport_num,
							(t3.last_name || ' ' || t3.first_name || ' ' || t3.patronymic ) as fio,
							t3.phone ,
							'3' as event_type,
							now() as report_dt
					from (
							select trans_date, trg2.*
							from (
									select * from (
											select * from (
													with lds as  (
															select
																	trans_date,
																	terminal_city,
																	card_num,
																	lag(terminal_city) over (
																			partition by gdftr.card_num
																			order by gdftr.trans_date) as lag_c,
																	lag(trans_date) over (
																			partition by card_num
																			order by gdftr.trans_date) as lag_t,
																	lead(terminal_city) over (
																			partition by gdftr.card_num
																			order by gdftr.trans_date) as lead_c ,
																	lead(trans_date) over (
																			partition by card_num
																			order by gdftr.trans_date) as lead_t
															from YOUR-USERNAME1.XXXX_dwh_fact_transactions as gdftr
															left join YOUR-USERNAME1.XXXX_dwh_dim_terminals as gddt
															on gdftr.terminal = gddt.terminal_id
															where gdftr.oper_result = 'SUCCESS'),
													current_dt as (
															select trans_date
															from YOUR-USERNAME1.XXXX_stg_transactions )
													select
															min(trans_date) as trans_date,
															card_num
													from lds
													where lag_c <> terminal_city
															and (trans_date-lag_t) < '01:00:00'
															and trans_date  in (
																	select trans_date
																	from current_dt)
													group by card_num) as trg
											left join YOUR-USERNAME1.XXXX_dwh_dim_cards as trg3
											on trim(trg.card_num) = trim(trg3.card_num )) as t
									left join YOUR-USERNAME1.XXXX_dwh_dim_accounts as trg4
									on t.account_num = trg4.account_num ) as t2
							left join YOUR-USERNAME1.XXXX_dwh_dim_clients as trg2
							on t2.client = trg2.client_id ) as t3;
			""")
	###################Попытка подбора суммы
	cursor_YOUR-DB1.execute("""
					INSERT INTO YOUR-USERNAME1.XXXX_rep_fraud 
					select 
							t3.trans_date, 
							t3.passport_num , 
							(t3.last_name || ' ' || t3.first_name || ' ' || t3.patronymic ) as fio, 
							t3.phone , 
							'4' as event_type,
							now() as report_dt
					from (
							select 
									trans_date, 
									gddcl.*
							from  (
									select * from (
											select * from(
													with rj as (
															select 
																	*,
																	lag(amt) over (
																			partition by gdft.card_num 
																			order by gdft.trans_date) as lag_a,
																	lag(amt,2) over (
																			partition by gdft.card_num 
																			order by gdft.trans_date) as lag_a2,
																	lag(amt,3) over (
																			partition by gdft.card_num 
																			order by gdft.trans_date) as lag_a3,
																	lag(oper_result) over (
																			partition by gdft.card_num 
																			order by gdft.trans_date) as lag_r,
																	lag(oper_result,2) over (
																			partition by gdft.card_num 
																			order by gdft.trans_date) as lag_r2,
																	lag(oper_result,3) over (
																			partition by gdft.card_num 
																			order by gdft.trans_date) as lag_r3,
																	lag(trans_date,3) over (
																			partition by gdft.card_num 
																			order by gdft.trans_date) as min_t
															from YOUR-USERNAME1.XXXX_dwh_fact_transactions gdft),
													current_dt as ( 
															select trans_date 
															from YOUR-USERNAME1.XXXX_stg_transactions )
													select * 
													from rj
													where oper_result = 'SUCCESS' 
															and lag_r = 'REJECT' 
															and lag_a > amt 
															and lag_r2 = 'REJECT' 
															and lag_a2 > lag_a
															and lag_r3 = 'REJECT' 
															and lag_a3 > lag_a2
															and (trans_date - min_t) <= '00:20:00'
															and trans_date  in (
																	select trans_date 
																	from current_dt)) as trg
											left join YOUR-USERNAME1.XXXX_dwh_dim_cards as gddca
											on trim(trg.card_num) = trim(gddca.card_num )) as t 
									left join YOUR-USERNAME1.XXXX_dwh_dim_accounts as gdda 
									on t.account_num = gdda.account_num ) as t2 
							left join YOUR-USERNAME1.XXXX_dwh_dim_clients as gddcl 
							on t2.client = gddcl.client_id ) as t3;
			""")
	######################################
	conn_YOUR-DB2.commit()
	conn_YOUR-DB1.commit()
	######################################Запись файлов в архив
	os.rename('/home/YOUR-USERNAME1/XXXX/project/terminals_01032021.xlsx', '/home/YOUR-USERNAME1/XXXX/project/archive/terminals_01032021.xlsx.backup')
	os.rename('/home/YOUR-USERNAME1/XXXX/project/transactions_01032021.txt', '/home/YOUR-USERNAME1/XXXX/project/archive/transactions_01032021.txt.backup')
	os.rename('/home/YOUR-USERNAME1/XXXX/project/passport_blacklist_01032021.xlsx', '/home/YOUR-USERNAME1/XXXX/project/archive/passport_blacklist_01032021.xlsx.backup')
	os.rename('/home/YOUR-USERNAME1/XXXX/project/terminals_02032021.xlsx', '/home/YOUR-USERNAME1/XXXX/project/archive/terminals_02032021.xlsx.backup')
	os.rename('/home/YOUR-USERNAME1/XXXX/project/transactions_02032021.txt', '/home/YOUR-USERNAME1/XXXX/project/archive/transactions_02032021.txt.backup')
	os.rename('/home/YOUR-USERNAME1/XXXX/project/passport_blacklist_02032021.xlsx', '/home/YOUR-USERNAME1/XXXX/project/archive/passport_blacklist_02032021.xlsx.backup')
	os.rename('/home/YOUR-USERNAME1/XXXX/project/terminals_03032021.xlsx', '/home/YOUR-USERNAME1/XXXX/project/archive/terminals_03032021.xlsx.backup')
	os.rename('/home/YOUR-USERNAME1/XXXX/project/transactions_03032021.txt', '/home/YOUR-USERNAME1/XXXX/project/archive/transactions_03032021.txt.backup')
	os.rename('/home/YOUR-USERNAME1/XXXX/project/passport_blacklist_03032021.xlsx', '/home/YOUR-USERNAME1/XXXX/project/archive/passport_blacklist_03032021.xlsx.backup')
	######################################
	cursor_YOUR-DB2.close();
	cursor_YOUR-DB1.close();
	$ sudo apt install python3.10-venv
	$ python3.10 -m venv venv
	$ source venv/bin/activate
	$ pip install pandas
	$ python3 main.py
	$ ls archive/
	passport_blacklist_01032021.xlsx.backup  terminals_01032021.xlsx.backup  transactions_01032021.txt.backup
	$ sudo psql -U YOUR-USERNAME
	YOUR-USERNAME=# select * from YOUR-USERNAME.XXXX_rep_fraud;
	Task:
	Заполните файл main.cron расписанием и командой исполнения вашего скрипта. Расписание установите так, как считаете нужным чтобы ваши данные заполнились корректно.
	Decision:
	$ vim main.cron 
	$ cat main.cron 
	0 0 * * * /home/david/project/main.py
	Decision:
	По следующим признакам получилось выявить мошеннические транзакции: выполнение операции с просроченным или заблокированным паспортом, выполнение операции с недействительным контрактом, выполнение операций в разных городах в течение одного часа
	Source:
	https://dzen.ru/a/Ypr65Wh4jmLimA3o
	https://www.youtube.com/watch?v=Je3Y8up0Qbs&list=LL&index=5&t=98s
	https://losst.pro/kak-posmotret-otkrytye-porty-v-linux?ysclid=lpe5qjsv9o445640330
	https://www.pgadmin.org/download/pgadmin-4-apt/
	https://losst.pro/spisok-kontejnerov-docker

Разработка скриптов на Bash и Python для автоматизации работы
	Task:
	Разработал скрипты на языках Bash и Python для автоматизации работы в компьютерных классах и аудиториях
	Task:
	В компьютерном классе по 20 компьютеров и в каждом надо было установить Microsoft Office. Для этого я написал скрипты инсталлятор и конфигуратор, которые позволяют мне выбрать дистрибутив, в котором я установливаю, саму программу для установки и настройки (не только офис)
	Decision:
	$ vim Linux-Installer.sh
	$ cat Linux-Installer.sh
	#!/bin/bash
	checkcmd(){
	if [ $? -eq 0 ]; then
		echo ! The command was executed successfully
	else
		echo ! The command was executed with an error
	fi
	} 
	#echo YOUR-PASSWORD | sudo -S sudo dnf update
	#checkcmd
	# https://stackoverflow.com/questions/226703/how-do-i-prompt-for-yes-no-cancel-input-in-a-linux-shell-script
	while true; do
		read -p "Do you want to install the program? (y/n) " yn
		case $yn in
			[Yy]* ) 
				echo -n "Select the distribution where you want to install the program (Centos 9 - 1 / Ubuntu 22.04 - 2 / Redos 7.3 - 3): "
				read choicedistr
				case "$choicedistr" in
					1|Centos) 
						echo YOUR-PASSWORD | sudo -S sudo dnf -y update
						checkcmd
						echo -n "Select a program (Test - 0 / Sublime Text - 1 / Postgresql - 2 / PgAdmin - 3 / Git - 4 / Kvm - 5 / nfts-3g - 6 / libreoffice - 7): "
						read choiceprogr
						case "$choiceprogr" in
							0)
								echo YOUR-PASSWORD | sudo -S sudo dnf -y update
								checkcmd
								;;
							1)
								# https://www.sublimetext.com/docs/linux_repositories.html#dnf
								echo YOUR-PASSWORD | sudo -S sudo rpm -v --import https://download.sublimetext.com/sublimehq-rpm-pub.gpg
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo dnf config-manager --add-repo https://download.sublimetext.com/rpm/stable/x86_64/sublime-text.repo
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo dnf -y install sublime-text
								checkcmd
								;;
							2)
								# https://www.postgresql.org/download/linux/redhat/
								echo YOUR-PASSWORD | sudo -S sudo dnf install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-9-x86_64/pgdg-redhat-repo-latest.noarch.rpm
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo dnf -qy module disable postgresql
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo dnf install -y postgresql15-server
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo /usr/pgsql-15/bin/postgresql-15-setup initdb
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo systemctl enable postgresql-15
								echo YOUR-PASSWORD | sudo -S sudo systemctl start postgresql-15
								checkcmd
								;;
							3)
								# https://www.pgadmin.org/download/pgadmin-4-rpm/
								echo YOUR-PASSWORD | sudo -S sudo rpm -i https://ftp.postgresql.org/pub/pgadmin/pgadmin4/yum/pgadmin4-redhat-repo-2-1.noarch.rpm
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo yum install -y pgadmin4
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo yum install -y pgadmin4-desktop
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo yum install -y pgadmin4-web
								checkcmd
								sudo /usr/pgadmin4/bin/setup-web.sh
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo yum upgrade -y pgadmin4
								checkcmd
								;;
							4)
								# https://unixcop.com/how-to-install-git-on-centos-9-stream-fedora/
								echo YOUR-PASSWORD | sudo -S sudo dnf -y install git
								checkcmd
								git --version
								;;
							5)
								# https://technixleo.com/install-kvm-on-centos-alma-rhel-9/
								egrep -c '(vmx|svm)' /proc/cpuinfo
								echo YOUR-PASSWORD | sudo -S sudo dnf install -y qemu-kvm qemu-img libvirt virt-install libvirt-client virt-manager
								checkcmd
								lsmod | grep kvm
								echo YOUR-PASSWORD | sudo -S sudo systemctl enable --now libvirtd
								checkcmd
								sudo virt-host-validate							
								;;
							6)
								# https://itisgood.ru/2019/03/26/kak-smontirovat-disk-ntfs-na-centos-rhel-scientific-linux/
								echo YOUR-PASSWORD | sudo -S sudo yum -y install epel-release
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo yum -y install ntfs-3g
								checkcmd			
								;;
							7)
								echo YOUR-PASSWORD | sudo -S sudo dnf -y install libreoffice
								checkcmd			
								;;
						esac
						;;
					2|u|U|Ubuntu) 
						echo YOUR-PASSWORD | sudo -S sudo apt -y update
						echo YOUR-PASSWORD | sudo -S sudo apt -y upgrade
						checkcmd
						echo -n "Select a program (Test - 0 / Net-Tools - 1 / Ssh - 2): "
						read choiceprogr
						case "$choiceprogr" in
							0)
								echo YOUR-PASSWORD | sudo -S sudo apt-get -y update
								checkcmd
								;;
							1)
								echo YOUR-PASSWORD | sudo -S sudo apt-get install net-tools
								checkcmd
								;;
							2)#https://help.reg.ru/support/servery-vps/oblachnyye-servery/rabota-s-serverom/kak-ustanovit-i-nastroit-ssh
								echo YOUR-PASSWORD | sudo -S sudo apt-get install ssh
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo apt-get install openssh-server
								checkcmd
								systemctl status ssh
								;;
						esac
						exit 0
						;;
					3|r|R|Redos)
						echo YOUR-PASSWORD | sudo -S sudo yum -y update
						checkcmd
						echo -n "Select a program (Test - 0 / Office - 1 / Playonlinux - 2): "
						read choiceprogr
						case "$choiceprogr" in
							0)
								echo YOUR-PASSWORD | sudo -S sudo yum -y update
								checkcmd
								;;						
							1)
								echo YOUR-PASSWORD | sudo -S sudo yum -y update
								checkcmd
								wine /LINKS/MS\ Office\ 2007-10-13/Office_Professional_Plus_2007_W32_Russia/SETUP.EXE
								checkcmd	
								;;
							2)
								echo YOUR-PASSWORD | sudo -S sudo yum -y update
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo touch /etc/yum.repos.d/playonlinux.repo
								echo YOUR-PASSWORD | sudo -S sudo echo '[playonlinux]
								name=PlayOnLinux Official repository
								baseurl=http://rpm.playonlinux.com/fedora/yum/base
								enable=1
								gpgcheck=0
								gpgkey=http://rpm.playonlinux.com/public.gpg' > /etc/yum.repos.d/playonlinux.repo
								cat /etc/yum.repos.d/playonlinux.repo
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo yum -y install playonlinux nc jq
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo reboot							
								;;
						esac
						;;
					*) 
						echo "Nothing was entered."
						;;
				esac			
				;;
			[Nn]* ) 
				exit
				;;
			* ) 
				echo "Please answer yes or no."
				;;
		esac
	done
	exit 0
	$ ./Linux-Installer.sh
	Do you want to install the program? (y/n) y
	Select the distribution where you want to install the program (Centos 9 - 1 / Ubuntu 22.04 - 2 / Redos 7.3 - 3): 1
	Select a program (Test - 0 / Sublime Text - 1 / Postgresql - 2 / PgAdmin - 3 / Git - 4 / Kvm - 5 / nfts-3g - 6 / libreoffice - 7): 0
	Do you want to install the program? (y/n) y
	Select the distribution where you want to install the program (Centos 9 - 1 / Ubuntu 22.04 - 2 / Redos 7.3 - 3): 2
	Select a program (Test - 0 / Net-Tools - 1 / Ssh - 2): 0
	$ vim Linux-Config.sh
	$ cat Linux-Config.sh
	#!/bin/bash 
	checkcmd(){
	if [ $? -eq 0 ]; then
		echo ! The command was executed successfully
	else
		echo ! The command was executed with an error
	fi
	}
	# https://stackoverflow.com/questions/226703/how-do-i-prompt-for-yes-no-cancel-input-in-a-linux-shell-script
	while true; do
		read -p "Do you want to configure the program? (y/n) " yn
		case $yn in
			[Yy]* ) 
				echo -n "Select the distribution where you want to configure the program (Redhat - 1 / Debian - 2): "
				read choicedistr
				case "$choicedistr" in
					1|r|R|Redhat) 
						echo YOUR-PASSWORD | sudo -S sudo dnf -y update
						checkcmd
						echo -n "Select a program (Test - 0 / Disabling Lamp - 1 / Starting Lamp - 2 / Add users in postgressql - 3 / Moving linux folders to another disk - 4): "
						read choiceprogr
						case "$choiceprogr" in
							0)
								echo YOUR-PASSWORD | sudo -S sudo dnf -y update
								checkcmd
								;;
							1)
								echo YOUR-PASSWORD | sudo -S sudo service apache2 stop
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo service mysql stop
								checkcmd
								;;
							2)
								echo YOUR-PASSWORD | sudo -S sudo service apache2 start
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo service mysql start
								checkcmd
								;;
							3) #https://www.dmosk.ru/miniinstruktions.php?mini=postgresql-users&ysclid=lig0a2xuou515754413#create
								echo -n "Please come up with a new user: "
								read userdb
								echo -n "Please come up with a new password: "
								read passwdb
								#echo YOUR-PASSWORD | sudo -S 
								cd /tmp
								sudo -u postgres -H -- psql -d template1 -c "CREATE USER $userdb WITH PASSWORD '$passwdb';"
								#psql -U postgres -d template1 -c "CREATE USER tuser0 WITH PASSWORD 'Tpsswd';"
								checkcmd
								echo -n "Please come up with a new base: "
								read namedb
								sudo -u postgres -H -- psql -d template1 -c "CREATE database $namedb;"
								checkcmd
								sudo -u postgres -H -- psql -d template1 -c "GRANT ALL PRIVILEGES ON DATABASE "$namedb" to $userdb;"
								checkcmd
								sudo -u postgres -H -- psql -d template1 -c "\c $namedb;"
								checkcmd
								sudo -u postgres -H -- psql -d template1 -c "GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO "$userdb";"
								checkcmd
								;;
							4) #http://sysengineering.ru/notes/peremeschenie-papok-linux-na-drugoy-disk?ysclid=lisfix46x1712981898
								lsblk
								echo -n "USB device for moving the base (hint above): "
								read mntusb
								echo YOUR-PASSWORD | sudo -S sudo fdisk $mntusb
								lsblk
								echo -n "USB device for moving the base (hint above): "
								read mntusb1
								echo -n "Enter the file system type: "
								read mntfs
								echo YOUR-PASSWORD | sudo -S sudo mkfs -t $mntfs $mntusb1
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo mount $mntusb1 /mnt
								checkcmd
								shopt -s dotglob
								echo -n "Which directory do you need to move: "
								read movedir
								echo YOUR-PASSWORD | sudo -S sudo rsync -aulvXpogtr $movedir/* /mnt
								checkcmd
								ls -Zd $movedir
								ls -Zd /mnt
								echo -n "Change the security settings of a new folder (hint above): "
								read mntset
								echo YOUR-PASSWORD | sudo -S sudo chcon -t $mntset /mnt
								checkcmd
								ls -Zd /mnt
								echo YOUR-PASSWORD | sudo -S sudo umount /mnt
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo blkid
								echo -n "Enter the UUID of the disk (hint above): "
								read mntuid
								echo YOUR-PASSWORD | sudo -S sudo sh -c "echo 'UUID=$mntuid   $movedir  $mntfs  defaults,noatime,nofail 0 2' >> /etc/fstab"
								checkcmd
								echo YOUR-PASSWORD | sudo -S sudo cat /etc/fstab | grep $mntfs
								echo YOUR-PASSWORD | sudo -S sudo mv $movedir $movedir.back
								echo YOUR-PASSWORD | sudo -S sudo mkdir $movedir
								echo YOUR-PASSWORD | sudo -S sudo mount -av
								checkcmd
								ls $movedir
								lsblk						
								;;
						esac
						;;
					2|d|D|Debian) 
						echo "deb"
						exit 0
						;;
					*) 
						echo "Nothing was entered."
						;;
				esac			
				;;
			[Nn]* ) 
				exit
				;;
			* ) 
				echo "Please answer yes or no."
				;;
		esac
	done
	exit 0
	$ ./Linux-Config.sh
	Do you want to configure the program? (y/n) y
	Select the distribution where you want to configure the program (Redhat - 1 / Debian - 2): 1
	Select a program (Test - 0 / Disabling Lamp - 1 / Starting Lamp - 2 / Add users in postgressql - 3 / Moving linux folders to another disk - 4): 0
	Do you want to configure the program? (y/n) n
	Task:
	Столкнулся с такой проблемой, что маленький неттоп флешки не читает, он не был добавлен в домен и антивирус на нем не стоял. Убедимся на другой машине, в моем случае ноутбуке, что флешка спокойно видит на нем файлы.	То есть это чистая флешка, не зараженная, и попробуем ее вставить в тот проблемный неттоп. Тут мы увидим, что во флешке другая информация будет. Как будто в самой флешке отображается сама флешка, и если ее открыть (что делать не стоит) тот там и будут наши файлы. на самом деле они будут зашифрованны и флешка заражена. Надо это исправлять. В тот момент у нас не было в неттопе никакого антивируса, поэтому я установил бесплатный антивирус Касперский FREE с официального сайта. И вот, он выдал в момент запуска после установки рекомендации по устранении проблм в компьютере. - Жмем устранить и перезагрузимся - В следующем запуске антивирус покажет информацию об устранении проблем, и можем сразу вставить флешку в неттопе. Тут мы увидим, что антвирус удалил какой-то файл (это и есть вирус) Открываем саму флешку после удаления вируса, а там пустой файл, хотя система показывает, что флешка заполнена. Он содержит зашифрованные файлы. Давайте попробуем их восстановить. Есть скрипт, который возвращает данные. Вставьте этот файл на флешку - запустите его - увидите файл и папку с файлами, которые ему удалось восстановить - Все готово, и неттоп мы подлечили, и данные со флешки восстановили, и антивирус бесплатный установили, и главное что этот бюджетный вариант с восстановлением вполне корректно работает.
	Decision:
	$ vim Windows-FlashDriveRecovery.bat
	$ cat Windows-FlashDriveRecovery.bat
	dir /AS /B > list.txt 
	FOR /F "eol=# tokens=1* delims=:" %%i in (list.txt) do ( 
	attrib -s -h -r "%%i" 
	) 
	pause
	Task:
	Разработать программу "Менеджер паролей", в которой можно вносить данные в файле.
	Decision:
	$ cat Py-PasswordStorage.py
	'''
	links      | email      | login | password  	- Keys
	http://fds | fgd@fgd.ru | admin | fgdfgdf 	- Values
	'''
	repeat="y"
	while repeat == "y":
		keys=['links', 'mails', 'logins', 'passwords']
		print(keys)	
		#print(keys[1])
		values=[]
		#print(values)
		link, email, login, password= input("Enter the link: "), input("Enter email: "), input("Enter login: "), input("Enter password: ")
		values.append(link)
		values.append(email)
		values.append(login)
		values.append(password)
		print(values)
		#print(values[2])
		row={}
		for i in range(len(keys)):
			#print(keys[i])
			#row[keys[i]] = None
			row[keys[i]] = values[i]
		print(row)
		createfile = input("Create new file? (y/n): ")
		if createfile == "y":
			namefile = input("Name the file where the passwords will be stored: ")
			with open(namefile,'w') as file:
				#file.write(str(row))
				for key, value in row.items():
					#print("Key: " + key)
					keyRow="|	"+str(key)+"	|"
					file.write(keyRow)
				file.write("\n")
				for key, value in row.items():
					valueRow="|	"+str(value)+"	|"
					file.write(valueRow)
				file.close()
		elif createfile == "n":
			namefile = input("Write the name of the file in which you want to make changes: ")
			with open(namefile,'a') as file:
				file.write("\n")
				for key, value in row.items():
					valueRow="|	"+str(value)+"	|"
					file.write(valueRow)
				file.write("\n")
				file.close()
		repeat = input("Do you want to continue? (y/n): ")
		if repeat == "n":
			break
		while (repeat!="y" and repeat!="n"):
			repeat = input("Please enter the correct answer (y/n): ")
	$ python Py-PasswordStorage.py
	['links', 'mails', 'logins', 'passwords']
	Enter the link: 1
	Enter email: 2
	Enter login: 3
	Enter password: 4
	['1', '2', '3', '4']
	{'links': '1', 'mails': '2', 'logins': '3', 'passwords': '4'}
	Create new file? (y/n): y
	Name the file where the passwords will be stored: testfile
	Do you want to continue? (y/n): 1213wa
	Please enter the correct answer (y/n): y
	['links', 'mails', 'logins', 'passwords']
	Enter the link: 11
	Enter email: 22
	Enter login: 33
	Enter password: 4444444
	['11', '22', '33', '4444444']
	{'links': '11', 'mails': '22', 'logins': '33', 'passwords': '4444444'}
	Create new file? (y/n): n
	Write the name of the file in which you want to make changes: testfile
	Do you want to continue? (y/n): n
	$ cat testfile
	| links || mails || logins || passwords |
	| 1 || 2 || 3 || 4 |
	| 11 || 22 || 33 || 4444444 |

База данных "Инвентаризация компьютерной техники в здании"
	Task:
	Развернул виртуальный сервер в Altlinux и разработал в нем базу "Инвентаризация компьютерной техники в здании". Это позволило мне быстро предоставлять отчеты об оборудовании в здании
	Task:
	Установить виртуальный сервер AltLinux в Hyper-V
	Decision:
	PS C:\Windows\system32> $ram=1*1024*1024*1024
	PS C:\Windows\system32> $ram
	1073741824
	PS C:\Windows\system32> $hdd=10*1024*1024*1024
	PS C:\Windows\system32> $hdd
	10737418240
	PS C:\Windows\system32> NEW-VM -Name Alt -MemoryStartupBytes $ram -NewVHDPath 'C:\Data\VM\Alt\Alt.vhdx'-NewVHD
	SizeBytes $hdd -Path 'C:\Data\VM\Alt'
	PS C:\Windows\system32> Start-VM -name Alt
	PS C:\Windows\system32> Get-VM
	Name State   CPUUsage(%) MemoryAssigned(M) Uptime   Status
	---- -----   ----------- ----------------- ------   ------
	Alt  Off     0           0                 00:00:00 Работает нормально
	Task:
	Set up an ActiveDirectory/Login
	Decision:
	$ ssh -X tuser@thost1
	$ su -
	# apt-get install task-auth-ad-sssd
	# net time set -S thost.ru
	# system-auth write ad thost.ru thost1 thost 'tuser' 'tpassword'
	Using short domain name -- thost
	Joined 'thost1' to dns domain 'thost.ru'
	Successfully registered hostname with DNS
	failed to call wbcGetDisplayName: WBC_ERR_WINBIND_NOT_AVAILABLE
	Could not lookup sid S-1-5-21-965402400-3010625364-1855727791-513
	failed to call wbcGetDisplayName: WBC_ERR_WINBIND_NOT_AVAILABLE
	Could not lookup sid S-1-5-21-965402400-3010625364-1855727791-512
	# wbinfo -t
	checking the trust secret for domain thost via RPC calls succeeded
	# acc
	2 keyboards found
	qt.qpa.xcb: could not connect to display
	qt.qpa.plugin: Could not load the Qt platfothost plugin "xcb" in "" even though it was found.
	This applthostation failed to start because no Qt platfothost plugin could be initialized. Reinstalling the applthostation may fix this problem.
	Available platfothost plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, xcb.
	# exit
	$ exit
	$ ssh -X tuser@thost1
	$ su -
	# acc
	2 keyboards found
	QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/.private/root/runtime-root'
	libpng warning: thostCP: known incorrect sRGB profile
	libpng warning: thostCP: known incorrect sRGB profile
	libpng warning: thostCP: known incorrect sRGB profile
	libpng warning: thostCP: known incorrect sRGB profile
	libpng warning: thostCP: known incorrect sRGB profile
	WARNING: (alterator lookout evaluation): imported module (alterator presentation events) overrides core binding `when'
	libpng warning: thostCP: known incorrect sRGB profile
	libpng warning: thostCP: known incorrect sRGB profile
	libpng warning: thostCP: known incorrect sRGB profile
	libpng warning: thostCP: known incorrect sRGB profile
	# vim /etc/net/ifaces/eth0/resolv.conf
	# cat /etc/net/ifaces/eth0/resolv.conf
	nameserver IpAddr1
	# hostnamectl set-hostname thost1.thost.ru
	# cat /etc/resolv.conf
	# Generated by resolvconf
	# Do not edit manually, use
	# /etc/net/ifaces/<interface>/resolv.conf instead.
	search thost.ru
	nameserver 127.0.0.1
	# vim /etc/resolv.conf
	# cat /etc/resolvconf.conf
	# Configuration for resolvconf(8)
	# See resolvconf.conf(5) for details
	resolv_conf_head='# Do not edit manually, use\n# /etc/net/ifaces/<interface>/resolv.conf instead.'
	resolv_conf=/etc/resolv.conf
	# These interfaces will always be processed first.
	interface_order='lo lo[0-9]* lo.*'
	# These interfaces will be processed next, unless they have a metrthost.
	dynamthost_order='tap[0-9]* tun[0-9]* vpn vpn[0-9]* wg[0-9]* ppp[0-9]* ippp[0-9]*'
	#Configuration files for named subscriber.
	named_zones=/var/lib/bind/etc/resolvconf-zones.conf
	named_options=/var/lib/bind/etc/resolvconf-options.conf
	#Configuration files for dnsmasq subscriber.
	dnsmasq_conf=/etc/dnsmasq.conf.d/60-resolvconf
	dnsmasq_resolv=/etc/resolv.conf.dnsmasq
	name_servers=127.0.0.1
	# vim /etc/resolvconf.conf
	# cat /etc/resolvconf.conf
	# Configuration for resolvconf(8)
	# See resolvconf.conf(5) for details
	resolv_conf_head='# Do not edit manually, use\n# /etc/net/ifaces/<interface>/resolv.conf instead.'
	resolv_conf=/etc/resolv.conf
	# These interfaces will always be processed first.
	#interface_order='lo lo[0-9]* lo.*'
	interface_order='lo lo[0-9]* lo.* eth0'
	search_domains=thost.ru
	# These interfaces will be processed next, unless they have a metrthost.
	dynamthost_order='tap[0-9]* tun[0-9]* vpn vpn[0-9]* wg[0-9]* ppp[0-9]* ippp[0-9]*'
	#Configuration files for named subscriber.
	named_zones=/var/lib/bind/etc/resolvconf-zones.conf
	named_options=/var/lib/bind/etc/resolvconf-options.conf
	#Configuration files for dnsmasq subscriber.
	dnsmasq_conf=/etc/dnsmasq.conf.d/60-resolvconf
	dnsmasq_resolv=/etc/resolv.conf.dnsmasq
	#name_servers=127.0.0.1
	# resolvconf -u
	# cat /etc/resolv.conf
	# Generated by resolvconf
	# Do not edit manually, use
	# /etc/net/ifaces/<interface>/resolv.conf instead.
	search thost.ru
	nameserver IpAddr1
	...
	nameserver 8.8.8.8
	# hostname
	thost1.thost.ru
	# dig _kerberos._udp.thost.ru SRV | grep ^_kerberos
	_kerberos._udp.thost.ru. 600 IN SRV 0 100 88 M-1.thost.ru.
	...
	_kerberos._udp.thost.ru. 600 IN SRV 0 100 88 T-1.thost.ru.
	# dig _kerberos._tcp.thost.ru SRV | grep ^_kerberos
	_kerberos._tcp.thost.ru. 600 IN SRV 0 100 88 M-c.thost.ru.
	...
	_kerberos._tcp.thost.ru. 600 IN SRV 0 100 88 T-1.thost.ru.
	# cat /etc/krb5.conf | grep default_realm
	default_realm = thost.ru
	# default_realm = EXAMPLE.COM
	# cat /etc/krb5.conf | grep dns_lookup_realm
	dns_lookup_realm = false
	# cat /etc/krb5.conf | grep dns_lookup_kdc
	dns_lookup_kdc = true
	# exit
	$ ssh -X tuser@thost1
	# kinit tuser
	# klist
	Tthostket cache: KEYRING:persistent:0:krb_ccache_CCjHpN1
	Default principal: a-r@thost.ru
	Valid starting       Expires              Servthoste principal
	10.08.2022 12:30:34  10.08.2022 22:30:34  krbtgt/thost.ru@thost.ru
		renew until 17.08.2022 12:30:32
	# apt-get install samba-client
	# cat /etc/samba/smb.conf | grep realm
		realm = thost.ru
	# cat /etc/samba/smb.conf | grep workgroup
		workgroup = thost
	# cat /etc/samba/smb.conf | grep netbios
		netbios name = thost1
	# cat /etc/samba/smb.conf | grep security
		security = ads
	# cat /etc/samba/smb.conf | grep method
		kerberos method = system keytab
	# cat /etc/samba/smb.conf | grep idmap
			idmap config * : range = 200000-2000200000
			idmap config * : backend = sss
	# vim /etc/samba/smb.conf
	# cat /etc/samba/smb.conf | grep idmap
			idmap config * : range = 200000-2000200000
	;        idmap config * : backend = sss
		idmap config * : backend = tdb
	# testpathost
	Load smb config files from /etc/samba/smb.conf
	Loaded servthostes file OK.
	Weak crypto is allowed
	Server role: ROLE_DOMAIN_MEMBER
	Press enter to see a dump of your servthoste definitions
	# Global parameters
	[global]
		kerberos method = system keytab
		machine password timeout = 0
		realm = thost.ru
		security = ADS
		template homedir = /home/thost.ru/%U
		template shell = /bin/bash
		winbind use default domain = Yes
		workgroup = thost
		idmap config * : range = 200000-2000200000
		idmap config * : backend = tdb
	[share]
		comment = Commonplace
		path = /srv/share
		read only = No
	[homes]
		browseable = No
		comment = Home Directory for '%u'
		read only = No
	# net ads join -U tuser
	Enter tuser's password:
	Using short domain name -- thost
	Joined 'thost1' to dns domain 'thost.ru'
	No DNS domain configured for thost1. Unable to perfothost DNS Update.
	DNS update failed: NT_STATUS_INVALID_PARAMETER
	# cat /etc/hosts
	127.0.0.1   localhost.localdomain localhost
	# vim /etc/hosts
	# cat /etc/hosts
	127.0.0.1   localhost.localdomain localhost
	127.0.0.1   thost1.thost.ru thost1
	# net ads join -U tuser
	Enter tuser's password:
	Using short domain name -- thost
	Joined 'thost1' to dns domain 'thost.ru'
	kerberos_kinit_password thost1$@thost.ru failed: Preauthentthostation failed
	DNS update failed: kinit failed: Preauthentthostation failed
	# vim /etc/hosts
	# cat /etc/hosts
	127.0.0.1   thost1.thost.ru thost1
	# net ads join -U tuser
	Enter tuser's password:
	Using short domain name -- thost
	Joined 'thost1' to dns domain 'thost.ru'
	# klist -k -e
	Keytab name: FILE:/etc/krb5.keytab
	KVNO Principal
	---- --------------------------------------------------------------------------
	9 host/thost1.thost.ru@thost.ru (aes256-cts-hmac-sha1-96)
	9 host/thost1@thost.ru (aes256-cts-hmac-sha1-96)
	9 host/thost1.thost.ru@thost.ru (aes128-cts-hmac-sha1-96)
	9 host/thost1@thost.ru (aes128-cts-hmac-sha1-96)
	9 host/thost1.thost.ru@thost.ru (DEPRECATED:arcfour-hmac)
	9 host/thost1@thost.ru (DEPRECATED:arcfour-hmac)
	9 thost1$@thost.ru (aes256-cts-hmac-sha1-96)
	9 thost1$@thost.ru (aes128-cts-hmac-sha1-96)
	9 thost1$@thost.ru (DEPRECATED:arcfour-hmac)
	8 host/thost1.thost.ru@thost.ru (aes256-cts-hmac-sha1-96)
	8 host/thost1@thost.ru (aes256-cts-hmac-sha1-96)
	8 host/thost1.thost.ru@thost.ru (aes128-cts-hmac-sha1-96)
	8 host/thost1@thost.ru (aes128-cts-hmac-sha1-96)
	8 host/thost1.thost.ru@thost.ru (DEPRECATED:arcfour-hmac)
	8 host/thost1@thost.ru (DEPRECATED:arcfour-hmac)
	8 thost1$@thost.ru (aes256-cts-hmac-sha1-96)
	8 thost1$@thost.ru (aes128-cts-hmac-sha1-96)
	8 thost1$@thost.ru (DEPRECATED:arcfour-hmac)
	# apt-get install sssd-ad
	# cat /etc/sssd/sssd.conf
	[sssd]
	config_file_version = 2
	servthostes = nss, pam
	# Managed by system facility command:
	## control sssd-drop-privileges unprivileged|privileged|default
	user = _sssd
	# SSSD will not start if you do not configure any domains.
	domains = thost.ru
	[nss]
	[pam]
	[domain/thost.ru]
	id_provider = ad
	auth_provider = ad
	chpass_provider = ad
	access_provider = ad
	default_shell = /bin/bash
	fallback_homedir = /home/%d/%u
	debug_level = 0
	; cache_credentials = false
	ad_gpo_ignore_unreadable = true
	ad_gpo_access_control = pethostissive
	ad_update_samba_machine_account_password = true
	# vim /etc/sssd/sssd.conf
	# cat /etc/sssd/sssd.conf
	[sssd]
	config_file_version = 2
	servthostes = nss, pam
	# Managed by system facility command:
	## control sssd-drop-privileges unprivileged|privileged|default
	#user = _sssd
	user=root
	# SSSD will not start if you do not configure any domains.
	domains = thost.ru
	[nss]
	[pam]
	[domain/thost.ru]
	id_provider = ad
	auth_provider = ad
	chpass_provider = ad
	access_provider = ad
	;ldap_id_mapping = False
	default_shell = /bin/bash
	fallback_homedir = /home/%d/%u
	debug_level = 0
	;use_fully_qualified_names = True
	; cache_credentials = True
	ad_gpo_ignore_unreadable = true
	ad_gpo_access_control = pethostissive
	ad_update_samba_machine_account_password = true
	# grep sss /etc/nsswitch.conf
	passwd: files sss
	shadow: tcb files sss
	group: files [SUCCESS=merge] sss role
	# control system-auth sss
	# servthoste sssd status
	active
	# servthoste sssd start
	# getent passwd tuser
	tuser:*:1-9:1-3:в-н:/home/thost.ru/tuser:/bin/bash
	# id tuser
	uid=1-9(tuser) gid=1-3(пользователи домена) группы=1-3(пользователи домена),1-8(администраторы dhcp),1-0(I-s),11-0(пользователи филиалы),1-1(tusert_users),1-9(i-n)
	# net ads info
	LDAP server: IpAddr2
	LDAP server name: MOW-1.thost.ru
	Realm: thost.ru
	Bind Path: dc=thost,dc=RU
	LDAP port: 389
	Server time: Ср, 10 авг 2022 13:08:06 +08
	KDC server: IpAddr2
	Server time offset: 0
	Last machine account password change: Ср, 10 авг 2022 12:50:51 +08
	# net ads testjoin
	Join is OK
	# cat /etc/lightdm/lightdm.conf | grep greeter-hide-
	# greeter-hide-users = True to hide the user list
	#greeter-hide-users=false
	greeter-hide-users = true
	# cat /etc/lightdm/lightdm-gtk-greeter.conf | grep show-language
	show-language-selector = false
	# cat /etc/lightdm/lightdm-gtk-greeter.conf | grep show-indthostators
	# vim /etc/lightdm/lightdm-gtk-greeter.conf
	# cat /etc/lightdm/lightdm-gtk-greeter.conf | grep show-indthostators
	show-indthostators=~a11y;~power;~session;~language
	# vim /etc/lightdm/lightdm-gtk-greeter.conf
	# cat /etc/lightdm/lightdm-gtk-greeter.conf | grep enter-
	enter-username = true
	# reboot
	$ ssh -X tuser@thost1
	Decision:
	$ su -
	# apt-get install libnss-role
	# groupadd -r localadmins
	groupadd: группа «localadmins» уже существует
	# groupadd -r remote
	groupadd: группа «remote» уже существует
	# control sshd-allow-groups enabled
	# sed -i 's/AllowGroups.*/AllowGroups = remote/' /etc/openssh/sshd_config
	# roleadd users cdwriter cdrom audio proc radio camera floppy xgrp scanner uucp fuse
	# roleadd localadmins wheel remote vboxusers
	# roleadd 'Domain Users' users
	Error 156: No such group
	# roleadd 'Пользователи домена' users
	# roleadd 'Администраторы домена' localadmins
	# rolelst
	users:cdwriter,cdrom,audio,proc,radio,camera,floppy,xgrp,scanner,uucp,fuse,video,vboxusers,vboxadd
	localadmins:wheel,remote,vboxusers,vboxadd
	пользователи домена:users
	администраторы домена:localadmins
	powerusers:remote,vboxadd,vboxusers
	vboxadd:vboxsf
	# id tuser
	uid=1-9(tuser) gid=1-3(пользователи домена) группы=1-3(пользователи домена),11-0(пользователи филиалы),1-9(i-n),1-0(I-s),1-1(tusert_users),1-8(администраторы dhcp),100(users),80(cdwriter),22(cdrom),81(audio),19(proc),83(radio),440(camera),71(floppy),466(xgrp),467(scanner),14(uucp),483(fuse),488(video),481(vboxusers),455(vboxadd),454(vboxsf)
	# roleadd 'I-s' localadmins
	# roleadd 'I-s' wheel
	# rolelst
	users:cdwriter,cdrom,audio,proc,radio,camera,floppy,xgrp,scanner,uucp,fuse,video,vboxusers,vboxadd
	localadmins:wheel,remote,vboxusers,vboxadd
	пользователи домена:users
	администраторы домена:localadmins
	I-s:localadmins
	powerusers:remote,vboxadd,vboxusers
	vboxadd:vboxsf
	# exit
	$ exit
	$ ssh -X tuser@thost1
	$ su -
	# id tuser
	uid=1-9(tuser) gid=1-3(пользователи домена) группы=1-3(пользователи домена),11-0(пользователи филиалы),1-9(i-n),1-0(I-s),1-1(tusert_users),1-8(администраторы dhcp),100(users),80(cdwriter),22(cdrom),81(audio),19(proc),83(radio),440(camera),71(floppy),466(xgrp),467(scanner),14(uucp),483(fuse),488(video),481(vboxusers),455(vboxadd),454(vboxsf),101(localadmins),10(wheel),110(remote)
	Task:
	Добавить сетевые папки
	Decision:
	# apt-get install autofs
	# vim /etc/auto.master
	# cat /etc/auto.master
	# Fothostat of this file:
	# mountpoint map options
	# For details of the fothostat look at autofs(8).
	/mnt/auto   /etc/auto.tab   -t 5
	/mnt/net    /etc/auto.avahi -t 120
	/mnt/.tdirectory  /etc/auto.samba --ghost
	# vim /etc/auto.samba
	# cat /etc/auto.samba
	s    -fstype=cifs,multiuser,cruid=$USER,sec=krb5,domain=thost.ru,vers=1.0 ://thost/tdirectory1
	o    -fstype=cifs,multiuser,cruid=$USER,sec=krb5,domain=thost.ru,vers=1.0 ://thost/tdirectory2
	# systemctl enable autofs
	# systemctl start autofs
	# ls -la /mnt/.tdirectory/
	drwxr-xr-x 4 root root    0 авг 11 14:09 .
	drwxr-xr-x 5 root root 4096 авг 11 14:09 ..
	d????????? ? ?    ?       ?            ? o
	d????????? ? ?    ?       ?            ? s
	Task:
	Creating a Network Bridge interface
	Decision:
	# mkdir /etc/net/ifaces/tethernet1
	# cp /etc/net/ifaces/tethernet/* /etc/net/ifaces/tethernet1
	# rm -f /etc/net/ifaces/tethernet/{i,r}*
	# ls /etc/net/ifaces/tethernet1/
	ipv4address  options  resolv.conf
	# cat /etc/net/ifaces/tethernet1/options
	BOOTPROTO=dhcp
	TYPE=eth
	NM_CONTROLLED=yes
	DISABLED=yes
	CONFIG_WIRELESS=no
	SYSTEMD_BOOTPROTO=dhcp4
	CONFIG_IPV4=yes
	SYSTEMD_CONTROLLED=no
	# vim /etc/net/ifaces/tethernet1/options
	# ls /etc/net/ifaces/
	default  tethernet  lo  unknown  tethernet1
	# vim /etc/net/ifaces/tethernet1/options
	# cat /etc/net/ifaces/tethernet1/options
	BOOTPROTO=static
	CONFIG_WIRELESS=no
	CONFIG_IPV4=yes
	HOST='tethernet'
	ONBOOT=yes
	TYPE=bri
	# ls /etc/net/ifaces/tethernet/
	ipv4address  options  resolv.conf
	# service network restart
	Task:
	Настройка сервера Postgresql+Pgadmin
	Decision:
	# apt-get update
	# apt-get install postgresql12-server
	# /etc/init.d/postgresql initdb
	# systemctl start postgresql
	# systemctl enable postgresql
	# pg_isready
	Decision:
	# psql -U postgres
	postgres=# CREATE USER tbuser WITH PASSWORD 'tbpassword';
	postgres=# CREATE DATABASE tbbase;
	postgres=# GRANT ALL PRIVILEGES ON DATABASE tbbase to tbuser;
	postgres=# psql -U postgres -c "\l+"
	                                                                         Список баз данных
	     Имя     | Владелец | Кодировка | LC_COLLATE  |  LC_CTYPE   |     Права доступа     | Размер  | Табл. пространство |                  Опис
	ание                  
	-------------+----------+-----------+-------------+-------------+-----------------------+---------+--------------------+----------------------
	----------------------
	 tbbase | postgres | UTF8      | ru_RU.UTF-8 | ru_RU.UTF-8 | =Tc/postgres         +| 8041 kB | pg_default         |
	             |          |           |             |             | postgres=CTc/postgres+|         |                    |
	             |          |           |             |             | tbuser=CTc/postgres    |         |                    |
	 postgres    | postgres | UTF8      | ru_RU.UTF-8 | ru_RU.UTF-8 |                       | 8185 kB | pg_default         | default administrativ
	e connection database
	...
	postgres=# \c tbbase
	tbbase=# GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO "tbuser";
	tbbase=# ALTER DATABASE tbbase OWNER TO tbuser;
	tbbase=# \q
	Task:
	Сделаем так, чтобы с клиентской машины Redos мы могли подключаться к серверу AltLinux удаленно
	Decision:
	# su - postgres -c "psql -c 'SHOW config_file;'"
	             config_file             
	-------------------------------------
	 /var/lib/pgsql/data/postgresql.conf
	(1 строка)
	# echo "listen_addresses = 'IpAddr3, IpAddr2, thost1'" >> /var/lib/pgsql/data/postgresql.conf
	# cat /var/lib/pgsql/data/pg_hba.conf
	...
	# "local" is for Unix domain socket connections only
	local   all             all                                     trust
	# IPv4 local connections:
	host    all             all             127.0.0.1/32            trust
	...
	# vim /var/lib/pgsql/data/pg_hba.conf
	# cat /var/lib/pgsql/data/pg_hba.conf | grep '10.38.'
	# "local" is for Unix domain socket connections only
	local   all             all                                     trust
	# IPv4 local connections:
	host    tbbase            tbuser             IpAddr2/21                  md5
	host    all             all             127.0.0.1/32            trust
	# systemctl restart postgresql
	# systemctl status postgresql
	$ psql -d tbbase -U tbuser -h thost1
	Task:
	Для работы с базой в графическом интерфейсе установим Pgadmin
	Decision:
	# apt-get install pgadmin3
	# pgadmin3 &
		add server - name - tbuser - host - thost1- password - tbpassword - ok
	# dnf install postgresql-server
	# postgresql-setup initdb
	# systemctl enable postgresql
	# systemctl start postgresql
	# pg_isready
	$ psql -d tbbase -U tbuser -h thost1
	tbbase=> \du
	                                          Список ролей
	 Имя роли |                                Атрибуты                                 | Член ролей
	----------+-------------------------------------------------------------------------+------------
	 tbuser   |                                                                         | {}
	 postgres | Суперпользователь, Создаёт роли, Создаёт БД, Репликация, Пропускать RLS | {}
	Decision:
	$ sudo dnf install pgadmin4 pgadmin4-qt
	$ pgadmin4-qt &
		root's password - add server - name - tbuser - host - thost1- password - tbpassword - ok
	Task:
	Создать таблицы. В первой таблице отметим в каких зданиях находятся кабинеты.
	Decision:
	CREATE TABLE offices (
	  id SERIAL PRIMARY KEY,
	  office VARCHAR,
	  building VARCHAR
	);
	INSERT INTO offices (office, building)
	VALUES ('1', 'Улица1'),
	('2', 'Улица1'),
	('3', 'Улица1'),
	('4', 'Улица2'),
	('5', 'Улица2'),
	('6', 'Улица2'),
	('7', 'Улица2'),
	('8', 'Улица2'),
	('9', 'Улица2');
	SELECT * FROM offices;
	+====+========+===============+
	| id | office | building      |
	+====+========+===============+
	| 1  | 1     | Улица1		|
	+----+--------+---------------+
	| 2  | 2     | Улица1		|
	+----+--------+---------------+
	| 3  | 3     | Улица1		|
	+----+--------+---------------+
	| 4  | 4    | Улица2		|
	+----+--------+---------------+
	| 5  | 5    | Улица2		|
	+----+--------+---------------+
	| 6  | 6    | Улица2		|
	+----+--------+---------------+
	| 7  | 7    | Улица2		|
	+----+--------+---------------+
	| 8  | 8    | Улица2		|
	+----+--------+---------------+
	| 9  | 9    | Улица2		|
	+----+--------+---------------+
	Task:
	Вторая таблица отвечает за названия оборудований
	Decision:
	CREATE TABLE devices (
	  id SERIAL PRIMARY KEY,
	  device VARCHAR,
	  title VARCHAR
	);
	INSERT INTO devices (device, title)
	VALUES ('Atc', 'Panasonic kx-tda100ru'),
	('Bидеокамера белая', NULL),
	('Интерактиная доска с короткофокусным проектором', 'Promethean activeboard i78 mount dlp'),
	('Колонки', 'Sven sps-605'),
	('Компьютер', 'Nuc'),
	('Программно-аапаратный комплекс', 'Vipnet coordinator hw 1000'),
	('Сетевой фильтр', 'Sven platinum'),
	('Маршрутизатор', 'Mtuserrottuser'),
	('Сетевой фильтр', 'Makel с заземлением mpg 137'),
	('Роутер', 'Mtuserrottuser browder rb9516'),
	('Dvd-плеер', 'Pioner dv-310-k'),
	('Акустическая система', 'Electrovoice'),
	('Коммутатор', 'Kramer'),
	('Масштабатор видео и графики', 'Kramer'),
	('Микрофон делегата настольный', NULL),
	('Микрофонная стойка', 'Qutuser lok a300'),
	('Мультимедийный проектор', 'Epson eb-965h'),
	('Пульт микшерский', 'Soundcraft epm8'),
	('Система вентилляции и кондиционирования', NULL),
	('Эран моторизированный', 'Projecta compact electrol'),
	('Ноутбук', 'Acer aspire as5560g-4333g32mn'),
	('Bидеокамера серая', NULL),
	('Микрофон', 'Shure sh58'),
	('Акустическая система', 'Mackie sr1530z'),
	('Сетевой фильтр', 'Sven optima'),
	('Сетевой фильтр', 'Surge protector'),
	('Мультимедийный проектор', 'Optima ex612'),
	('Экран настенный', 'Screenmedia economy-p'),
	('Колонки', 'Logitech s120'),
	('Компьютер', 'Nuc mini pc kit'),
	('Сетевой фильтр', 'Pc pet'),
	('Интерактиная доска', '80'),
	('Короткофокусный проектор', 'Benq'),
	('Колонки', 'Microlab solo 1'),
	('Акустическая система', 'Eurosound eg-26w 2*6'),
	('Микшерный пульт', 'Eurosound compact-802'),
	('Планшет графический', 'Wacom pl-1600'),
	('Телевизор плазменный', 'Lg 50pa6500'),
	('Усилитель мощности', 'Eurosound xz-400'),
	('8 port video splitter', NULL);
	SELECT * FROM devices;
	+====+=================================================+======================================+
	| id | device                                          | title                                |
	+====+=================================================+======================================+
	| 1  | Atc                                             | Panasonic kx-tda100ru                |
	+----+-------------------------------------------------+--------------------------------------+
	| 2  | Bидеокамера белая                               | (null)                               |
	+----+-------------------------------------------------+--------------------------------------+
	| 3  | Интерактиная доска с короткофокусным проектором | Promethean activeboard i78 mount dlp |
	+----+-------------------------------------------------+--------------------------------------+
	| 4  | Колонки                                         | Sven sps-605                         |
	+----+-------------------------------------------------+--------------------------------------+
	| 5  | Компьютер                                       | Nuc                                  |
	+----+-------------------------------------------------+--------------------------------------+
	| 6  | Программно-аапаратный комплекс                  | Vipnet coordinator hw 1000           |
	+----+-------------------------------------------------+--------------------------------------+
	| 7  | Сетевой фильтр                                  | Sven platinum                        |
	+----+-------------------------------------------------+--------------------------------------+
	| 8  | Маршрутизатор                                   | Mtuserrottuser                       |
	+----+-------------------------------------------------+--------------------------------------+
	| 9  | Сетевой фильтр                                  | Makel с заземлением mpg 137          |
	+----+-------------------------------------------------+--------------------------------------+
	| 10 | Роутер                                          | Mtuserrottuser browder rb9516        |
	+----+-------------------------------------------------+--------------------------------------+
	| 11 | Dvd-плеер                                       | Pioner dv-310-k                      |
	+----+-------------------------------------------------+--------------------------------------+
	| 12 | Акустическая система                            | Electrovoice                         |
	+----+-------------------------------------------------+--------------------------------------+
	| 13 | Коммутатор                                      | Kramer                               |
	+----+-------------------------------------------------+--------------------------------------+
	| 14 | Масштабатор видео и графики                     | Kramer                               |
	+----+-------------------------------------------------+--------------------------------------+
	| 15 | Микрофон делегата настольный                    | (null)                               |
	+----+-------------------------------------------------+--------------------------------------+
	| 16 | Микрофонная стойка                              | Qutuser lok a300                     |
	+----+-------------------------------------------------+--------------------------------------+
	| 17 | Мультимедийный проектор                         | Epson eb-965h                        |
	+----+-------------------------------------------------+--------------------------------------+
	| 18 | Пульт микшерский                                | Soundcraft epm8                      |
	+----+-------------------------------------------------+--------------------------------------+
	| 19 | Система вентилляции и кондиционирования         | (null)                               |
	+----+-------------------------------------------------+--------------------------------------+
	| 20 | Эран моторизированный                           | Projecta compact electrol            |
	+----+-------------------------------------------------+--------------------------------------+
	| 21 | Ноутбук                                         | Acer aspire as5560g-4333g32mn        |
	+----+-------------------------------------------------+--------------------------------------+
	| 22 | Bидеокамера серая                               | (null)                               |
	+----+-------------------------------------------------+--------------------------------------+
	| 23 | Микрофон                                        | Shure sh58                           |
	+----+-------------------------------------------------+--------------------------------------+
	| 24 | Акустическая система                            | Mackie sr1530z                       |
	+----+-------------------------------------------------+--------------------------------------+
	| 25 | Сетевой фильтр                                  | Sven optima                          |
	+----+-------------------------------------------------+--------------------------------------+
	| 26 | Сетевой фильтр                                  | Surge protector                      |
	+----+-------------------------------------------------+--------------------------------------+
	| 27 | Мультимедийный проектор                         | Optima ex612                         |
	+----+-------------------------------------------------+--------------------------------------+
	| 28 | Экран настенный                                 | Screenmedia economy-p                |
	+----+-------------------------------------------------+--------------------------------------+
	| 29 | Колонки                                         | Logitech s120                        |
	+----+-------------------------------------------------+--------------------------------------+
	| 30 | Компьютер                                       | Nuc mini pc kit                      |
	+----+-------------------------------------------------+--------------------------------------+
	| 31 | Сетевой фильтр                                  | Pc pet                               |
	+----+-------------------------------------------------+--------------------------------------+
	| 32 | Интерактиная доска                              | 80                                   |
	+----+-------------------------------------------------+--------------------------------------+
	| 33 | Короткофокусный проектор                        | Benq                                 |
	+----+-------------------------------------------------+--------------------------------------+
	| 34 | Колонки                                         | Microlab solo 1                      |
	+----+-------------------------------------------------+--------------------------------------+
	| 35 | Акустическая система                            | Eurosound eg-26w 2*6                 |
	+----+-------------------------------------------------+--------------------------------------+
	| 36 | Микшерный пульт                                 | Eurosound compact-802                |
	+----+-------------------------------------------------+--------------------------------------+
	| 37 | Планшет графический                             | Wacom pl-1600                        |
	+----+-------------------------------------------------+--------------------------------------+
	| 38 | Телевизор плазменный                            | Lg 50pa6500                          |
	+----+-------------------------------------------------+--------------------------------------+
	| 39 | Усилитель мощности                              | Eurosound xz-400                     |
	+----+-------------------------------------------------+--------------------------------------+
	| 40 | 8 port video splitter                           | (null)                               |
	+----+-------------------------------------------------+--------------------------------------+
	Task:
	Третья таблица отвечает за инвентарные номера оборудований, кабинетов и даты изменения
	Decision:
	CREATE TABLE inventories (
	  id SERIAL PRIMARY KEY,
	  inventory VARCHAR,
	  device_id INT REFERENCES devices(id),
	  office_id INT REFERENCES offices(id),                                               
	  quantity INT,                                                   
	  date TIMESTAMP
	);
	INSERT INTO inventories (inventory, device_id, office_id, quantity, date)
	VALUES ('457', '1', '1', '1', '2017-12-12 12:12:12'),
	(NULL, '2', '1', '1', '2017-12-12 12:12:12'),
	...
	('750', '30', '6', '1', '2021-09-12 12:12:12'),
	('707', '17', '6', '1', '2020-05-12 12:12:12'),
	('008', '4', '6', '1', '2020-09-12 12:12:12'),
	('010', '34', '9', '1', '2017-12-12 12:12:12'),
	('650', '5', '9', '1', '2017-12-12 12:12:12'),
	('556', '33', '9', '1', '2017-12-12 12:12:12'),
	('426', '32', '9', '1', '2017-12-12 12:12:12');
	SELECT * FROM inventories;
	+====+=============+===========+===========+==========+=====================+
	| id | inventory   | device_id | office_id | quantity | date                |
	+====+=============+===========+===========+==========+=====================+
	| 1  | 457         | 1         | 1         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	| 2  | (null)      | 2         | 1         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	...
	+----+-------------+-----------+-----------+----------+---------------------+
	| 62 | 750         | 30        | 6         | 1        | 2021-09-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	| 63 | 707         | 17        | 6         | 1        | 2020-05-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	| 64 | 008         | 4         | 6         | 1        | 2020-09-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	| 65 | 010         | 34        | 9         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	| 66 | 650         | 5         | 9         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	| 67 | 556         | 33        | 9         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	| 68 | 426         | 32        | 9         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	Task:
	Мне нужно вывести информацию, где в первом столбце будет инвентарные номера, во втором столбце, устройство и его название, а в третьем столбце вывести кабинеты, где находятся те самые оборудования
	Decision:
	SELECT inventory, device, title, office
	FROM inventories
	INNER JOIN devices
	ON inventories.device_id = devices.id
	INNER JOIN offices
	ON inventories.office_id = offices.id
	WHERE office_id='6';
	+=============+=========================+=================+========+
	| inventory   | device                  | title           | office |
	+=============+=========================+=================+========+
	| 008         | Колонки                 | Sven sps-605    | 6      |
	+-------------+-------------------------+-----------------+--------+
	| 707         | Мультимедийный проектор | Epson eb-965h   | 6      |
	+-------------+-------------------------+-----------------+--------+
	| 750         | Компьютер               | Nuc mini pc kit | 6      |
	+-------------+-------------------------+-----------------+--------+
	Task:
	Заметил ошибку в таблице, есть лишнее оборудование в кабинете. его на самом деле нету в кабинете и значит не должно быть в таблице.
	Decision:
	DELETE FROM inventories
	WHERE id='64';
	SELECT inventory, device, title, office
	FROM inventories
	INNER JOIN devices
	ON inventories.device_id = devices.id
	INNER JOIN offices
	ON inventories.office_id = offices.id
	WHERE office_id='6';
	+===========+=========================+=================+========+
	| inventory | device                  | title           | office |
	+===========+=========================+=================+========+
	| 707       | Мультимедийный проектор | Epson eb-965h   | 6      |
	+-----------+-------------------------+-----------------+--------+
	| 750       | Компьютер               | Nuc mini pc kit | 6      |
	+-----------+-------------------------+-----------------+--------+
	SELECT * FROM inventories;
	+====+=============+===========+===========+==========+=====================+
	| id | inventory   | device_id | office_id | quantity | date                |
	+====+=============+===========+===========+==========+=====================+
	| 1  | 457         | 1         | 1         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	| 2  | (null)      | 2         | 1         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	...
	+----+-------------+-----------+-----------+----------+---------------------+
	| 62 | 750         | 30        | 6         | 1        | 2021-09-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	| 63 | 707         | 17        | 6         | 1        | 2020-05-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	| 65 | 010         | 34        | 9         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	| 66 | 650         | 5         | 9         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	| 67 | 556         | 33        | 9         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	| 68 | 426         | 32        | 9         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	Task:
	Появилось новое обрудование, и нужно его внести в таблицу.
	Decision:
	INSERT INTO inventories (inventory, device_id, office_id, quantity, date)
	VALUES ('testinv1', '39', '1', '1', '2017-12-12 12:12:12');
	SELECT * FROM inventories;
	+====+=============+===========+===========+==========+=====================+
	| id | inventory   | device_id | office_id | quantity | date                |
	+====+=============+===========+===========+==========+=====================+
	| 1  | 457         | 1         | 1         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	| 2  | (null)      | 2         | 1         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	...
	+----+-------------+-----------+-----------+----------+---------------------+
	| 68 | 426         | 32        | 9         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	| 69 | testinv1    | 39        | 1         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	Task:
	в предыдущей задаче не верные данные внес в таблицу, нужно подкорректировать.
	Decision:
	UPDATE inventories
	SET inventory='testinv2', office_id='9'
	WHERE id='69';
	SELECT * FROM inventories;
	+====+=============+===========+===========+==========+=====================+
	| id | inventory   | device_id | office_id | quantity | date                |
	+====+=============+===========+===========+==========+=====================+
	| 1  | 457         | 1         | 1         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	| 2  | (null)      | 2         | 1         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	...
	+----+-------------+-----------+-----------+----------+---------------------+
	| 68 | 426         | 32        | 9         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	| 69 | testinv2    | 39        | 9         | 1        | 2017-12-12 12:12:12 |
	+----+-------------+-----------+-----------+----------+---------------------+
	SELECT inventory, device, title, office
	FROM inventories
	INNER JOIN devices
	ON inventories.device_id = devices.id
	INNER JOIN offices
	ON inventories.office_id = offices.id
	WHERE office_id='9';
	+=============+==========================+==================+========+
	| inventory   | device                   | title            | office |
	+=============+==========================+==================+========+
	| 650         | Компьютер                | Nuc              | 9      |
	+-------------+--------------------------+------------------+--------+
	| 426         | Интерактиная доска       | 80               | 9      |
	+-------------+--------------------------+------------------+--------+
	| 556         | Короткофокусный проектор | Benq             | 9      |
	+-------------+--------------------------+------------------+--------+
	| 010         | Колонки                  | Microlab solo 1  | 9      |
	+-------------+--------------------------+------------------+--------+
	| testinv2    | Усилитель мощности       | Eurosound xz-400 | 9      |
	+-------------+--------------------------+------------------+--------+
	Source:
	https://www.altlinux.org/ActiveDirectory/Login#%D0%A3%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BA%D0%B0_%D0%BF%D0%B0%D0%BA%D0%B5%D1%82%D0%BE%D0%B2
	https://www.altlinux.org/SSSD/AD
	https://www.altlinux.org/%D0%A1%D0%B5%D1%82%D0%B5%D0%B2%D0%BE%D0%B9_%D0%BC%D0%BE%D1%81%D1%82
	https://www.altlinux.org/PostgreSQL#%D0%A3%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BA%D0%B0_%D0%B8_%D0%BD%D0%B0%D1%87%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9_%D0%B7%D0%B0%D0%BF%D1%83%D1%81%D0%BA
	https://www.tecmint.com/install-postgresql-and-pgadmin-in-ubuntu/
	https://o7planning.org/11353/install-pgadmin-on-ubuntu
	https://redos.red-soft.ru/base/server-configuring/dbms/install-postgresql/?sphrase_id=53348
	https://redos.red-soft.ru/base/server-configuring/dbms/pgadmin4/

Установка и настройка Linux в рамках импортозамещения
	Task:
	Для импортозамещения с Windows на Redos, развернул виртуальную тестовую машину Redos в Hyber-v, протестировал машину на работоспобность с программами в организации, развернул Pxe сервер для развертывания Redos с загрузкой в Uefi по сети. Это сэкономило время на внедрение системы Redos в компьютерных классах
	Task:
	Установить Hyper-V в Windows Server 2012 c помощью Shell
	Decision:
	PS C:\Windows\system32> Get-Module -ListAvailable
	Каталог: C:\Windows\system32\WindowsPowerShell\v1.0\Modules
	ModuleType Version    Name                                ExportedCo
	---------- -------    ----                                ----------
	...
	Binary     1.1        Hyper-V                             {Add-VMDvd
	...
	PS C:\Windows\system32> Import-Module -Name Hyper-V
	Task:
	Развернуть виртуальную тестовую машину Redos в Hyber-v с помощью Powershell
	Decision:
	PS C:\Windows\system32> Get-VM
	Name State CPUUsage(%) MemoryAssigned(M) Uptime   Status
	---- ----- ----------- ----------------- ------   ------
	Alt  Off   0           0                 00:00:00 Работает нормально
	PS C:\Windows\system32> $ram=1*1024*1024*1024
	PS C:\Windows\system32> $ram
	1073741824
	PS C:\Windows\system32> $hdd=10*1024*1024*1024
	PS C:\Windows\system32> $hdd
	10737418240
	PS C:\Windows\system32> NEW-VM -Name Redos -MemoryStartupBytes $ram -NewVHDPath 'C:\Data\VM\Redos\Redos.vhdx'-NewVHD
	SizeBytes $hdd -Path 'C:\Data\VM\Redos'
	Name State CPUUsage(%) MemoryAssigned(M) Uptime   Status
	---- ----- ----------- ----------------- ------   ------
	Redos Off   0           0                 00:00:00 Работает нормально
	PS C:\Windows\system32> Get-VM
	Name State CPUUsage(%) MemoryAssigned(M) Uptime   Status
	---- ----- ----------- ----------------- ------   ------
	Alt  Off   0           0                 00:00:00 Работает нормально
	Redos Off   0           0                 00:00:00 Работает нормально
	PS C:\Windows\system32> Start-VM -name Redos
	PS C:\Windows\system32> Get-VM
	Name State   CPUUsage(%) MemoryAssigned(M) Uptime   Status
	---- -----   ----------- ----------------- ------   ------
	Alt  Off     0           0                 00:00:00 Работает нормально
	Redos Running 0           1024              00:00:16 Работает нормально
	PS C:\Windows\system32> Stop-VM Redos
	Подтверждение
	Hyper-V не удается завершить работу виртуальной машины Redos, так как служба интеграции по завершению работы недоступна.
	Во избежание потенциальной потери данных вы можете приостановить виртуальную машину или сохранить ее состояние. Другим
	вариантом является отключение виртуальной машины, но при этом возможна потеря данных.
	[Y] Да - Y  [N] Нет - N  [S] Приостановить - S  [?] Справка (значением по умолчанию является "Y"): y
	PS C:\Windows\system32> Get-VM
	Name State CPUUsage(%) MemoryAssigned(M) Uptime   Status
	---- ----- ----------- ----------------- ------   ------
	Alt  Off   0           0                 00:00:00 Работает нормально
	Redos Off   0           0                 00:00:00 Работает нормально
	Task:
	Установить и настроить дистрибутив Redos в виртуальной машине
	Decision:
	выбираем Install - выбрать размер разделов - поменяем систему разметки LVM на Standart Portition - нажимаем на click here to create them autmaticaly - установщик автоматически разделит диск - уменьшить размер корневого раздела / и раздел /home - Меняю у них размеры - Done - интернет настроить в Network & Host Name - Ethernet переключаем тумблер - Done - software Slection - minmal install- Done - запуск
	Task:
	# dnf -y install kernel-devel-$(uname -r)
		Last metadata expiration check: 0:52:33 ago on Thu 14 May 2020 03:36:35 AM EDT.
		No match for argument: kernel-devel-4.18.0-147.el8.x86_64
		Error: Unable to find a match: kernel-devel-4.18.0-147.el8.x86_64
	Decision:
	# dnf -y install kernel-devel
	Task:  
	Ввод компьютера в домен Windows и изменение имени хоста
	Decision:
	# yum install join-to-domain
	# join-to-domain.sh
	# reboot
	# hostname
		thost1.tdomain.ru
	# hostname thost2.tdomain.ru
	# hostname
		thost2.tdomain.ru
	# vim /etc/hostname
	# cat /etc/hostname
		thost2.tdomain.ru
	# vim /etc/hosts
	# cat /etc/hosts
		127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4
		::1 localhost localhost.localdomain localhost6 localhost6.localdomain6
		127.0.0.1 thost2.tdomain.ru thost2
	Task:
	Установка Консультант Плюс и подключение сетевых директорий с использованием automount и механизма Kerberos
	Decision:
	$ yum install wine winetricks
	# winetricks riched30 winhttp
	Wine -> Wine Configuration -> Графика -> уберите галочку в пункте "Разрешить менеджеру окон декорировать окна". -> Для запуска «Консультант Плюс» на рабочей станции подключите сетевой диск с «Консультантом» -> Сделать это можно с помощью подключения сетевых директорий с использованием automount и механизма Kerberos
	$ smbclient -L thost -k
	Sharename Type Comment
	--------- ---- -------
	Consultant Disk
	ConsultantR Disk
	...
	S Disk
	Reconnecting with SMB1 for workgroup listing.
	Server Comment
	-------- -------
	Wg M
	--------- -------
	# yum install cifs-utils autofs
	$ klist
	Ticket cache: FILE:/tmp/krb5cc_1
	Default principal: tuser@tdomain.RU
	Valid starting Expires service principal
	31.08.2020 09:51:47 31.08.2020 19:51:47 krbtgt/tdomain.RU@tdomain.RU
	renew until 31.08.2020 19:51:47
	# vim /etc/auto.master
	# cat /etc/auto.master
	...
	/mnt/.thost    /etc/auto.samba    --ghost
	# vim /etc/auto.samba
	# cat /etc/auto.samba
	Consultant    -fstype=cifs,multiuser,cruid=tuser,sec=krb5,domain=tdomain.RU,vers=2.1     ://thost/Consultant
	Students      -fstype=cifs,multiuser,cruid=tuser,sec=krb5,domain=tdomain.RU,vers=2.1       ://thost/Students
	Для подключения скрытых сетевых каталогов необходимо экранировать символ $
	Consultant    -fstype=cifs,multiuser,cruid=tuser,sec=krb5,domain=tdomain.RU,vers=2.1     ://thost/Consultant\$
	В файле /etc/krb5.conf нужно закомментировать строку (если она ещё не закомментирована)
	#cat /etc/krb5.conf
	...
	default_ccache_name = KEYRING:persistent:%{uid}
	...
	#vim/etc/krb5.conf
	#cat /etc/krb5.conf
	...
	default_ccache_name = FILE:/tmp/krb5cc_%{uid}
	...
	# systemctl start autofs.service
	# systemctl enable autofs --now
	Created symlink from /etc/systemd/system/multi-user.target.wants/autofs.service to /usr/lib/systemd/system/autofs.service.
	# ls /mnt/.thost/Consultant/
	... cons.exe ...
	$ winecfg
	# wine /mnt/.thost/Сonsultant/cons.exe /linux /yes
	Task:
	Создать ярлык Консультант +
	Decision:
	# vim /home/tuser@thost.ru/Рабочий\ стол/Consultant.desktop
	# cat /home/tuser@thost.ru/Рабочий\ стол/Consultant.desktop
	[Desktop Entry]
	Name=ConsultantPlus
	Exec=wine /mnt/.thost/Сonsultant/cons.exe
	Type=Application
	StartupNotify=true
	Comment=ConsultantPlus
	icon=43D4_Cons.0
	StartupWMClass=c.exe
	Task:
	В случае замедленной работы можно добавить ключ /sprocess=0 При нормальной работе, не добавляйте этот ключ. Ключ /yes необходим для подавления сообщения об ошибке [WNetGetUniversalName ...] : NO_NETWORK
	Decision:
	# ln -s /mnt/.thost/S /home/tuser@thost.ru/Рабочий\ стол/S
	Task:
	Создание ярлыка для сетевой директории
	Decision:
	# ln -s /mnt/.thost/S /home/tuser@thost.ru/Рабочий\ стол/S
	Task:
	После добавления сетевых папок через Kerberos на следующий месяц, все етевые папки на ПК перестали работать. Для решения проблемы нужно обновлять билеты через kdestroy и kinit:
	Decision:
	$ smbclient -L thost -k
	gse_get_client_auth_token: gss_init_sec_context failed with [ Miscellaneous failure (see text): encryption type 0 not supported](2529639062)
	gensec_spnego_client_negTokenInit_step: gse_krb5: creating NEG_TOKEN_INIT for cifs/tusertsrvdoc failed (next[(null)]): NT_STATUS_LOGON_FAILURE
	session setup failed: NT_STATUS_LOGON_FAILURE
	$ klist
	Ticket cache: FILE:/tmp/krb5cc_17
	Default principal: tuser@tdomain.RU
	Valid starting Expires Service principal
	01.01.1970 08:00:00 01.01.1970 08:00:00 krbtgt/tdomain.RU@tdomain.RU
	$ date
	Вт окт 6 14:46:12 +08 2020
	$ kdestroy
	$ kinit
	$ klist
	Ticket cache: FILE:/tmp/krb5cc_17
	Default principal: tuser@tdomain.RU
	Valid starting Expires Service principal
	06.10.2020 14:46:51 07.10.2020 00:46:51 krbtgt/tdomain.RU@tdomain.RU
	renew until 13.10.2020 14:46:29
	Decision:
	Проблема решена, но билет кеширования, нужно было обновлять постоянно. Поэтому, на мой взгляд, проще подключать сетевые папки через automount без Kerberos.
	Task:
	Oграничение доступа к USB накопителям
	Decision:
	# vim /etc/udev/rules.d/99-usb.rules
	# cat /etc/udev/rules.d/99-usb.rules
	ENV{ID_USB_DRIVER}=="usb-storage",ENV{UDISKS_IGNORE}="1"
	# udevadm control --reload-rules
	Task:
	Запрет создания ярлыков и файлов на рабочем столе
	Decision:
	Проверить, поддерживает ли ваша файловая система ACL можно командой (вместо /dev/sda1 указать нужное имя дискового раздела):
	# tune2fs -l /dev/sda1 | grep "Default mount options:"
	Default mount options: user_xattr acl
	# vim /home/tuser@thost.ru/.config/user-dirs.dirs
	# cat /home/tuser@thost.ru/.config/user-dirs.dirs
	# This file is written by xdg-user-dirs-update
	# If you want to change or add directories, just edit the line you're
	# interested in. All local changes will be retained on the next run
	# Format is XDG_xxx_DIR="$HOME/yyy", where yyy is a shell-escaped
	# homedir-relative path, or XDG_xxx_DIR="/yyy", where /yyy is an
	# absolute path. No other format is supported.
	#
	XDG_DESKTOP_DIR="$HOME/Рабочий стол"
	XDG_DOWNLOAD_DIR="$HOME/Загрузки"
	XDG_TEMPLATES_DIR="$HOME/Шаблоны"
	XDG_PUBLICSHARE_DIR="$HOME/Общедоступные"
	XDG_DOCUMENTS_DIR="$HOME/Документы"
	XDG_MUSIC_DIR="$HOME/Музыка"
	XDG_PICTURES_DIR="$HOME/Изображения"
	XDG_VIDEOS_DIR="$HOME/Видео"
	# ls -la /home/tuser@thost.ru/.config
	...
	-rw-------. 1 tuser ïîëüçîâàòåëè äîìåíà 714 àâã 31 09:52 user-dirs.dirs
	-rw-r--r--. 1 tuser ïîëüçîâàòåëè äîìåíà 5 àâã 31 09:52 user-dirs.locale
	# chown root:root /home/tuser@thost.ru/.config/user-dirs.dirs
	# ls -la /home/tuser@thost.ru/.config
	...
	-rw-------. 1 root root 714 àâã 31 09:52 user-dirs.dirs
	-rw-r--r--. 1 tuser ïîëüçîâàòåëè äîìåíà 5 àâã 31 09:52 user-dirs.locale
	# ls -la
	...
	drwxr-xr-x. 2 tuser ïîëüçîâàòåëè äîìåíà 4096 àâã 31 09:52 'Рабочий стол'
	...
	# chown -R root:root /home/tuser@thost.ru/Рабочий\ стол/
	# ls -la
	...
	drwxr-xr-x. 2 root root 4096 àâã 31 09:52 'Рабочий стол'
	drwxr-xr-x. 2 tuser ïîëüçîâàòåëè äîìåíà 4096 àâã 31 09:52 Øàáëîíû
	Task:
	Как скрыть пользователей от экрана входа
	Decision:
	# cat /var/lib/AccountsService/users/user
	[User]
	Language=
	XSession=
	SystemAccount=false
	# vim /var/lib/AccountsService/users/user
	# cat /var/lib/AccountsService/users/user
	[User]
	Language=
	XSession=gnome
	SystemAccount=true
	Task:
	Добавление пользователя из доменной сети
	Decision:
	[root@thost user]# vim /var/lib/AccountsService/users/tuser
	[root@thost user]# cat /var/lib/AccountsService/users/tuser
	[User]
	Language=
	XSession=
	Icon=/home/tuser@thost.ru/.face
	SystemAccount=false
	Task:
	отключить сетевые принтеры
	Decision:
	# vim /etc/avahi/avahi-daemon.conf
	cat /etc/avahi/avahi-daemon.conf
	# This file is part of avahi.
	...
	enable-dbus=no
	...
	# reboot
	Task:
	Установка Gimp
	Decision:
	yum provides gimp
	Загружены модули: fastestmirror, langpacks
	Loading mirror speeds from cached hostfile
	2:gimp-2.8.22-2.el7.thost86 : GNU Image Manipulation Program
	Источник: base
	2:gimp-2.8.22-2.el7.x86_64 : GNU Image Manipulation Program
	Источник: base
	# yum list | grep gimp
	gimp.thost86 2:2.8.22-2.el7 base
	# yum -y install gimp
	Task:
	Запись дисков Linux в терминале
	Decision:
	# ls
	13 'MyTestX Tests'
	# pwd
	/home/is@tdomain.RU/Documents
	# mkisofs -o first.iso /home/is@tdomain.RU/Documents/13
	# ls
	13 first.iso 'MyTestX Tests'
	# mkisofs -l -o first.iso /home/is@tdomain.RU/Documents/13
	# ls
	13 first.iso 'MyTestX Tests'
	# mkisofs -D -l -o first.iso /home/is@tdomain.RU/Documents/13
	# mkisofs -J -l -o first.iso /home/is@tdomain.RU/Documents/13
	Warning: creating filesystem with Joliet extensions but without Rock Ridge
	extensions. It is highly recommended to add Rock Ridge.
	...
	Joliet tree sort failed. The -joliet-long switch may help you.
	# -joliet-long
	# mkisofs -J -joliet-long -l -o first.iso /home/is@tdomain.RU/Documents/13
	Warning: creating filesystem with Joliet extensions but without Rock Ridge
	extensions. It is highly recommended to add Rock Ridge.
	...
	99.90% done, estimate finish Fri Jan 15 11:29:00 2021
	Total translation table size: 0
	Total rockridge attributes bytes: 0
	Total directory bytes: 0
	Path table size(bytes): 10
	Max brk space used 0
	545548 extents written (1065 MB# lsblk -d -o +MODEL
	NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT MODEL
	sda 8:0 0 111,8G 0 disk KINGSTON SA400S3
	sr0 11:0 1 2K 0 rom CDDVDW SN-208BB
	Если вы собираетесь записать образ на диск ubuntu DVD-RW/CD-RW необходимо сначала его очистить:
	cdrecord -dev=/dev/sr0 -v blank=fast
	# cd Documents/
	# ls
	13 first.iso 'MyTestX Tests'
	Здесь и ниже /dev/sr0 - адрес файла вашего привода:
	# cdrecord -dev=/dev/sr0 -speed=16 -eject -v first.iso
	first.iso - это файл образа, 16 - скорость записи, а опция -eject - заставляет извлечь диск из привода после записи. Можно указывать скорость 4, 8 и 16. Желательно выбирать минимальную скорость, так диск запишется более качественно.
	Task:
	Установка клиента 1С
	Decision:
	# yum -y install webkitgtk3
	# ls
	1C_Enterprise83-client-8.3.14-17.x86_64.rpm
	1C_Enterprise83-common-8.3.14-17.x86_64.rpm
	1C_Enterprise83-server-8.3.14-17.x86_64.rpm
	# yum -y install 1C_Enterprise83-*
	Task:
	Установка аналога paint
	Decision:
	# yum list | grep paint
	kolourpaint.thost86 17.12.1-2.el7 base
	kolourpaint.x86_64 17.12.1-2.el7 base
	kolourpaint-libs.thost86 17.12.1-2.el7 base
	kolourpaint-libs.x86_64 17.12.1-2.el7 base
	# yum -y install kolourpaint
	Task:
	Настройка оповещения и автоматического обновления пакетов в РЕД ОС с помощью yum-cron
	Decision:
	# yum -y install yum-cron
	# vim /etc/yum/yum-cron.conf
	# cat /etc/yum/yum-cron.conf
	[commands]
	# What kind of update to use:
	# default = yum upgrade
	# security = yum --security upgrade
	# security-severity:Critical = yum --sec-severity=Critical upgrade
	# minimal = yum --bugfix update-minimal
	# minimal-security = yum --security update-minimal
	# minimal-security-severity:Critical = --sec-severity=Critical update-minimal
	update_cmd = default
	# Whether a message should be emitted when updates are available,
	# were downloaded, or applied.
	update_messages = yes
	# Whether updates should be downloaded when they are available.
	download_updates = yes
	# Whether updates should be applied when they are available. Note
	# that download_updates must also be yes for the update to be applied.
	apply_updates = no
	# Maximum amout of time to randomly sleep, in minutes. The program
	# will sleep for a random amount of time between 0 and random_sleep
	# minutes before running. This is useful for e.g. staggering the
	# times that multiple systems will access update servers. If
	# random_sleep is 0 or negative, the program will run immediately.
	# 6*60 = 360
	random_sleep = 360
	...
	# vim /etc/yum/yum-cron.conf
	# cat /etc/yum/yum-cron.conf
	[commands]
	# What kind of update to use:
	# default = yum upgrade
	# security = yum --security upgrade
	# security-severity:Critical = yum --sec-severity=Critical upgrade
	# minimal = yum --bugfix update-minimal
	# minimal-security = yum --security update-minimal
	# minimal-security-severity:Critical = --sec-severity=Critical update-minimal
	update_cmd = default
	# Whether a message should be emitted when updates are available,
	# were downloaded, or applied.
	update_messages = yes
	# Whether updates should be downloaded when they are available.
	download_updates = yes
	# Whether updates should be applied when they are available. Note
	# that download_updates must also be yes for the update to be applied.
	apply_updates = yes
	# Maximum amout of time to randomly sleep, in minutes. The program
	# will sleep for a random amount of time between 0 and random_sleep
	# minutes before running. This is useful for e.g. staggering the
	# times that multiple systems will access update servers. If
	# random_sleep is 0 or negative, the program will run immediately.
	# 6*60 = 360
	random_sleep = 360
	...
	# systemctl enable yum-cron --now
	Task:  
	Если не требуется обновлять определенные пакеты (как вручную, так и автоматически), то добавляем их в исключение. Например, kernel и php.
	Decision:
	#nano /etc/yum.conf
	exclude=kernel, php
	Task:  
	Если не требуется обновлять пакеты ТОЛЬКО в автоматическом режиме, тогда в /etc/yum/yum-cron.conf , в раздел [base], добавляем следующую строку:
	Decision
	# nano /etc/yum/yum-cron.conf
	exclude=kernel* php*
	Task:
	Установка Anydesk
	Decision:
	# tee /etc/yum.repos.d/AnyDesk-RHEL.repo <<EOF
	> [anydesk]
	> name=AnyDesk RHEL - stable
	> baseurl=http://rpm.anydesk.com/rhel/\$basearch/
	> gpgcheck=1
	> repo_gpgcheck=1
	> gpgkey=https://keys.anydesk.com/repos/RPM-GPG-KEY
	> EOF
	# dnf makecache
	# dnf install -y redhat-lsb-core
	# dnf install anydesk
	# rpm -qi anydesk
	# systemctl status anydesk.service
	# exit
	$ anydesk
	Task:
	Создание загрузочных носителей
	Decision:
	# fdisk -l
	...
	Диск /dev/sdb: 3,61 GiB, 3880452096 байт, 7579008 секторов
	Disk model: Silicon-Power4G
	Единицы: секторов по 1 * 512 = 512 байт
	Размер сектора (логический/физический): 512 байт / 512 байт
	Размер I/O (минимальный/оптимальный): 512 байт / 512 байт
	Тип метки диска: gpt
	Идентификатор диска: 2BAC44AA-F36D-461C-B12A-D251E7E9373F
	Устр-во    начало   Конец Секторы Размер Тип
	/dev/sdb1    2048 7578974 7576927   3,6G Microsoft basic data
	# ls
	Zorin-OS-16.1-Core-64-bit.iso
	# dd if=Zorin-OS-16.1-Core-64-bit.iso of=/dev/sdb1 bs=8MB status=progress oflag=direct
	3058237440 байт (3,1 GB, 2,8 GiB) скопирован, 659 s, 4,6 MB/s
	382+1 записей получено
	382+1 записей отправлено
	3058237440 байт (3,1 GB, 2,8 GiB) скопирован, 659,39 s, 4,6 MB/s
	Task:
	Развернуть Pxe сервер для развертывания Redos с загрузкой в Uefi по сети
	Decision:
	# ifconfig
	enp2s0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
		inet IpAddr1  netmask Mask1  broadcast IpAddr.255
		...
	# dnf install dhcp tftp-server syslinux httpd dnf-plugins-core -y
	# mkdir /var/lib/tftpboot/pxelinux.cfg
	# mkdir /var/lib/tftpboot/uefi
	# mkdir -p /var/lib/tftpboot/images/REDOS
	# cp /usr/share/syslinux/{chain.c32,mboot.c32,memdisk,menu.c32,pxelinux.0,ldlinux.c32,libutil.c32} /var/lib/tftpboot/
	# chmod 777 /var/lib/tftpboot/pxelinux.0
	# dnf download shim-x64 grub2-efi-x64 --downloaddir=/root/
	# cd /root/
	# rpm2cpio shim-x64-*.rpm | cpio -dimv
	# rpm2cpio grub2-efi-x64-*.rpm | cpio -dimv
	# cp ./boot/efi/EFI/BOOT/BOOTX64.EFI /var/lib/tftpboot/uefi
	# cp ./boot/efi/EFI/redos/grubx64.efi /var/lib/tftpboot/uefi
	# chmod 777 /var/lib/tftpboot/uefi/*.*
	# mount -t iso9660 -o loop redos-MUROM-7.3.2-20211027.0-Everything-x86_64-DVD1.iso /mnt/
	# cp -vR /mnt/* /var/lib/tftpboot/images/REDOS/
	# umount /mnt/
	# vim /etc/dhcp/dhcpd.conf
	# cat /etc/dhcp/dhcpd.conf
	non-authoritative;
	allow bootp;
	option space pxelinux;
	option pxelinux.magic code 208 = string;
	option pxelinux.configfile code 209 = text;
	option pxelinux.pathprefix code 210 = text;
	option pxelinux.reboottime code 211 = unsigned integer 32;
	option architecture-type code 93 = unsigned integer 16;
	subnet IpAddr.0 netmask Mask1 {
	option routers IpAddr1;
	range IpAddr.70 IpAddr.80;
	class "pxeclients" {
	match if substring (option vendor-class-identifier, 0, 9) = "PXEClient";
	next-server IpAddr1;
	if option architecture-type = 00:07 {
	filename "uefi/grubx64.efi";
	} 
	else {
	filename "pxelinux.0";
	}
	}
	} 
	# systemctl enable dhcpd --now
	# vim /var/lib/tftpboot/pxelinux.cfg/default
	# cat /var/lib/tftpboot/pxelinux.cfg/default
	default menu.c32
	PROMPT 0
	TIMEOUT 150
	MENU TITLE PXE Menu
	LABEL REDOS 7.3
	MENU LABEL REDOS 7.3
	KERNEL images/REDOS/images/pxeboot/vmlinuz
	APPEND initrd=images/REDOS/images/pxeboot/initrd.img ramdisk_size=128000 ip=dhcp inst.repo=http://IpAddr1/images/REDOS/ 
	# vim /var/lib/tftpboot/uefi/grub.cfg 
	# cat /var/lib/tftpboot/uefi/grub.cfg 
	function load_video { 
	insmod efi_gop  
	insmod efi_uga 
	insmod video_bochs 
	insmod video_cirrus 
	insmod all_video 
	} 
	load_video 
	set gfxpayload=keep 
	insmod gzio 
	menuentry 'REDOS 7.3' { 
	linux images/REDOS/images/pxeboot/vmlinuz ip=dhcp inst.repo=http://IpAddr1/images/REDOS/ 
	initrd images/REDOS/images/pxeboot/initrd.img 
	} 
	Task:
	Используем web-сервер для публикации файлов дистрибутива РЕД ОС в локальной сети.
	Decision:
	# vim /etc/httpd/conf.d/pxeboot.conf 
	# cat /etc/httpd/conf.d/pxeboot.conf 
	Alias /images /var/lib/tftpboot/images 
	<Directory /var/lib/tftpboot/images> 
	Options Indexes FollowSymLinks 
	Require ip 127.0.0.1 IpAddr.0/24 
	</Directory>
	# systemctl enable httpd --now
	Task:
	Настройка и запуск службы tftp.
	Decision:
	# vim /usr/lib/systemd/system/tftp.service
	# cat /usr/lib/systemd/system/tftp.service
	...
	[Install]:
	WantedBy=multi-user.target 
	Also=tftp.socket
	...
	# vim /usr/lib/systemd/system/tftp.socket
	# cat /usr/lib/systemd/system/tftp.socket
	...
	[Unit] 
	Description=Tftp Server Activation Socket 
	[Socket] 
	ListenDatagram=0.0.0.0:69 
	[Install] 
	WantedBy=sockets.target 
	...
	# systemctl daemon-reload
	# systemctl enable tftp --now
	# ausearch -c 'httpd' --raw | audit2allow -M my-httpd
	# semodule -i my-httpd.pp
	Task:
	Автоматизация развертывания (kickstart)
	Создать файл кикстарта, Записать файл на локальный или удаленный носитель, Создать загрузочный диск, с которого будет запускаться установка, Предоставить доступ к установочной структуре, Начать процесс установки.
	Decision:
	# dnf install pykickstart -y
	# cp /root/anaconda-ks.cfg /var/lib/tftpboot
	# mv /var/lib/tftpboot/anaconda-ks.cfg /var/lib/tftpboot/ks.cfg
	# vim /var/lib/tftpboot/pxelinux.cfg/default
	# cat /var/lib/tftpboot/pxelinux.cfg/default
	...
	APPEND initrd=images/REDOS/images/pxeboot/initrd.img ramdisk_size=128000 ip=dhcp method=http://IpAddr1/images/REDOS/ devfs=nomount inst.ks=http://IpAddr1/ks.cfg 
	# vim /var/lib/tftpboot/uefi/grub.cfg
	# cat /var/lib/tftpboot/uefi/grub.cfg
	... { 
	linux images/REDOS/images/pxeboot/vmlinuz ip=dhcp kernel vmlinuz inst.repo=http://IpAddr1/images/REDOS/ inst.ks=http://IpAddr1/ks.cfg 
	initrd images/REDOS/images/pxeboot/initrd.img
	} 
	# vim /var/lib/tftpboot/ks.cfg
	# cat /var/lib/tftpboot/ks.cfg
	# Здесь указываем раскладку клавиатуры
	keyboard --vckeymap=us --xlayouts='us','ru' --switch='grp:alt_shift_toggle'
	# Системная локаль
	lang ru_RU.UTF-8
	# Информация о сетевом интерфейсе и имя машины
	network  --bootproto=dhcp --device=enp2s0 --noipv6 --activate
	network  --hostname=hostname1337
	# Пароль Root представлен в виде хэш-суммы
	rootpw --iscrypted $6$DUu0yyOYMRbGS8gL$9zHYPsxROGEZdDKG0wnf7h8SGnKOp3V272De6oGTVUsz2uBLmEeiR6T6cInRN5dyWcxNXh5fVluEUTQ/3rmzB0
	# Настройка сервисов (в данном случае сервис по обновлению меток времени и дат)
	services --enabled="chronyd"
	# Настройка временной зоны
	timezone Europe/Moscow --isUtc
	#Настройка локального пользователя
	user --groups=wheel --name=mekka --password=$6$83fyYZ7KMS7G9t6A$E5/99/ffOwjUOo8THr1ngqGDdKMimpTZf3IT9S/SI98BTV7dta7GksLYnQEZjtqqyZQrwibSRlvYccRqHB7m8/ --iscrypted --gecos="Mekka"
	# Настройка xorg при загрузке
	xconfig  --startxonboot
	# Указание загрузочного сектора и тип структуры
	bootloader --location=mbr --boot-drive=sda
	# Удаление всей информации с партиций для последующей установки
	clearpart --none --initlabel
	# Здесь указана вся разметка диска
	part /boot --fstype="xfs" --onpart=sda2
	part biosboot --fstype="biosboot" --noformat --onpart=sda4
	part pv.31 --fstype="lvmpv" --noformat --onpart=sda3
	part /boot/efi --fstype="efi" --onpart=sda1 --fsoptions="umask=0077,shortname=winnt"
	volgroup ro --noformat --useexisting
	logvol swap  --fstype="swap" --useexisting --name=swap --vgname=ro
	logvol /home  --fstype="xfs" --noformat --useexisting --name=home --vgname=ro
	logvol /  --fstype="ext4" --useexisting --name=root --vgname=ro
	#дополнительные пакеты для установки
	%packages
	@^mate-desktop-environment
	@backup-client
	@base
	@branding
	@core
	@desktop-debugging
	@dial-up
	@directory-client
	@fonts
	@guest-agents
	@guest-desktop-agents
	@input-methods
	@internet-applications
	@internet-browser
	@java-platform
	@mate-desktop
	@multimedia
	@network-file-system-client
	@print-client
	@x11
	chrony
	%end
	#настройка аварийных дампов памяти в случае сбоев (оставить как есть)
	%addon com_redhat_kdump --enable --reserve-mb='auto'
	%end
	#настройка анаконды (оставить как есть)
	%anaconda
	pwpolicy root --minlen=6 --minquality=1 --notstrict --nochanges --notempty
	pwpolicy user --minlen=6 --minquality=1 --notstrict --nochanges --emptyok
	pwpolicy luks --minlen=6 --minquality=1 --notstrict --nochanges --notempty
	%end
	Task:
	Настройка установки РЕД ОС по PXE через VNC вместо Kickstart
	Decision:
	# vim /var/lib/tftpboot/pxelinux.cfg/default
	# cat /var/lib/tftpboot/pxelinux.cfg/default
	...
	APPEND initrd=images/REDOS/images/pxeboot/initrd.img ramdisk_size=128000 ip=dhcp method=http://IpAddr1/images/REDOS/ devfs=nomount inst.vnc inst.vncpassword=tpassword
	# vim /var/lib/tftpboot/uefi/grub.cfg
	# cat /var/lib/tftpboot/uefi/grub.cfg
	... { 
	linux images/REDOS/images/pxeboot/vmlinuz ip=dhcp kernel vmlinuz inst.repo=http://IpAddr1/images/REDOS/ inst.vnc inst.vncpassword=tpassword
	initrd images/REDOS/images/pxeboot/initrd.img
	}
	Task:
	После того как в виртуальной машине Centos настроил Docker контейнер и презагрузил сам физический сервер с Centos, произошла бесконечная загрузка и как когда стрелками нажимал, появилась следующая ошибка в момент загрузки сервера:
	Dependency failed for /var
	Dependency failed for D-Bus System Message Bus
	staring file system check on dev/disk/by-uuid/UUID1
	failed to start file system check on dev/disk/UUID1
	See 'systemctl status "systemd-fsck@dev-disk-by\\xzduuid-UUID1"'
	Decision:
	Для решения данной проблемы нужнозагрузиться с LiveCd (Ubuntu) и запустить Fsck для проверки и восстановления файловой системы
	$ ls /dev/disk/by-uuid/UUID1
	/dev/disk/by-uuid/UUID1
	$ systemctl status "systemd-fsck@dev-disk-by\\UUID1.service" > test.txt
	$ cat test.txt 
	○ systemd-fsck@dev-disk-by\xZduuid-8f07e18e\xZd9d1a\xZd4dab\xZd8904\xZda86e0c2058b4.service
		Loaded: bad-setting (Reason: Unit systemd-fsck@dev-disk-by\xZduuid-8f07e18e\xZd9d1a\xZd4dab\xZd8904\xZda86e0c2058b4.service has a bad unit file setting.)
		Active: inactive (dead)
		Docs: man:systemd-fsck@.service(8)
	Nov 15 12:50:19 ubuntu systemd[1]: systemd-fsck@dev-disk-by\xZduuid-8f07e18e\xZd9d1a\xZd4dab\xZd8904\xZda86e0c2058b4.service: Unit configuration has fatal error, unit will not be started.
	Nov 15 12:52:01 ubuntu systemd[1]: /lib/systemd/system/systemd-fsck@.service:11: Failed to resolve unit specifiers in 'File System Check on %f', ignoring: Invalid argument
	Nov 15 12:52:01 ubuntu systemd[1]: /lib/systemd/system/systemd-fsck@.service:23: Failed to resolve unit specifiers in %f: Invalid argument
	Nov 15 12:52:01 ubuntu systemd[1]: systemd-fsck@dev-disk-by\xZduuid-8f07e18e\xZd9d1a\xZd4dab\xZd8904\xZda86e0c2058b4.service: Unit configuration has fatal error, unit will not be started.
	Nov 15 12:52:30 ubuntu systemd[1]: /lib/systemd/system/systemd-fsck@.service:11: Failed to resolve unit specifiers in 'File System Check on %f', ignoring: Invalid argument
	Nov 15 12:52:30 ubuntu systemd[1]: /lib/systemd/system/systemd-fsck@.service:23: Failed to resolve unit specifiers in %f: Invalid argument
	Nov 15 12:52:30 ubuntu systemd[1]: systemd-fsck@dev-disk-by\xZduuid-8f07e18e\xZd9d1a\xZd4dab\xZd8904\xZda86e0c2058b4.service: Unit configuration has fatal error, unit will not be started.
	Nov 15 12:53:50 ubuntu systemd[1]: /lib/systemd/system/systemd-fsck@.service:11: Failed to resolve unit specifiers in 'File System Check on %f', ignoring: Invalid argument
	Nov 15 12:53:50 ubuntu systemd[1]: /lib/systemd/system/systemd-fsck@.service:23: Failed to resolve unit specifiers in %f: Invalid argument
	Nov 15 12:53:50 ubuntu systemd[1]: systemd-fsck@dev-disk-by\xZduuid-8f07e18e\xZd9d1a\xZd4dab\xZd8904\xZda86e0c2058b4.service: Unit configuration has fatal error, unit will not be started.
	$ df -h
	...
	/dev/sdb1            916G  339G  531G  39% /media/ubuntu/UUID1
	...
	$ sudo parted /dev/sdb1 'print'
	Model: Unknown (unknown)
	Disk /dev/sdb1: 1000GB
	Sector size (logical/physical): 512B/4096B
	Partition Table: loop
	Disk Flags: 
	Number  Start  End     Size    File system  Flags
	1      0.00B  1000GB  1000GB  ext4
	$ sudo umount /dev/sdb1
	$ sudo fsck /dev/sdb1
	fsck from util-linux 2.37.2
	e2fsck 1.46.5 (30-Dec-2021)
	/dev/sdb1 contains a file system with errors, check forced.
	Pass 1: Checking inodes, blocks, and sizes
	Inode 40895771 extent tree (at level 2) could be narrower.  Optimize<y>? yes
	...
	Inode 41156677 extent tree (at level 2) could be narrower.  Optimize<y>? yes
	Inodes that were part of a corrupted orphan linked list found.  Fix<y>? yes
	Inode 56623165 was part of the orphaned inode list.  FIXED.
	...
	Inode 56623196 was part of the orphaned inode list.  FIXED.
	Pass 1E: Optimizing extent trees
	Pass 2: Checking directory structure
	Pass 3: Checking directory connectivity
	Pass 4: Checking reference counts
	Pass 5: Checking group summary information
	Block bitmap differences:  -(4172736--4172783) -(7917568--7917588) -(22880252--22881251) -(22910443--22912073) -(164069760--164069882) -(164070454--164070541)
	Fix<y>? yes
	Free blocks count wrong for group #127 (2152, counted=2200).
	...
	Fix<y>? yes
	Free inodes count wrong (61050243, counted=61050252).
	Fix<y>? yes
	/dev/sdb1: ***** FILE SYSTEM WAS MODIFIED *****
	/dev/sdb1: 4724/61054976 files (4.8% non-contiguous), 92947008/244190390 blocks
	$ sudo fsck -N /dev/sdb1
	fsck from util-linux 2.37.2
	[/usr/sbin/fsck.ext4 (1) -- /dev/sdb1] fsck.ext4 /dev/sdb1 
	$ sudo fsck -y /dev/sdb1
	fsck from util-linux 2.37.2
	e2fsck 1.46.5 (30-Dec-2021)
	/dev/sdb1: clean, 4724/61054976 files, 92947008/244190390 blocks
	$ sudo fsck -f /dev/sdb1
	fsck from util-linux 2.37.2
	e2fsck 1.46.5 (30-Dec-2021)
	Pass 1: Checking inodes, blocks, and sizes
	Pass 2: Checking directory structure
	Pass 3: Checking directory connectivity
	Pass 4: Checking reference counts
	Pass 5: Checking group summary information
	/dev/sdb1: 4724/61054976 files (4.8% non-contiguous), 92947008/244190390 blocks
	$ sudo reboot
	Source:
	https://redos.red-soft.ru/base/other-soft/other-other/consultant/?sphrase_id=53349
	https://wtuseri.astralinuthost.ru/pages/viewpage.action?pageId=61574227
	https://askubuntu.ru/questions/21203/kak-skry-t-pol-zovatelej-ot-e-krana-vxoda-v-gdm
	https://computingforgeeks.com/how-to-install-gimp-on-centos-rhel-8-desktop/
	https://ru.wtuserihow.com/%D1%81%D0%BE%D0%B7%D0%B4%D0%B0%D1%82%D1%8C-ISO-%D1%84%D0%B0%D0%B9%D0%BB-%D0%B2-Linux
	https://losst.ru/zapis-diskov-v-ubuntu
	https://losst.ru/luchshie-analogi-paint-dlya-linux
	https://computingforgeeks.com/how-to-install-anydesk-on-centos-rhel-8/
	https://wiki.merionet.ru/articles/rukovodstvo-po-komande-fsck-dlya-proverki-i-vosstanovleniya-fajlovoj-sistemy/

Разработка программ, которые анализируют, обрабатывают и сортируют код на сайте организации
	Task:
	Разработал программы, которые анализируют, обрабатывают и сортируют код на сайте организации. Это позволило мне ускорить процесс корректировки тегов на сайте по запросу Россобрнадзор
	Task:
	Рроверить все страницы на сайте, на отсутствие pdf файлов больше 15 Мб.
	Decision:
	$ wget https://tsite.ru/sveden/document
	--2018-11-01 20:12:20--  https://tsite.ru/sveden/document
	Распознаётся tsite.ru (tsite.ru)… IpAddr
	Подключение к tsite.ru (tsite.ru)|IpAddr|:443... соединение установлено.
	ОШИБКА: Нет доверия сертификату для «tsite.ru».
	ОШИБКА: Неизвестный издатель сертификата «tsite.ru».
	$ wget --no-check-certificate -r -l 1 -A pdf https://tsite.ru/sveden/document
	$ vim bash-FileWeight.sh
	$ chmod +x bash-FileWeight.sh
	$ cat bash-FileWeight.sh
	#!/bin/bash
	echo -n "Enter a link to the site: "
	read linksite
	user=$(whoami)
	echo -n "Come up with a name for the directory where the files will be written: "
	read linkfiles
	if [ -d /home/$user/$linkfiles ]; then
			rm -rf /home/$user/$linkfiles
			echo "/home/$user/$filename delete"
	else
			mkdir /home/$user/$linkfiles
			cd /home/$user/$linkfiles
			echo "/home/$user/$linkfiles create"
	fi
	wget --no-check-certificate -r -l 1 -A pdf $linksite
	find /home/$user/$linkfiles -size +15M > output
	$ ./bash-FileWeight.sh
	Enter a link to the site: https://tsite.ru/sveden/document
	Come up with a name for the directory where the files will be written: tdir
	/home/tUser/tdir create
	...
	$ cat /home/tUser/tdir/output
	/home/tUser/tdir/tsite.ru/Media/irk/Документы института/2018/tDoc1.pdf
	$ ls -l /home/tUser/tdir/tsite.ru/Media/irk/Документы\ института/2018/tDoc1.pdf 
	-rw-r--r--. 1 tUser tUser 20055320 июн 30  2018 '/home/tUser/tdir/tsite.ru/Media/irk/Документы института/2018/tDoc1.pdf'
	Task:
	Отсортировать html-код страницы по атрибутам <tr><td>
	Decision:

	Task:
	Поменять/добавить тег на странице с таблицей,в некоторых строках тег отсутствует
	Decision:
	
	Source:
	https://stackoverflow.com/questions/4846007/check-if-directory-exists-and-delete-in-one-command-unix

Утилита для сканирования безопасности сети Nmap 
	  Task:
    Для защиты выпускной квалификационной работы по теме "Утилита для сканирования безопасности сети Nmap" проанализировал состояние виртуальных машин в Qemu-Kvm инструментом Nmap. Дополнительно для перехвата трафиков использовал Tcpdump. Для того чтобы обезопасить свои системы, настроил правила в Iptables и развернул антируткит, который по определенному раписанию выгружал отчет о состоянии системы. В курсовой работе своил навыки по следующим инструментам: Qemu-Kvm, Virtualbox, Linux, Ssh, Iptables, Nmap, Bash, Vsftpd, Telnet, Nginx, Squid, Tcpdump, Icmp, Tripwire, Rkhunter, Crontab
    Task:
    У руководителя уже установлена тестовая система Centos в машине Qemu-Kvm. Наша первая задача скоипровать у него систему 
    Decision:
    $ sudo virsh list --all
     ID   Имя           Состояние
    -------------------------------
     2    Centos9       работает
     -    Alt           выключен
     -    Centos        выключен
     -    Kali          выключен
     -    ubuntu22.04   выключен
     -    Windows       выключен
    $ sudo virsh shutdown Centos9
    $ sudo virsh domblklist Centos9
     Назначение   Источник
    ---------------------------------------------------
     vda          /images/Centos9.img
     sda          -
    $ rsync -avzP user@IpAddrCentos:/images/Centos9.img Centos/.
    $ sudo virsh dumpxml Centos9 > Centos9.xml
    $ head Centos9.xml
    <domain type='kvm'>
      <name>Centos9</name>
      <uuid>8cdbcba9-a955-4ca1-b75e-ab3145255c8d</uuid>
      <metadata>
        <libosinfo:libosinfo xmlns:libosinfo="http://libosinfo.org/xmlns/libvirt/domain/1.0">
          <libosinfo:os id="http://centos.org/centos-stream/9"/>
        </libosinfo:libosinfo>
      </metadata>
      <memory unit='KiB'>2097152</memory>
      <currentMemory unit='KiB'>2097152</currentMemory>
    $ sudo head /etc/libvirt/qemu/Centos9.xml
    <!--
    WARNING: THIS IS AN AUTO-GENERATED FILE. CHANGES TO IT ARE LIKELY TO BE
    OVERWRITTEN AND LOST. Changes to this xml configuration should be made using:
      virsh edit Centos9
    or other application using the libvirt API.
    -->
    <domain type='kvm'>
      <name>Centos9</name>
      <uuid>8cdbcba9-a955-4ca1-b75e-ab3145255c8d</uuid>
    $ sudo qemu-img info Centos9.img
    image: Centos9.img
    file format: qcow2
    virtual size: 50 GiB (53687091200 bytes)
    disk size: 50 GiB
    cluster_size: 65536
    Snapshot list:
    ID        TAG               VM SIZE                DATE     VM CLOCK     ICOUNT
    1         snapshot1             0 B 2023-09-28 13:18:14 00:00:00.000          0
    Format specific information:
        compat: 1.1
        compression type: zlib
        lazy refcounts: true
        refcount bits: 16
        corrupt: false
        extended l2: false
    Child node '/file':
        filename: Centos9.img
        protocol type: file
        file length: 50 GiB (53695545344 bytes)
        disk size: 50 GiB
    $ sudo qemu-img convert Centos9.img tmp.bin -p
    $ VBoxManage convertdd tmp.bin virtualbox.vdi
    $ VBoxManage modifyvdi virtualbox.vdi compact
    Task:
    Работа с iptables. 
    Проверяем наличие пакета iptables. 
    Убедимся, что iptables запускается при старте системы. 
    Выводим список текущих правил iptables.
    проанализируем какие порты открыты в сервере Centos. 
    Для этого нужно подключиться с сервера ubuntu.
    Decision:
    $ virsh start Centos9
    $ virsh start ubuntu22.04
    $ ssh -X user@IpAddrCentos
    $ sudo su
    [root@centos user]# yum install iptables-services
    [root@centos user]# iptables --version
    [root@centos user]# iptables -L -v
    Chain INPUT (policy ACCEPT 0 packets, 0 bytes)
     pkts bytes target     prot opt in     out     source               destination         
    Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
     pkts bytes target     prot opt in     out     source               destination         
    Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes)
     pkts bytes target     prot opt in     out     source               destination 
    [root@centos user]# systemctl start iptables
    [root@centos user]# systemctl enable iptables
    [root@centos user]# service iptables status
    $ ssh -X user@IpAddrUbuntu
    user@ubuntu:~$ nmap IpAddrCentos
    Starting Nmap 7.80 ( https://nmap.org ) at 2023-10-17 08:33 CDT
    Nmap scan report for centos (IpAddrCentos)
    Host is up (0.00089s latency).
    Not shown: 999 filtered ports
    PORT   STATE SERVICE
    22/tcp open  ssh
    MAC Address: MacAddrCentos (QEMU virtual NIC)
    Nmap done: 1 IP address (1 host up) scanned in 5.30 seconds
    Task:
    В сервере Centos Напишем набор правил iptables, в котором мы разрешаем все исходящие соединения и строго ограничиваем входящие. 
    Доступ будет возможен по портам TCP: 21, 22, 25, 53, 80, 143, 443, по портам UDP: 20, 21, 53, также мы пропускаем пакеты для уже установленных соединений.
    С удаленной машины просканируем порты на нашем сервере.
    Decision:
    [root@centos user]# vim firewall.sh
    [root@centos user]# cat firewall.sh
    #!/bin/bash
    IPT="/sbin/iptables"
    # Очищаем правила и удаляем цепочки.
    $IPT -F
    $IPT -X
    # По умолчанию доступ запрещен.
    $IPT -P INPUT DROP
    $IPT -P FORWARD DROP
    $IPT -P OUTPUT DROP
    # Список разрешенных TCP и UDP портов.
    TCP_PORTS="21,22,25,53,80,143,443"
    UDP_PORTS="53,21,20"
    # Разрешаем пакеты для интерфейса обратной петли.
    $IPT -A INPUT -i lo -j ACCEPT
    $IPT -A OUTPUT -o lo -j ACCEPT
    # Разрешаем пакеты для установленных соединений.
    $IPT -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
    # Разрешаем исходящие соединения.
    $IPT -A OUTPUT -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT
    # Разрешаем доступ к портам, описанным в переменных TCP_PORTS и UDP_PORTS.
    $IPT -A INPUT -p tcp -m multiport --dport $TCP_PORTS -j ACCEPT
    $IPT -A INPUT -p udp -m multiport --dport $UDP_PORTS -j ACCEPT
    # Разрешаем исходящий ping.
    $IPT -A INPUT -p icmp -m icmp --icmp-type echo-reply -j ACCEPT
    [root@centos user]# chmod +x firewall.sh
    [root@centos user]# ./firewall.sh
    [root@centos user]# iptables -L -v
    Chain INPUT (policy DROP 1986 packets, 87384 bytes)
     pkts bytes target     prot opt in     out     source               destination         
        0     0 ACCEPT     all  --  lo     any     anywhere             anywhere            
       79  5604 ACCEPT     all  --  any    any     anywhere             anywhere             state RELATED,ESTABLISHED
        9   396 ACCEPT     tcp  --  any    any     anywhere             anywhere             multiport dports ftp,ssh,smtp,domain,http,imap,https
        0     0 ACCEPT     udp  --  any    any     anywhere             anywhere             multiport dports domain,ftp,ftp-data
        0     0 ACCEPT     icmp --  any    any     anywhere             anywhere             icmp echo-reply
    Chain FORWARD (policy DROP 0 packets, 0 bytes)
     pkts bytes target     prot opt in     out     source               destination         
    Chain OUTPUT (policy DROP 0 packets, 0 bytes)
     pkts bytes target     prot opt in     out     source               destination         
        0     0 ACCEPT     all  --  any    lo      anywhere             anywhere            
       60  7920 ACCEPT     all  --  any    any     anywhere             anywhere             state NEW,RELATED,ESTABLISHED
    [root@centos user]# service iptables save
    user@ubuntu:~$ nmap IpAddrCentos
    Starting Nmap 7.80 ( https://nmap.org ) at 2023-10-17 08:37 CDT
    Nmap scan report for centos (IpAddrCentos)
    Host is up (0.00095s latency).
    Not shown: 993 filtered ports
    PORT    STATE  SERVICE
    21/tcp  closed ftp
    22/tcp  open   ssh
    25/tcp  closed smtp
    53/tcp  closed domain
    80/tcp  closed http
    143/tcp closed imap
    443/tcp closed https
    MAC Address: MacAddrCentos (QEMU virtual NIC)
    Nmap done: 1 IP address (1 host up) scanned in 5.20 seconds
    Task:
    Если сетевых интерфейсов два или более, то необходимо включить перенаправление трафика и перечитать конфигурационный файл
    Decision:
    [root@centos user]# vim /etc/sysctl.conf
    [root@centos user]# cat /etc/sysctl.conf
    ...
    net.ipv4.ip_forward = 1
    [root@centos user]# sysctl -p /etc/sysctl.conf
    net.ipv4.ip_forward = 1
    Task:
    Install and configure an FTP server. Securing the connection to the FTP server.
    Decision:
    [root@centos user]# dnf update -y
    [root@centos user]# dnf install vsftpd -y
    [root@centos user]# systemctl enable vsftpd --now
    [root@centos user]# systemctl status vsftpd
    user@ubuntu:~$ nmap IpAddrCentos
    Starting Nmap 7.80 ( https://nmap.org ) at 2023-10-18 07:41 CDT
    Nmap scan report for IpAddrCentos
    Host is up (0.0012s latency).
    Not shown: 993 filtered ports
    PORT    STATE  SERVICE
    21/tcp  open   ftp
    22/tcp  open   ssh
    25/tcp  closed smtp
    53/tcp  closed domain
    80/tcp  closed http
    143/tcp closed imap
    443/tcp closed https
    MAC Address: MacAddrCentos (QEMU virtual NIC)
    Nmap done: 1 IP address (1 host up) scanned in 5.80 seconds
    [root@centos user]# useradd -m -d "/home/ftpuser" ftpuser
    [root@centos user]# passwd ftpuser
    [root@centos user]# mkdir -p /home/ftpuser/shared
    [root@centos user]# chmod -R 750 /home/ftpuser/shared
    [root@centos user]# chown ftpuser: /home/ftpuser/shared
    [root@centos user]# vim /etc/vsftpd/user_list
    [root@centos user]# cat /etc/vsftpd/user_list | grep ftp
    ftpuser
    [root@centos user]# vim /etc/vsftpd/vsftpd.conf
    [root@centos user]# cat /etc/vsftpd/vsftpd.conf
    ...
    anonymous_enable=NO
    ...
    local_enable=YES
    ...
    write_enable=YES
    ...
    chroot_local_user=YES
    ...
    allow_writeable_chroot=YES
    pasv_min_port=31500
    pasv_max_port=32500
    userlist_file=/etc/vsftpd/user_list
    userlist_deny=NO
    [root@centos user]# systemctl restart vsftpd
    [root@centos user]# openssl req -x509 -nodes -days 3650 \
    -newkey rsa:2048 -keyout /etc/vsftpd.pem \
    -out /etc/vsftpd/vsftpd.pem
    [root@centos user]# vim /etc/vsftpd/vsftpd.conf
    [root@centos user]# cat /etc/vsftpd/vsftpd.conf
    ...
    #rsa_cert_file=/etc/vsftpd/vsftpd.pem
    #rsa_private_key_file=/etc/vsftpd.pem
    #ssl_enable=YES
    [root@centos user]# systemctl restart vsftpd
    [root@centos user]# cat /etc/sysconfig/iptables
    # Generated by iptables-save v1.8.8 (nf_tables) on Sun Oct 22 12:10:33 2023
    *mangle
    :PREROUTING ACCEPT [45:3316]
    :INPUT ACCEPT [45:3316]
    :FORWARD ACCEPT [0:0]
    :OUTPUT ACCEPT [34:5048]
    :POSTROUTING ACCEPT [34:5048]
    COMMIT
    # Completed on Sun Oct 22 12:10:33 2023
    # Generated by iptables-save v1.8.8 (nf_tables) on Sun Oct 22 12:10:33 2023
    *raw
    :PREROUTING ACCEPT [45:3316]
    :OUTPUT ACCEPT [34:5048]
    COMMIT
    # Completed on Sun Oct 22 12:10:33 2023
    # Generated by iptables-save v1.8.8 (nf_tables) on Sun Oct 22 12:10:33 2023
    *filter
    :INPUT DROP [0:0]
    :FORWARD DROP [0:0]
    :OUTPUT DROP [0:0]
    -A INPUT -i lo -j ACCEPT
    -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
    -A INPUT -p tcp -m multiport --dports 21,22,25,53,80,143,443 -j ACCEPT
    -A INPUT -p udp -m multiport --dports 53,21,20 -j ACCEPT
    -A INPUT -p icmp -m icmp --icmp-type 0 -j ACCEPT
    -A OUTPUT -o lo -j ACCEPT
    -A OUTPUT -m state --state NEW,RELATED,ESTABLISHED -j ACCEPT
    COMMIT
    # Completed on Sun Oct 22 12:10:33 2023
    # Generated by iptables-save v1.8.8 (nf_tables) on Sun Oct 22 12:10:33 2023
    *nat
    :PREROUTING ACCEPT [0:0]
    :INPUT ACCEPT [0:0]
    :OUTPUT ACCEPT [0:0]
    :POSTROUTING ACCEPT [0:0]
    COMMIT
    # Completed on Sun Oct 22 12:10:33 2023
    [root@centos user]# iptables -t filter -A INPUT -p tcp --dport 20:21 -j ACCEPT
    [root@centos user]# iptables -t filter -A INPUT -p tcp --dport 31500:32500 -j ACCEPT
    [root@centos user]# service iptables save
    [root@centos user]# systemctl restart iptables
    user@ubuntu:~$ telnet IpAddrCentos 21
    Trying IpAddrCentos...
    Connected to IpAddrCentos.
    Escape character is '^]'.
    220 (vsFTPd 3.0.5)
    USER ftpuser
    331 Please specify the password.
    PASS tpassword
    230 Login successful.
    PWD
    257 "/" is the current directory
    PASV
    227 Entering Passive Mode (I,P,126,61).
    user@ubuntu:~$ 126*256+61
    32317
    user@ubuntu:~$ telnet IpAddrCentos 32317
    Trying IpAddrCentos...
    Connected to IpAddrCentos.
    Escape character is '^]'.
    LIST
    150 Here comes the directory listing.
    226 Directory send OK.
    drwxr-x---    2 1001     1001            6 Oct 21 03:06 shared
    QUIT
    221 Goodbye.
    Task:
    Запрещаем доступ к фтп всем пользователям, кроме пользователя с MAC-адресом MacAddrUbuntu.
    Decision:
    [root@centos user]# iptables -I INPUT -p tcp --dport 21 -m mac ! --mac-source MacAddrUbuntu -j REJECT
    [root@centos user]# service iptables save
    [root@centos user]# systemctl restart iptables
    user@ubuntu:~$ ifconfig | grep MacAddrUbuntu
            ether MacAddrUbuntu  txqueuelen 1000  (Ethernet)
    user@ubuntu:~$ ftp IpAddrCentos
    Connected to IpAddrCentos.
    220 (vsFTPd 3.0.5)
    Name (IpAddrCentos:user): ftpuser
    331 Please specify the password.
    Password: 
    230 Login successful.
    Remote system type is UNIX.
    Using binary mode to transfer files.
    ftp> ls
    229 Entering Extended Passive Mode (|||31695|)
    150 Here comes the directory listing.
    drwxr-x---    2 1001     1001            6 Oct 21 03:06 shared
    226 Directory send OK.
    ftp> exit
    221 Goodbye.
    ┌──(user㉿kali)-[~]
    └─$ ftp IpAddrCentos
    ftp: Can't connect to `IpAddrCentos:21': Connection refused
    ftp: Can't connect to `IpAddrCentos:ftp'
    ftp> exit
    ┌──(user㉿kali)-[~]
    └─$ telnet IpAddrCentos 21 
    Trying IpAddrCentos...
    telnet: Unable to connect to remote host: Connection refused
    Task:
    Install Nginx
    Decision:
    [root@centos user]# dnf update -y
    [root@centos user]# dnf install nginx
    [root@centos user]# systemctl start nginx
    [root@centos user]# systemctl enable nginx
    [root@centos user]# nginx -v
    nginx version: nginx/1.22.1
    [root@centos user]# firefox IpAddrCentos
    Task:
    Configure Squid Proxy
    Decision:
    ┌──(root㉿kali)-[/home/user]
    └─#  apt update
    ┌──(root㉿kali)-[/home/user]
    └─#  apt install squid
    ┌──(root㉿kali)-[/home/user]
    └─# systemctl start squid
    ┌──(root㉿kali)-[/home/user]
    └─# systemctl enable squid
    ┌──(root㉿kali)-[/home/user]
    └─# grep -Eiv '(^#|^$)' /etc/squid/squid.conf
    acl localnet src 0.0.0.1-0.255.255.255  # RFC 1122 "this" network (LAN)
    acl localnet src 10.0.0.0/8             # RFC 1918 local private network (LAN)
    acl localnet src 100.64.0.0/10          # RFC 6598 shared address space (CGN)
    acl localnet src 169.254.0.0/16         # RFC 3927 link-local (directly plugged) machines
    acl localnet src 172.16.0.0/12          # RFC 1918 local private network (LAN)
    acl localnet src 192.168.0.0/16         # RFC 1918 local private network (LAN)
    acl localnet src fc00::/7               # RFC 4193 local private network range
    acl localnet src fe80::/10              # RFC 4291 link-local (directly plugged) machines
    acl SSL_ports port 443
    acl Safe_ports port 80          # http
    acl Safe_ports port 21          # ftp
    acl Safe_ports port 443         # https
    acl Safe_ports port 70          # gopher
    acl Safe_ports port 210         # wais
    acl Safe_ports port 1025-65535  # unregistered ports
    acl Safe_ports port 280         # http-mgmt
    acl Safe_ports port 488         # gss-http
    acl Safe_ports port 591         # filemaker
    acl Safe_ports port 777         # multiling http
    http_access deny !Safe_ports
    http_access deny CONNECT !SSL_ports
    http_access allow localhost manager
    http_access deny manager
    http_access allow localhost
    http_access deny to_localhost
    http_access deny to_linklocal
    include /etc/squid/conf.d/*.conf
    http_access deny all
    http_port 3128
    coredump_dir /var/spool/squid
    refresh_pattern ^ftp:           1440    20%     10080
    refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
    refresh_pattern .               0       20%     4320
    ┌──(root㉿kali)-[/home/user]
    └─# mv /etc/squid/squid.conf /etc/squid/squid.conf.bac
    ┌──(root㉿kali)-[/home/user]
    └─# vim /etc/squid/squid.conf
    ┌──(root㉿kali)-[/home/user]
    └─# cat /etc/squid/squid.conf    
    acl localnet src IpAddrKali
    http_access allow localnet
    http_port 3128
    ┌──(root㉿kali)-[/home/user]
    └─# systemctl restart squid   
    ┌──(root㉿kali)-[/home/user]
    └─# cat /var/log/squid/access.log
    1698153952.395      1 IpAddrKali NONE_NONE/400 3816 - / - HIER_NONE/- text/html
    1698153953.958      7 IpAddrKali TCP_DENIED/403 4185 GET http://kali:3128/squid-internal-static/icons/SN.png - HIER_NONE/- text/html
    ...
    1698154905.087      2 IpAddrKali TCP_MISS/400 3889 GET http://IpAddrKali:3128/ - HIER_DIRECT/IpAddrKali text/html
    1698154906.698    149 IpAddrKali TCP_TUNNEL/200 39 CONNECT static.vk.com:443 - HIER_DIRECT/87.240.137.164 -
    1698154906.796      0 IpAddrKali NONE_NONE/400 3838 - /favicon.ico - HIER_NONE/- text/html
    1698154906.797      2 IpAddrKali TCP_MISS/400 3911 GET http://IpAddrKali:3128/favicon.ico - HIER_DIRECT/IpAddrKali text/html
    ┌──(root㉿kali)-[/home/user]
    └─# apt install apache2-utils   
    ┌──(root㉿kali)-[/home/user]
    └─# htpasswd -c /etc/squid/passwd user
    ┌──(root㉿kali)-[/home/user]
    └─# vim /etc/squid/squid.conf         
    ┌──(root㉿kali)-[/home/user]
    └─# cat /etc/squid/squid.conf
    #acl localnet src IpAddrKali
    #http_access allow localnet
    http_port 3128
    via off
    auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/passwd
    auth_param basic children 5
    auth_param basic credentialsttl 2 hours
    auth_param basic casesensitive on
    auth_param basic realm Squid proxy for kali
    acl auth_users proxy_auth REQUIRED
    http_access allow auth_users
    ┌──(root㉿kali)-[/home/user]
    └─# systemctl restart squid
    Task:
    Configure Squid Proxy
    Decision:
    [root@centos user]# dnf install squid
    [root@centos user]# systemctl enable --now squid
    [root@centos user]# grep -Eiv '(^#|^$)' /etc/squid/squid.conf
    acl localnet src 0.0.0.1-0.255.255.255  # RFC 1122 "this" network (LAN)
    acl localnet src 10.0.0.0/8             # RFC 1918 local private network (LAN)
    acl localnet src 100.64.0.0/10          # RFC 6598 shared address space (CGN)
    acl localnet src 169.254.0.0/16         # RFC 3927 link-local (directly plugged) machines
    acl localnet src 172.16.0.0/12          # RFC 1918 local private network (LAN)
    acl localnet src 192.168.0.0/16         # RFC 1918 local private network (LAN)
    acl localnet src fc00::/7               # RFC 4193 local private network range
    acl localnet src fe80::/10              # RFC 4291 link-local (directly plugged) machines
    acl SSL_ports port 443
    acl Safe_ports port 80          # http
    acl Safe_ports port 21          # ftp
    acl Safe_ports port 443         # https
    acl Safe_ports port 70          # gopher
    acl Safe_ports port 210         # wais
    acl Safe_ports port 1025-65535  # unregistered ports
    acl Safe_ports port 280         # http-mgmt
    acl Safe_ports port 488         # gss-http
    acl Safe_ports port 591         # filemaker
    acl Safe_ports port 777         # multiling http
    http_access deny !Safe_ports
    http_access deny CONNECT !SSL_ports
    http_access allow localhost manager
    http_access deny manager
    http_access allow localnet
    http_access allow localhost
    http_access deny all
    http_port 3128
    coredump_dir /var/spool/squid
    refresh_pattern ^ftp:           1440    20%     10080
    refresh_pattern ^gopher:        1440    0%      1440
    refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
    refresh_pattern .               0       20%     4320
    [root@centos user]# mv /etc/squid/squid.conf /etc/squid/squid.conf.bac
    [root@centos user]# vim /etc/squid/squid.conf
    [root@centos user]# cat /etc/squid/squid.conf
    acl localnet src IpAddrCentos
    acl localnet src IpAddr.0/32
    acl Safe_ports port 80
    acl Safe_ports port 443
    cache_dir ufs /var/spool/squid 1000 16 256
    http_access allow localnet
    http_port 3128   
    [root@centos user]# systemctl restart squid
    [root@centos user]# iptables -t filter -A INPUT -p tcp --dport 3128 -j ACCEPT
    [root@centos user]# service iptables save
    [root@centos user]# systemctl restart iptables.service
    [root@centos user]# curl -O -L "https://www.redhat.com/index.html" -x "IpAddrCentos:
    3128"
      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                     Dload  Upload   Total   Spent    Left  Speed
      0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
    100  156k    0  156k    0     0   147k      0 --:--:--  0:00:01 --:--:--  795k
    user@ubuntu:~$ sudo nmap IpAddrCentos
    Starting Nmap 7.80 ( https://nmap.org ) at 2023-10-25 08:12 CDT
    Nmap scan report for IpAddrCentos
    Host is up (0.00086s latency).
    Not shown: 991 filtered ports
    PORT     STATE  SERVICE
    20/tcp   closed ftp-data
    21/tcp   open   ftp
    22/tcp   open   ssh
    25/tcp   closed smtp
    53/tcp   closed domain
    80/tcp   open   http
    143/tcp  closed imap
    443/tcp  closed https
    3128/tcp open   squid-http
    MAC Address: MacAddrCentos (QEMU virtual NIC)
    Nmap done: 1 IP address (1 host up) scanned in 5.16 seconds
    [root@centos user]# dnf install httpd-tools
    [root@centos user]# touch /etc/squid/passwd
    [root@centos user]# chown squid /etc/squid/passwd
    [root@centos user]# htpasswd /etc/squid/passwd proxyuser
    [root@centos user]# vim /etc/squid/squid.conf
    [root@centos user]# cat /etc/squid/squid.conf
    #acl localnet src IpAddrCentos
    #acl localnet src IpAddr.0/32
    #acl Safe_ports port 80
    #acl Safe_ports port 443
    #cache_dir ufs /var/spool/squid 1000 16 256
    #http_access allow localnet
    http_port 3128   
    auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/passwd
    auth_param basic children 5
    auth_param basic realm Squid Basic Authentication
    auth_param basic credentialsttl 2 hours
    acl auth_users proxy_auth REQUIRED
    http_access allow auth_users
    [root@centos user]# systemctl restart squid
    [root@centos user]# curl -O -L "https://www.redhat.com/index.html" -x "proxyuser:tpassword@IpAddrCentos:3128"
      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                     Dload  Upload   Total   Spent    Left  Speed
      0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
    100  156k    0  156k    0     0   124k      0 --:--:--  0:00:01 --:--:--  562k
    Task:
    Перенаправление пакетов, идущих на 80-й порт, на стандартный порт прокси-сервера.
    Decision:
    [root@centos user]# iptables -t nat -A PREROUTING -s IpAddr.0 -p tcp --dport 80 -j REDIRECT --to-port 3128
    [root@centos user]# service iptables save
    [root@centos user]# systemctl restart iptables.service
    Task:
    Предоставляем доступ из Интернет к веб-серверу, который расположен в локальной сети (проброс порта). Вместо IpAddrUbuntu укажите IP-адрес вашего веб-сервера.
    Decision:
    [root@centos user]# iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to-destination IpAddrUbuntu:80
    [root@centos user]# service iptables save
    [root@centos user]# systemctl restart iptables.service
    Tas
    Включаем маскарадинг для доступа в Интернет пользователей локальной сети.
    Decision:
    [root@centos user]# iptables -t nat -A POSTROUTING -o LanCard -s IpAddr.0/32 -j MASQUERADE
    [root@centos user]# service iptables save
    [root@centos user]# systemctl restart iptables.service
    Task:
    сканирование сети
    Decision:
    [root@centos user]# nmap -sL IpAddrCentos/24
    Starting Nmap 7.92 ( https://nmap.org ) at 2023-10-29 11:25 +08
    Nmap scan report for IpAddr.0
    ...
    Nmap scan report for kali (IpAddrKali)
    ...
    Nmap scan report for centos (IpAddr)
    ...
    Nmap scan report for ubuntu (IpAddrUbuntu)
    ...
    Nmap scan report for IpAddr.255
    Nmap done: 256 IP addresses (0 hosts up) scanned in 28.06 seconds
    Decision:
    Утилита обнаружила активные устройства в сети. Среди них показались ip-адреса, в котором были развернуты мои виртуальные машины
    Task:
    Просканируем удаленный хост (агрессивный режим).
    Decision:
    user@ubuntu:~$ sudo nmap -A -T4 IpAddrCentos
    Starting Nmap 7.80 ( https://nmap.org ) at 2023-10-27 23:24 CDT
    Nmap scan report for IpAddrCentos
    Host is up (0.00084s latency).
    Not shown: 992 filtered ports
    PORT     STATE  SERVICE    VERSION
    20/tcp   closed ftp-data
    21/tcp   open   ftp        vsftpd 3.0.5
    22/tcp   open   ssh        OpenSSH 8.7 (protocol 2.0)
    25/tcp   closed smtp
    53/tcp   closed domain
    143/tcp  closed imap
    443/tcp  closed https
    3128/tcp open   http-proxy Squid http proxy 5.5
    |_http-server-header: squid/5.5
    |_http-title: ERROR: The requested URL could not be retrieved
    MAC Address: MacAddrCentos (QEMU virtual NIC)
    Aggressive OS guesses: Linux 2.6.32 - 3.13 (94%), Linux 2.6.22 - 2.6.36 (92%), Linux 3.10 (92%), Linux 3.10 - 4.11 (92%), Linux 2.6.39 (92%), Linux 2.6.32 (91%), Linux 3.2 - 4.9 (91%), Linux 2.6.32 - 3.10 (91%), Linux 2.6.18 (90%), Linux 3.16 - 4.6 (90%)
    No exact OS matches for host (test conditions non-ideal).
    Network Distance: 1 hop
    Service Info: OS: Unix
    TRACEROUTE
    HOP RTT     ADDRESS
    1   0.83 ms IpAddrCentos
    OS and Service detection performed. Please report any incorrect results at https://nmap.org/submit/ .
    Nmap done: 1 IP address (1 host up) scanned in 24.18 seconds
    Task:
    узнать более подробную информацию о машине и запущенных на ней сервисах
    Decision:
    [root@centos user]# nmap -sV IpAddrUbuntu
    Starting Nmap 7.92 ( https://nmap.org ) at 2023-10-29 11:35 +08
    Nmap scan report for ubuntu (IpAddrUbuntu)
    Host is up (0.0010s latency).
    Not shown: 997 filtered tcp ports (no-response)
    PORT    STATE  SERVICE VERSION
    22/tcp  open   ssh     OpenSSH 8.9p1 Ubuntu 3ubuntu0.3 (Ubuntu Linux; protocol 2.0)
    80/tcp  open   http    Apache httpd 2.4.52 ((Ubuntu))
    443/tcp closed https
    MAC Address: MacAddrUbuntu (QEMU virtual NIC)
    Service Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel
    Service detection performed. Please report any incorrect results at https://nmap.org/submit/ .
    Nmap done: 1 IP address (1 host up) scanned in 12.27 seconds
    Task:
    В виртуальной машине запущен веб-сервер, поэтому мы можем рассмотреть эту службу подробнее.
    Decision:
    [root@centos user]# nmap -sC IpAddrUbuntu -p 80
    Starting Nmap 7.92 ( https://nmap.org ) at 2023-10-29 11:45 +08
    Nmap scan report for ubuntu (IpAddrUbuntu)
    Host is up (0.00078s latency).
    PORT   STATE SERVICE
    80/tcp open  http
    |_http-title: david138it
    MAC Address: MacAddrUbuntu (QEMU virtual NIC)
    Nmap done: 1 IP address (1 host up) scanned in 1.88 seconds
    Task:
    Найти все скрипты, которые позволяют проверить порты более детально, для нашего веб-сервера. Затем попытаемся использовать один из них, для этого достаточно указать его с помощью опции —script. Но сначала вы можете посмотреть информацию о скрипте. 
    Decision:
    [root@centos user]# find /usr/share/nmap/scripts/ -name '*.nse' | grep http
    /usr/share/nmap/scripts/http-brute.nse
    ...
    /usr/share/nmap/scripts/riak-http-info.nse
    [root@centos user]# nmap --script-help http-brute.nse
    Starting Nmap 7.92 ( https://nmap.org ) at 2023-10-29 11:50 +08
    http-brute
    Categories: intrusive brute
    https://nmap.org/nsedoc/scripts/http-brute.html
      Performs brute force password auditing against http basic, digest and ntlm authentication.
      This script uses the unpwdb and brute libraries to perform password
      guessing. Any successful guesses are stored in the nmap registry, using
      the creds library, for other scripts to use.
    [root@centos user]# nmap --script http-brute.nse IpAddrWindServ84 -p 80
    Starting Nmap 7.92 ( https://nmap.org ) at 2023-10-29 11:54 +08
    Note: Host seems down. If it is really up, but blocking our ping probes, try -Pn
    Nmap done: 1 IP address (0 hosts up) scanned in 3.61 seconds
    [root@centos user]# nmap --script http-brute.nse IpAddrWindServ84 -p 80 -Pn
    Starting Nmap 7.92 ( https://nmap.org ) at 2023-10-29 12:08 +08
    Nmap scan report for IpAddrWindServ84
    Host is up.
    PORT   STATE    SERVICE
    80/tcp filtered http
    Nmap done: 1 IP address (1 host up) scanned in 2.61 seconds
    Decision:
    В последней команде пытался узнать пароль веб-сервера. В некоторых случаях можно вытащить логин и пароль. Такое происходит, когда используются параметры входа по умолчанию.
    Task:
    Сканируем диапазон портов
    Decision:
    user@ubuntu:~$ sudo nmap -sT -p 21-80 IpAddrCentos
    Starting Nmap 7.80 ( https://nmap.org ) at 2023-10-27 23:41 CDT
    Nmap scan report for IpAddrCentos
    Host is up (0.0030s latency).
    Not shown: 56 filtered ports
    PORT   STATE  SERVICE
    21/tcp open   ftp
    22/tcp open   ssh
    25/tcp closed smtp
    53/tcp closed domain
    MAC Address: MacAddrCentos (QEMU virtual NIC)
    Nmap done: 1 IP address (1 host up) scanned in 1.85 seconds
    Task:
    Перехватываем весь трафик для MAC-адреса MacAddrUbuntu на сетевом интерфейсе LanCard.
    Decision:
    user@ubuntu:~$ sudo tcpdump -n -i LanCard "ether host MacAddrCentos"
    tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
    listening on LanCard, link-type EN10MB (Ethernet), snapshot length 262144 bytes
    [root@centos user]# ssh user@IpAddrUbuntu
    user@IpAddrUbuntu's password: 
    user@ubuntu:~$ sudo tcpdump -n -i LanCard "ether host MacAddrCentos"
    ...
    00:11:09.121772 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [S], seq 3127527477, win 64240, options [mss 1460,sackOK,TS val 3107336019 ecr 0,nop,wscale 7], length 0
    00:11:09.121894 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [S.], seq 2200785231, ack 3127527478, win 65160, options [mss 1460,sackOK,TS val 2462886528 ecr 3107336019,nop,wscale 7], length 0
    00:11:09.122300 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [.], ack 1, win 502, options [nop,nop,TS val 3107336020 ecr 2462886528], length 0
    00:11:09.123009 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [P.], seq 1:22, ack 1, win 502, options [nop,nop,TS val 3107336020 ecr 2462886528], length 21: SSH: SSH-2.0-OpenSSH_8.7
    00:11:09.123086 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [.], ack 22, win 509, options [nop,nop,TS val 2462886529 ecr 3107336020], length 0
    00:11:09.148795 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 1:42, ack 22, win 509, options [nop,nop,TS val 2462886555 ecr 3107336020], length 41: SSH: SSH-2.0-OpenSSH_8.9p1 Ubuntu-3ubuntu0.3
    00:11:09.149526 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [.], ack 42, win 502, options [nop,nop,TS val 3107336047 ecr 2462886555], length 0
    00:11:09.150579 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [P.], seq 22:1382, ack 42, win 502, options [nop,nop,TS val 3107336048 ecr 2462886555], length 1360
    00:11:09.151887 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 42:1122, ack 1382, win 501, options [nop,nop,TS val 2462886558 ecr 3107336048], length 1080
    00:11:09.157642 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [P.], seq 1382:1430, ack 1122, win 501, options [nop,nop,TS val 3107336055 ecr 2462886558], length 48
    00:11:09.171709 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 1122:1654, ack 1430, win 501, options [nop,nop,TS val 2462886578 ecr 3107336055], length 532
    00:11:09.185522 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [P.], seq 1430:1446, ack 1654, win 501, options [nop,nop,TS val 3107336083 ecr 2462886578], length 16
    00:11:09.228923 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [.], ack 1446, win 501, options [nop,nop,TS val 2462886635 ecr 3107336083], length 0
    00:11:09.229685 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [P.], seq 1446:1498, ack 1654, win 501, options [nop,nop,TS val 3107336127 ecr 2462886635], length 52
    00:11:09.229782 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [.], ack 1498, win 501, options [nop,nop,TS val 2462886636 ecr 3107336127], length 0
    00:11:09.229995 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 1654:1706, ack 1498, win 501, options [nop,nop,TS val 2462886636 ecr 3107336127], length 52
    00:11:09.230579 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [P.], seq 1498:1566, ack 1706, win 501, options [nop,nop,TS val 3107336128 ecr 2462886636], length 68
    00:11:09.238565 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 1706:1758, ack 1566, win 501, options [nop,nop,TS val 2462886644 ecr 3107336128], length 52
    00:11:09.280247 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [.], ack 1758, win 501, options [nop,nop,TS val 3107336177 ecr 2462886644], length 0
    00:11:12.034402 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [P.], seq 1566:1714, ack 1758, win 501, options [nop,nop,TS val 3107338932 ecr 2462886644], length 148
    00:11:12.076713 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [.], ack 1714, win 501, options [nop,nop,TS val 2462889483 ecr 3107338932], length 0
    00:11:12.274441 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 1758:1794, ack 1714, win 501, options [nop,nop,TS val 2462889680 ecr 3107338932], length 36
    00:11:12.275003 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [.], ack 1794, win 501, options [nop,nop,TS val 3107339172 ecr 2462889680], length 0
    00:11:12.275445 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [P.], seq 1714:1834, ack 1794, win 501, options [nop,nop,TS val 3107339173 ecr 2462889680], length 120
    00:11:12.275497 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [.], ack 1834, win 501, options [nop,nop,TS val 2462889681 ecr 3107339173], length 0
    00:11:12.447785 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 1794:2422, ack 1834, win 501, options [nop,nop,TS val 2462889854 ecr 3107339173], length 628
    00:11:12.489012 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [.], ack 2422, win 501, options [nop,nop,TS val 3107339386 ecr 2462889854], length 0
    00:11:12.489092 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 2422:2474, ack 1834, win 501, options [nop,nop,TS val 2462889895 ecr 3107339386], length 52
    00:11:12.489453 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [.], ack 2474, win 501, options [nop,nop,TS val 3107339387 ecr 2462889895], length 0
    00:11:12.489928 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [P.], seq 1834:2242, ack 2474, win 501, options [nop,nop,TS val 3107339387 ecr 2462889895], length 408
    00:11:12.490014 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [.], ack 2242, win 501, options [nop,nop,TS val 2462889896 ecr 3107339387], length 0
    00:11:12.493478 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 2474:2582, ack 2242, win 501, options [nop,nop,TS val 2462889899 ecr 3107339387], length 108
    00:11:12.494668 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 2582:2906, ack 2242, win 501, options [nop,nop,TS val 2462889901 ecr 3107339387], length 324
    00:11:12.494876 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 2906:2942, ack 2242, win 501, options [nop,nop,TS val 2462889901 ecr 3107339387], length 36
    00:11:12.494942 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 2942:2978, ack 2242, win 501, options [nop,nop,TS val 2462889901 ecr 3107339387], length 36
    00:11:12.494972 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 2978:3062, ack 2242, win 501, options [nop,nop,TS val 2462889901 ecr 3107339387], length 84
    00:11:12.495040 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 3062:3098, ack 2242, win 501, options [nop,nop,TS val 2462889901 ecr 3107339387], length 36
    00:11:12.495108 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 3098:3198, ack 2242, win 501, options [nop,nop,TS val 2462889901 ecr 3107339387], length 100
    00:11:12.495173 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 3198:3234, ack 2242, win 501, options [nop,nop,TS val 2462889901 ecr 3107339387], length 36
    00:11:12.495242 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 3234:3270, ack 2242, win 501, options [nop,nop,TS val 2462889901 ecr 3107339387], length 36
    00:11:12.495311 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 3270:3370, ack 2242, win 501, options [nop,nop,TS val 2462889901 ecr 3107339387], length 100
    00:11:12.495324 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [.], ack 3198, win 501, options [nop,nop,TS val 3107339393 ecr 2462889899], length 0
    00:11:12.495436 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 3370:3406, ack 2242, win 501, options [nop,nop,TS val 2462889901 ecr 3107339393], length 36
    00:11:12.495512 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [.], ack 3370, win 501, options [nop,nop,TS val 3107339393 ecr 2462889901], length 0
    00:11:12.495525 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 3406:3522, ack 2242, win 501, options [nop,nop,TS val 2462889901 ecr 3107339393], length 116
    00:11:12.495620 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 3522:3558, ack 2242, win 501, options [nop,nop,TS val 2462889902 ecr 3107339393], length 36
    00:11:12.495693 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 3558:3594, ack 2242, win 501, options [nop,nop,TS val 2462889902 ecr 3107339393], length 36
    00:11:12.495753 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [.], ack 3522, win 501, options [nop,nop,TS val 3107339393 ecr 2462889901], length 0
    00:11:12.495765 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 3594:3694, ack 2242, win 501, options [nop,nop,TS val 2462889902 ecr 3107339393], length 100
    00:11:12.495844 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 3694:3730, ack 2242, win 501, options [nop,nop,TS val 2462889902 ecr 3107339393], length 36
    00:11:12.495907 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [.], ack 3694, win 501, options [nop,nop,TS val 3107339393 ecr 2462889902], length 0
    00:11:12.536934 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [.], ack 3730, win 501, options [nop,nop,TS val 3107339434 ecr 2462889902], length 0
    00:11:12.586762 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 3730:3782, ack 2242, win 501, options [nop,nop,TS val 2462889993 ecr 3107339434], length 52
    00:11:12.586838 IP IpAddrUbuntu.22 > IpAddrCentos.58598: Flags [P.], seq 3782:3898, ack 2242, win 501, options [nop,nop,TS val 2462889993 ecr 3107339434], length 116
    00:11:12.588205 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [.], ack 3782, win 501, options [nop,nop,TS val 3107339485 ecr 2462889993], length 0
    00:11:12.588207 IP IpAddrCentos.58598 > IpAddrUbuntu.22: Flags [.], ack 3898, win 501, options [nop,nop,TS val 3107339486 ecr 2462889993], length 0
    Task:
    Перехватываем только ICMP-пакеты
    Decision:
    user@ubuntu:~$ sudo tcpdump -i LanCard -n -nn -ttt 'ip proto \icmp'
    tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
    listening on LanCard, link-type EN10MB (Ethernet), snapshot length 262144 bytes
    [root@centos user]# ping IpAddrUbuntu
    PING IpAddrUbuntu (IpAddrUbuntu) 56(84) bytes of data.
    64 bytes from IpAddrUbuntu: icmp_seq=1 ttl=64 time=0.692 ms
    64 bytes from IpAddrUbuntu: icmp_seq=2 ttl=64 time=0.764 ms
    64 bytes from IpAddrUbuntu: icmp_seq=3 ttl=64 time=0.868 ms
    64 bytes from IpAddrUbuntu: icmp_seq=4 ttl=64 time=1.01 ms
    64 bytes from IpAddrUbuntu: icmp_seq=5 ttl=64 time=1.13 ms
    ^C
    --- IpAddrUbuntu ping statistics ---
    5 packets transmitted, 5 received, 0% packet loss, time 4006ms
    rtt min/avg/max/mdev = 0.692/0.892/1.130/0.159 ms
    user@ubuntu:~$ sudo tcpdump -i LanCard -n -nn -ttt 'ip proto \icmp'
    ...
     00:00:00.000000 IP IpAddrCentos > IpAddrUbuntu: ICMP echo request, id 4, seq 1, length 64
     00:00:00.000171 IP IpAddrUbuntu > IpAddrCentos: ICMP echo reply, id 4, seq 1, length 64
     00:00:01.001145 IP IpAddrCentos > IpAddrUbuntu: ICMP echo request, id 4, seq 2, length 64
     00:00:00.000077 IP IpAddrUbuntu > IpAddrCentos: ICMP echo reply, id 4, seq 2, length 64
     00:00:01.001365 IP IpAddrCentos > IpAddrUbuntu: ICMP echo request, id 4, seq 3, length 64
     00:00:00.000077 IP IpAddrUbuntu > IpAddrCentos: ICMP echo reply, id 4, seq 3, length 64
     00:00:01.001375 IP IpAddrCentos > IpAddrUbuntu: ICMP echo request, id 4, seq 4, length 64
     00:00:00.000077 IP IpAddrUbuntu > IpAddrCentos: ICMP echo reply, id 4, seq 4, length 64
     00:00:01.001573 IP IpAddrCentos > IpAddrUbuntu: ICMP echo request, id 4, seq 5, length 64
     00:00:00.000125 IP IpAddrUbuntu > IpAddrCentos: ICMP echo reply, id 4, seq 5, length 64
    Task:
    Перехватываем DNS-трафик между сервером и каким-нибудь узлом в сети.
    Decision:
    $ sudo virsh start Windows12
    [root@centos user]# nmap IpAddrWindServ
    Starting Nmap 7.92 ( https://nmap.org ) at 2023-10-28 13:24 +08
    Nmap scan report for RT-GM2-9 (IpAddrWindServ)
    Host is up (0.0017s latency).
    Not shown: 901 filtered tcp ports (no-response), 96 closed tcp ports (reset)
    PORT    STATE SERVICE
    53/tcp  open  domain
    80/tcp  open  http
    443/tcp open  https
    Nmap done: 1 IP address (1 host up) scanned in 3.64 seconds
    [root@centos user]# tcpdump -i LanCard -n -nn -ttt 'host IpAddrWindServ and port 53'
    Task:
    Перехватываем входящий трафик на порт 80. сохраняем статистику в файл my.log для первых 500 пакетов. Будет создан бинарный файл my.log, который можно отпарсить с помощью команды
    Decision:
    [root@centos user]# firefox IpAddrUbuntu
    [root@centos user]# tcpdump -v -i LanCard dst port 80
    dropped privs to tcpdump
    tcpdump: listening on LanCard, link-type EN10MB (Ethernet), snapshot length 262144 bytes
    14:07:24.835633 IP (tos 0x0, ttl 64, id 15549, offset 0, flags [DF], proto TCP (6), length 60)
        centos.45022 > ubuntu.http: Flags [S], cksum 0x76d1 (incorrect -> 0x9a90), seq 1076682845, win 64240, options [mss 1460,sackOK,TS val 3110711737 ecr 0,nop,wscale 7], length 0
    14:07:24.836511 IP (tos 0x0, ttl 64, id 15550, offset 0, flags [DF], proto TCP (6), length 52)
        centos.45022 > ubuntu.http: Flags [.], cksum 0x76c9 (incorrect -> 0x51b3), ack 2952605173, win 502, options [nop,nop,TS val 3110711738 ecr 3247231251], length 0
    14:07:24.836838 IP (tos 0x0, ttl 64, id 15551, offset 0, flags [DF], proto TCP (6), length 412)
        centos.45022 > ubuntu.http: Flags [P.], cksum 0x7831 (incorrect -> 0xb0a2), seq 0:360, ack 1, win 502, options [nop,nop,TS val 3110711739 ecr 3247231251], length 360: HTTP, length: 360
            GET / HTTP/1.1
            Host: IpAddrUbuntu
            User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0
            Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8
            Accept-Language: ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3
            Accept-Encoding: gzip, deflate
            Connection: keep-alive
            Upgrade-Insecure-Requests: 1
    14:07:24.844920 IP (tos 0x0, ttl 64, id 15552, offset 0, flags [DF], proto TCP (6), length 52)
        centos.45022 > ubuntu.http: Flags [.], cksum 0x76c9 (incorrect -> 0x4285), ack 3522, win 489, options [nop,nop,TS val 3110711747 ecr 3247231260], length 0
    14:07:29.846014 IP (tos 0x0, ttl 64, id 15553, offset 0, flags [DF], proto TCP (6), length 52)
        centos.45022 > ubuntu.http: Flags [F.], cksum 0x76c9 (incorrect -> 0x2eef), seq 360, ack 3522, win 501, options [nop,nop,TS val 3110716748 ecr 3247231260], length 0
    14:07:29.846795 IP (tos 0x0, ttl 64, id 15554, offset 0, flags [DF], proto TCP (6), length 52)
        centos.45022 > ubuntu.http: Flags [.], cksum 0x76c9 (incorrect -> 0x1b63), ack 3523, win 501, options [nop,nop,TS val 3110716749 ecr 3247236262], length 0
    [root@centos user]# tcpdump -v -n -w my.log dst port 80 -c 500
    dropped privs to tcpdump
    tcpdump: listening on LanCard, link-type EN10MB (Ethernet), snapshot length 262144 bytes
    [root@centos user]# ls my.log
    my.log
    [root@centos user]# tcpdump -nr my.log | awk '{print $3}' | grep -oE '[0-9]{1,}\.[0-9]{1,}\.[0-9]{1,}\.[0-9]{1,}' | sort | uniq -c | sort -rn
    reading from file my.log, link-type EN10MB (Ethernet), snapshot length 262144
    dropped privs to tcpdump
          6 IpAddrCentos
    Decision:
    Чем больше подключений для одного IP-адреса, тем больше вероятность, что это какая-то нехорошая деятельность.
    Task:
    IP-адреса, которые вызывают подозрение, можно заблокировать в iptables с помощью команды, указанной ниже.
    Decision:
    [root@centos user]# iptables -A INPUT -s IpAddrUbuntu -j DROP
    [root@centos user]# service iptables save
    [root@centos user]# systemctl restart iptables.service
    Task:
    Системы обнаружения вторжения (Intrusion Detection System, IDS). Установка и настройка tripwire.
    Decision:
    [root@centos user]# dnf -y install epel-releasey
    [root@centos user]# dnf -y install tripwire
    [root@centos user]# tripwire-setup-keyfiles
    [root@centos user]# tripwire --init
    ...
    ### Warning: File system error.
    ### Filename: /proc/pci
    ### Нет такого файла или каталога
    ### Continuing...
    ...
    [root@centos user]# cat /etc/tripwire/twpol.txt
    ...
         /proc/pci                         -> $(Device) ;
    ...
    [root@centos user]# vim /etc/tripwire/twpol.txt
    [root@centos user]# cat /etc/tripwire/twpol.txt
    ...
         #/proc/pci                         -> $(Device) ;
    ...
    [root@centos user]# tripwire --update-policy --secure-mode low /etc/tripwire/twpol.txt
    [root@centos user]# tripwire --check —interactive
    Task:
    Создаем конфигурацию, Создаем базу данных, Проверим на работоспособность
    Decision:
    [root@centos user]# ls /etc/tripwire/
    centos-local.key  site.key  tw.cfg  twcfg.txt  tw.pol  tw.pol.bak  twpol.txt
    [root@centos user]# twadmin -m F -c /etc/tripwire/tw.cfg -S /etc/tripwire/site.key /etc/tripwire/twcfg.txt
    [root@centos user]# vim /etc/tripwire/twpolmake.pl
    [root@centos user]# cat /etc/tripwire/twpolmake.pl
    #!/usr/bin/perl
    # Tripwire Policy File customize tool
    # ----------------------------------------------------------------
    # Copyright (C) 2003 Hiroaki Izumi
    # This program is free software; you can redistribute it and/or
    # modify it under the terms of the GNU General Public License
    # as published by the Free Software Foundation; either version 2
    # of the License, or (at your option) any later version.
    # This program is distributed in the hope that it will be useful,
    # but WITHOUT ANY WARRANTY; without even the implied warranty of
    # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    # GNU General Public License for more details.
    # You should have received a copy of the GNU General Public License
    # along with this program; if not, write to the Free Software
    # Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
    # ----------------------------------------------------------------
    # Usage:
    #     perl twpolmake.pl {Pol file}
    # ----------------------------------------------------------------
    #
    $POLFILE=$ARGV[0];
    open(POL,"$POLFILE") or die "open error: $POLFILE" ;
    my($myhost,$thost) ;
    my($sharp,$tpath,$cond) ;
    my($INRULE) = 0 ;
    while (<POL>) {
        chomp;
        if (($thost) = /^HOSTNAME\s*=\s*(.*)\s*;/) {
            $myhost = `hostname` ; chomp($myhost) ;
            if ($thost ne $myhost) {
                $_="HOSTNAME=\"$myhost\";" ;
            }
        }
        elsif ( /^{/ ) {
            $INRULE=1 ;
        }
        elsif ( /^}/ ) {
            $INRULE=0 ;
        }
        elsif ($INRULE == 1 and ($sharp,$tpath,$cond) = /^(\s*\#?\s*)(\/\S+)\b(\s+->\s+.+)$/) {
            $ret = ($sharp =~ s/\#//g) ;
            if ($tpath eq '/sbin/e2fsadm' ) {
                $cond =~ s/;\s+(tune2fs.*)$/; \#$1/ ;
            }
            if (! -s $tpath) {
                $_ = "$sharp#$tpath$cond" if ($ret == 0) ;
            }
            else {
                $_ = "$sharp$tpath$cond" ;
            }
        }
        print "$_\n" ;
    }
    close(POL) ;
    [root@centos user]# perl /etc/tripwire/twpolmake.pl /etc/tripwire/twpol.txt > /etc/tripwire/twpol.txt.new 
    [root@centos user]# twadmin -m P -c /etc/tripwire/tw.cfg -p /etc/tripwire/tw.pol -S /etc/tripwire/site.key /etc/tripwire/twpol.txt.new
    [root@centos user]# tripwire -m i -s -c /etc/tripwire/tw.cfg
    [root@centos user]# tripwire -m c -s -c /etc/tripwire/tw.cfg
    Open Source Tripwire(R) 2.4.3.7 Integrity Check Report
    Report generated by:          root
    Report created on:            Вс 29 окт 2023 14:26:04
    Database last updated on:     Never
    ...
    ===============================================================================
    Object Summary: 
    ===============================================================================
    -------------------------------------------------------------------------------
    # Section: Unix File System
    -------------------------------------------------------------------------------
    No violations.
    ===============================================================================
    Error Report: 
    ===============================================================================
    ...
    [root@centos user]# ll /var/lib/tripwire/report
    итого 28
    -rw-r--r--. 1 root root  350 окт 29 13:55 centos-20231029-135511.twr
    -rw-r--r--. 1 root root  350 окт 29 13:57 centos-20231029-135712.twr
    -rw-r--r--. 1 root root  350 окт 29 14:00 centos-20231029-140029.twr
    -rw-r--r--. 1 root root  350 окт 29 14:08 centos-20231029-140853.twr
    -rw-r--r--. 1 root root  350 окт 29 14:09 centos-20231029-140919.twr
    -rw-r--r--. 1 root root 6446 окт 29 14:27 centos-20231029-142604.twr
    [root@centos user]# touch /var/lib/tripwire/tfile3.txt
    [root@centos user]# tripwire -m c -s -c /etc/tripwire/tw.cfg
    Open Source Tripwire(R) 2.4.3.7 Integrity Check Report
    ...
    -------------------------------------------------------------------------------
    Rule Name: Tripwire Data Files (/var/lib/tripwire)
    Severity Level: 100
    -------------------------------------------------------------------------------
    Added:
    "/var/lib/tripwire/tfile3.txt"
    ===============================================================================
    Error Report: 
    ===============================================================================
    ...
    Decision:
    Мы создали файл /var/lib/tripwire/tfile3.txt и утилита tripwire показала нам, что мы создали один файл.
    Task:
    Поиск руткитов. Установка RkHunter, обновление БД вирусов, Создадим образ нашей ОС, для того, чтобы RootKit Hunter мог сравнивать текущее состояние команд в системе с созданной базой, сканируем систему
    Decision:
    [root@centos user]# dnf install rkhunter -y
    [root@centos user]# rkhunter --update
    [root@centos user]# cat /etc/rkhunter.conf
    [root@centos user]# rkhunter --propupd
    [root@centos user]# rkhunter --check
    [root@centos user]# cat /var/log/rkhunter/rkhunter.log
    Task:
    добавить выполнение сканирования krootkit в планировщик под пользователем
    Decision:
    [user@centos ~]$ crontab -e
    [user@centos ~]$ crontab -l
    0 14 * * *    /usr/bin/rkhunter --update; /usr/bin/rkhunter -c --createlogfile --cronjob
    Task:
    Сбор информации до и после взлома
    сравнить образ раздела или файла перед взломом с образом из взломанной системы.
    Decision:
    [root@centos user]# dd if=/bin/ls of=ls.dd | md5sum ls.dd > sum.txt
    274+1 записей получено
    274+1 записей отправлено
    140760 байт (141 kB, 137 KiB) скопирован, 0,00362538 s, 38,8 MB/s
    [root@centos user]# cat sum.txt
    0ad8006df0caff12335f45fdf6c7c0f7  ls.dd
    [root@centos user]# file /bin/ls
    /bin/ls: ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=bdfe7bf382f12e8361d590aa148cb3e591f83d30, for GNU/Linux 3.2.0, stripped
    [root@centos user]# stat /bin/ls
      Файл: /bin/ls
      Размер: 140760        Блоков: 280        Блок В/В: 4096   обычный файл
    Устройство: fd00h/64768d        Инода: 33894006    Ссылки: 1
    Доступ: (0755/-rwxr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)
    Контекст: system_u:object_r:bin_t:s0
    Доступ:        2023-10-29 10:10:37.199000000 +0800
    Модифицирован: 2023-01-06 19:42:38.000000000 +0800
    Изменён:       2023-09-28 12:38:12.337575533 +0800
    Создан:        2023-09-28 12:38:12.333575533 +0800
    Task:
    создать список контрольных сумм для популярных файлов, которые обычно могут изменяться при взломе системы
    Decision:
    [root@centos user]# md5sum /bin/ls >> sums.txt
    [root@centos user]# cat sums.txt
    07c62424e4c26afc0d7e393bab60546c  /bin/ls
    Source:
    https://serverspace.ru/support/help/utilita-rsync-dlya-sinkhronizatsii-fajlov/
    https://elatov.github.io/2013/06/migrate-from-libvirt-kvm-to-virtualbox/
    https://www.opennet.ru/tips/1870_qemu_virtualbox_disk.shtml
    https://blog.sedicomm.com/2016/12/16/iptables-ustanovka-i-nastrojka/?ysclid=ln3wplng53988958006#1
    https://www.youtube.com/playlist?list=PLtSGboPf3g50Aejrp6KjQsqqjxvAO4aKw
    https://unixcop.com/how-to-install-and-configure-an-ftp-server-on-centos-9-stream/
    https://losst.pro/kak-otkryt-port-iptables?ysclid=lnvrpgxwj8175390861
    https://techviewleo.com/configure-vsftpd-ftp-server-on-ubuntu-linux/
    https://tokmakov.msk.ru/blog/item/477
    https://stackoverflow.com/questions/38523250/vsftpd-login-is-not-successful
    https://linux-notes.org/fil-tratsiya-mac-ispol-zuya-iptables-v-linux/?ysclid=lo0z3u2sro842609893
    https://ru.linux-console.net/?p=1297&ysclid=lo0tx5owhk496498608
    https://devcoops.com/install-nginx-on-centos-9-stream/
    https://itsecforu.ru/2022/01/14/%F0%9F%90%89-%D0%BD%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B9%D0%BA%D0%B0-http-%D1%81%D0%B5%D1%80%D0%B2%D0%B5%D1%80%D0%B0-kali-linux/?ysclid=lo4exvw84m943299837
    https://hackware.ru/?p=15739
    https://support.mozilla.org/ru/kb/parametry-soedineniya-v-firefox
    https://techviewleo.com/configure-squid-proxy-on-centos-almalinux-rhel/
    https://uzverss.livejournal.com/109496.html?ysclid=lo9hqkxw2s612202120
    https://habr.com/ru/companies/alexhost/articles/531170/
    https://linux-notes.org/ustanovka-i-nastrojka-tripwire-v-centos-redhat-fedora/?ysclid=loazeq8vjc96566460
    https://linux-notes.org/poisk-rutkitov-v-debian-ubuntu-linux-mint-i-red-hat-centos-fedora/?ysclid=lob3rn9gk6547904603
    https://winitpro.ru/index.php/2020/04/21/planirovshhik-zadach-cron-v-linux/
    https://drive.google.com/drive/folders/1SUpqELdGy0O2zEDbmfnx35zViexy5xZL
    https://losst.ru/kak-polzovatsya-nmap-dlya-skanirovaniya-seti
    https://www.server-world.info/en/note?os=CentOS_7&p=tripwire
    https://www.lisenet.com/2017/configure-tripwire-on-centos-7/
    https://habr.com/sandbox/29825/
    https://kamaok.org.ua/?p=813
    https://habr.com/company/first/blog/243487/
    https://habr.com/sandbox/29825/

Организация локальной сети
    Task:
	  Для защиты лабораторных работ по дисциплине Локально-вычислительные сети в виртуальной машине Ubuntu настроил программу для проектирования сетей Packet Tracer, спроектрировал лабораторные работы по темам "Использование DHCP-протокола через маршрутизатор и через сервер", "Wi-Fi - беспроводная передача данных", а также по теме "Локальная сеть" развернул виртуальные машины в Virtualbox две операционные системы Windows 10 и Windows Server 2012 для настройки локальной сети. В Windows Server установил DHCP и DNS сервера и добавил в домен клиентского компьютера Windows 10.
    Task:
    Packet Tracer Install Ubuntu
    Decision:
    $ sudo virsh start ubuntu22.04
    $ ssh -X user@IpAddrUbuntu
    $ google-chrome https://www.google.ru/search?q=cisco+account+create&newwindow=1&source=hp&ei=Z8eqYdOSEcTIrgSIwIeQDQ&iflsig=ALs-wAMAAAAAYarVd9Qob1A5NSTfBETDiQsuP3G5I7gv&ved=0ahUKEwiT4p6Jgsn0AhVEpIsKHQjgAdIQ4dUDCAY&uact=5&oq=cisco+account+create&gs_lcp=Cgdnd3Mtd2l6EAMyBQgAEIAEMgUIABCABDIGCAAQFhAeMgYIABAWEB4yBggAEBYQHjIGCAAQFhAeMgYIABAWEB4yBggAEBYQHjIGCAAQFhAeMgYIABAWEB5QAFgAYPgEaABwAHgAgAGBAYgBgQGSAQMwLjGYAQCgAQKgAQE&sclient=gws-wiz &
    $ google-chrome https://id.cisco.com/signin/register &
    $ google-chrome https://temp-mail.org/ &
    xanaveb196@simdpi.com - https://id.cisco.com/signin/register - mail: xanaveb196@simdpi.com - https://temp-mail.org/ - 
    Активировать Аккаунт - https://id.cisco.com/signin/register - xanaveb196@simdpi.com
    $ google-chrome https://www.google.ru/search?q=netacad+introduction+to+packet+tracer&newwindow=1&ei=Ks-qYZugA5a_rgTF_beoDw&oq=netacad+into+to+pa&gs_lcp=Cgdnd3Mtd2l6EAMYAjIGCAAQFhAeMgYIABAWEB4yBggAEBYQHjoHCAAQRxCwAzoNCC4QxwEQ0QMQsAMQQzoHCAAQsAMQQzoFCAAQgAQ6BQghEKABOgUIABDNAkoFCDwSATJKBAhBGABKBAhGGABQ96cZWMXDGWC44RloAnACeACAAZwBiAHGCpIBBDAuMTCYAQCgAQHIAQrAAQE&sclient=gws-wiz &
    $ google-chrome https://www.netacad.com/portal/resources/packet-tracer &
    $ wget https://www.netacad.com/portal/resources/file/71b29477-0abb-4a52-97eb-cc23d5771574
    $ ls
    CiscoPacketTracer_821_Ubuntu_64bit.deb
    $ sudo dpkg -i CiscoPacketTracer_821_Ubuntu_64bit.deb
    ...
    dpkg: error processing package packettracer (--install):
     dependency problems - leaving unconfigured
    ...
    Errors were encountered while processing:
     packettracer
    $ sudo apt install -f
    $ sudo dpkg -i CiscoPacketTracer_821_Ubuntu_64bit.deb
    $ packettracer &
    Task:
    The simplest network
    Для организации простейшей сети потребуется Два компьютера и Патч-корд. В случае отсутствия патч-корда
    необходимы: Кусок витой пары, Два коннектора RJ45.
    Decision:
    end devices - PC0 - PC1 - connections - copper cross-cover - PC0 - Desktop - ip configuration - ipv4 - 192.168.1.1 - subnet mask - 255.255.255.0 - PC1 - Desktop - ip configuration - ipv4 - 192.168.1.2 - subnet mask - 255.255.255.0 - PC0 - Desktop - Command Prompt
    C:\>ping 192.168.1.2
        Pinging 192.168.1.2 with 32 bytes of data:
        Reply from 192.168.1.2: bytes=32 time<1ms TTL=128
        Reply from 192.168.1.2: bytes=32 time<1ms TTL=128
        Reply from 192.168.1.2: bytes=32 time<1ms TTL=128
        Reply from 192.168.1.2: bytes=32 time<1ms TTL=128
        Ping statistics for 192.168.1.2:
            Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),
        Approximate round trip times in milli-seconds:
            Minimum = 0ms, Maximum = 0ms, Average = 0ms
    Task:
    Используем DHCP-протокол через маршрутизатор: We have one switch and router and several computers, Create a DHCP pool (some kind of ip address space), Exclude certain ip addresses, Настроить компьютеры (Убедимся, что ip адреса автоматически добавляются), Проверить связь, See which computers and which ip addresses were issued, Настроим порты, задать VLAN 2,3 на порты fa0/1,2,3,4, The vlan needs to be pushed to the router, Configure the router's sub-interface for 2,3,4 vlans, Configure the DHCP Server. The Dhcp server is located in a separate segment and a broadcast request from computers 3,4,5,6 in the search for a Dhcp server, the router will not pass. Since the computers are in different segments. It is necessary to forward computer requests to the Dhcp server
    Decision:
    Switch -2960 - PC0 - PC1 - PC2 - Router0 - 1841 - Copper Straight-Through - Router0 - CLI
    Router>en
    Router#conf t
        Enter configuration commands, one per line.  End with CNTL/Z.
    Router(config)#int fa0/0
    Router(config-if)#no sh
    Router(config-if)#no shutdown
    Router(config-if)#
        %LINK-5-CHANGED: Interface FastEthernet0/0, changed state to up
        %LINEPROTO-5-UPDOWN: Line protocol on Interface FastEthernet0/0, changed state to up
    Router(config-if)#ip add
    Router(config-if)#ip address 192.168.1.1 255.255.255.0
    Router(config-if)#exit
    Router(config)#ip dhcp pool DHCP
    Router(dhcp-config)#network 192.168.1.0 255.255.255.0
    Router(dhcp-config)#defa
    Router(dhcp-config)#default-router 192.168.1.1
    Router(dhcp-config)#dns-server 8.8.8.8
    Router(dhcp-config)#exit
    Router(config)#ip dhcp  excluded-address 192.168.1.100
    Router(config)#ip dhcp  excluded-address 192.168.1.1
    Router(config)#exit
    Router#wr mem
    PC0,1,2 - Desktop - ip configuration - +DHCP - PC0 - Desktop - command prompt
    C:\>ping 192.168.1.1
    ...
    Ping statistics for 192.168.1.1:
    	Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),
    Approximate round trip times in milli-seconds:
    	Minimum = 0ms, Maximum = 11ms, Average = 2ms
    C:\>ping 192.168.1.3
    ...
    Ping statistics for 192.168.1.3:
    	Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),
    Approximate round trip times in milli-seconds:
    	Minimum = 0ms, Maximum = 1ms, Average = 0ms
    C:\>ping 192.168.1.4
    ...
    Ping statistics for 192.168.1.4:
    	Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),
    Approximate round trip times in milli-seconds:
    	Minimum = 0ms, Maximum = 0ms, Average = 0ms
    Router0 - CLI
    Router#show ip dhcp binding
    IP address       Client-ID/              Lease expiration        Type
                    Hardware address
    192.168.1.2      0090.21EC.999A           --                     Automatic
    192.168.1.3      0060.3E23.49DD           --                     Automatic
    192.168.1.4      0010.1109.2ACD           --                     Automatic
    Task:
    Используем DHCP-протокол через сервер: We have 3 different network segments (Vlan2,3,4) and a dedicated dhcp server, Segmenting our network, Create a VLAN, 
    Decision:
    Switch - 2960 - PC3,4,5 - Router1 - 1841 - Sever0 - Copper Straight-Through - switch1 - cli
    Switch>en
    Switch#conf t
    Switch(config)#vlan 2
    Switch(config-vlan)#name VLAN2
    Switch(config-vlan)#exit
    Switch(config)#vlan 3
    Switch(config-vlan)#name VLAN3
    Switch(config-vlan)#exit
    Switch(config)#vlan 4
    Switch(config-vlan)#name DHCP
    Switch(config-vlan)#exit
    Switch(config)#int range fa
    Switch(config)#int range fastEthernet 0/1-2
    Switch(config-if-range)#swi
    Switch(config-if-range)#switchport mode
    Switch(config-if-range)#switchport mode acc
    Switch(config-if-range)#switchport mode access
    Switch(config-if-range)#swi
    Switch(config-if-range)#switchport acc
    Switch(config-if-range)#switchport access vla
    Switch(config-if-range)#switchport access vlan 2
    Switch(config-if-range)#exit
    Switch(config)#int range fastEthernet 0/3-4
    Switch(config-if-range)#switchport mode access
    Switch(config-if-range)#switchport access vlan 3
    Switch(config-if-range)#exit
    Switch(config)#int range fastEthernet 0/5
    Switch(config-if-range)#switchport mode access
    Switch(config-if-range)#switchport access vlan 4
    Switch(config-if-range)#exit
    Switch(config)#int fa
    Switch(config)#int fastEthernet 0/6
    Switch(config-if)#switchport mode trunk
    Switch(config-if)#swi
    Switch(config-if)#switchport tr
    Switch(config-if)#switchport trunk all
    Switch(config-if)#switchport trunk allowed vla
    Switch(config-if)#switchport trunk allowed vlan 2,3,4
    Switch(config-if)#exit
    Switch(config)#end
    Switch#wr mem
    Switch#show run
        Building configuration...
        Current configuration : 1388 bytes
        !
        version 15.0
        no service timestamps log datetime msec
        no service timestamps debug datetime msec
        no service password-encryption
        !
        hostname Switch
        !
        !
        !
        !
        !
        !
        spanning-tree mode pvst
        spanning-tree extend system-id
        !
        interface FastEthernet0/1
         switchport access vlan 2
         switchport mode access
        !
        interface FastEthernet0/2
         switchport access vlan 2
         switchport mode access
        !
        interface FastEthernet0/3
         switchport access vlan 3
         switchport mode access
        !
        interface FastEthernet0/4
         switchport access vlan 3
         switchport mode access
        !
        interface FastEthernet0/5
         switchport access vlan 4
         switchport mode access
        !
        interface FastEthernet0/6
         switchport trunk allowed vlan 2-4
         switchport mode trunk
    router1 - cli
    Router>en
    Router#conf t
    Router(config)#int g
    Router(config)#int gigabitEthernet 0/0.2
    Router(config-subif)#enc
    Router(config-subif)#encapsulation do
    Router(config-subif)#encapsulation dot1Q 2
    Router(config-subif)#ip
    Router(config-subif)#ip add
    Router(config-subif)#ip address 192.168.2.1 255.255.255.0
    Router(config-subif)#no sh
    Router(config-subif)#no shutdown
    Router(config-subif)#exit
    Router(config)#int gigabitEthernet 0/0
    Router(config-if)#no shutdown
    Router(config-if)#
        %LINK-5-CHANGED: Interface GigabitEthernet0/0, changed state to up
        %LINEPROTO-5-UPDOWN: Line protocol on Interface GigabitEthernet0/0, changed state to up
        %LINK-5-CHANGED: Interface GigabitEthernet0/0.2, changed state to up
        %LINEPROTO-5-UPDOWN: Line protocol on Interface GigabitEthernet0/0.2, changed state to up
    Router(config-if)#exit
    Router(config)#int gigabitEthernet 0/0.3
    Router(config-subif)#
        %LINK-5-CHANGED: Interface GigabitEthernet0/0.3, changed state to up
        %LINEPROTO-5-UPDOWN: Line protocol on Interface GigabitEthernet0/0.3, changed state to up
    Router(config-subif)#encapsulation dot1Q 3
    Router(config-subif)#ip address 192.168.3.1 255.255.255.0
    Router(config-subif)#no shutdown
    Router(config-subif)#exit
    Router(config)#int gigabitEthernet 0/0.4
    Router(config-subif)#
        %LINK-5-CHANGED: Interface GigabitEthernet0/0.4, changed state to up
        %LINEPROTO-5-UPDOWN: Line protocol on Interface GigabitEthernet0/0.4, changed state to up
    Router(config-subif)#encapsulation dot1Q 4
    Router(config-subif)#ip address 192.168.4.1 255.255.255.0
    Router(config-subif)#no shutdown
    Router(config-subif)#end
    Router#
    Router#wr mem
    Router#show run
        Building configuration...
        Current configuration : 882 bytes
        !
        version 15.1
        no service timestamps log datetime msec
        no service timestamps debug datetime msec
        no service password-encryption
        !
        hostname Router
        !
        !
        !
        !
        !
        !
        !
        !
        ip cef
        no ipv6 cef
        !
        !
        !
        !
        license udi pid CISCO2901/K9 sn FTX1524I7AI-
        !
        !
        !
        !
        !
        !
        !
        !
        !
        !
        !
        spanning-tree mode pvst
        !
        !
        !
        !
        !
        !
        interface GigabitEthernet0/0
         no ip address
         duplex auto
         speed auto
        !
        interface GigabitEthernet0/0.2
         encapsulation dot1Q 2
         ip address 192.168.2.1 255.255.255.0
        !
        interface GigabitEthernet0/0.3
         encapsulation dot1Q 3
         ip address 192.168.3.1 255.255.255.0
        !
        interface GigabitEthernet0/0.4
         encapsulation dot1Q 4
         ip address 192.168.4.1 255.255.255.0
        !
        interface GigabitEthernet0/1
         no ip address
         duplex auto
         speed auto
         shutdown
        !
    server0 - dekstop - ip configuration - ip address 192.168.4.2 - subnet mask 255.255.255.0 - default gateway 192.168.4.1 - command prompt
    C:\>ping 192.168.4.1
        ...
        Ping statistics for 192.168.4.1:
            Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),
        Approximate round trip times in milli-seconds:
            Minimum = 0ms, Maximum = 6ms, Average = 1ms
        services - dhcp - убеждаемся, что видим один дефолтный server pool - pool name DCHP-VLAN2 - start ip address 192.168.2.0 - default gateway 192.168.2.1 - dns server - 8.8.8.8 - service +On - Add - тоже самое для 3тьего vlan
    Router1 - ClI
    Router>en
    Router#conf t
    Router(config)#int g
    Router(config)#int gigabitEthernet 0/0.2
    Router(config-subif)#ip hel
    Router(config-subif)#ip help
    Router(config-subif)#ip helper-address 192.168.4.2
    Router(config-subif)#exit
    Router(config)#int gigabitEthernet 0/0.3
    Router(config-subif)#ip helper-address 192.168.4.2
    Router(config-subif)#end
    Router#wr mem
    PC3,4,5,6 - desktop - ip configuration - +dhcp - Убеждаемтся в том, что dhcp автоматически раздал адреса - PC6 - command prompt
    C:\>ping 192.168.3.1
        ...
        Ping statistics for 192.168.3.1:
            Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),
        Approximate round trip times in milli-seconds:
            Minimum = 0ms, Maximum = 2ms, Average = 1ms
    C:\>ping 192.168.2.2
        ...
        Ping statistics for 192.168.2.2:
            Packets: Sent = 4, Received = 3, Lost = 1 (25% loss),
        Approximate round trip times in milli-seconds:
            Minimum = 0ms, Maximum = 1ms, Average = 0ms
    Task:
    Wifi: setting up a Wi fi router, Add a custom device laptop, add another stationary computer. assume that the organization already has some kind of network. we need to deploy wi-fi to our organization. we have a guest room of the organization. make sure that the wi-fi network is in a separate segment. On the port that connects to our router1 router, you need to add this vlan4 to trunkport. We need to create a sub interface on our router. Set the possibility to distribute addresses. Add some kind of wireless device, that is, let's assume that a client of the organization came to us and he has a laptop that wants to connect to wifi. Что получить ноутбуку ip адрес wifi нужно определить точку доступа к vlan.
    Необходимо донастроить nat.    
    Decision:
    Router0-CLI
    Router>en
    Router#conf t
    Router(config)#int fa
    Router(config)#int fastEthernet 0/0
    Router(config-if)#ip add
    Router(config-if)#ip address 210.210.0.1 255.255.255.252
    Router(config-if)#no sh
    Router(config-if)#no shutdown
    Router(config-if)#
        %LINK-5-CHANGED: Interface FastEthernet0/0, changed state to up
        %LINEPROTO-5-UPDOWN: Line protocol on Interface FastEthernet0/0, changed state to up
    Router(config-if)#end
    Router#wr mem
    Wireless router 0 - gui - static ip - ip address: 210.210.0.2 - subnet mask: 255.255.255.252 - gateway: 210.210.0.1 - save setting - wireless - network name ssid: netskills-home - save setting - wireless security - wpa2 personal - passphase: ciscocisco - save setting
    lalptop0 - выключаем - убираем (перетаскиваем) подключенный сетевой модуль - заместо него wrc300n - включаем ноутбук - desktop - pc wireless - connect - refresh - netskills-home - connect - pre-shared key - ciscocisco - command promprt
    C:\>ipconfig
        Bluetooth Connection:(default port)
           Connection-specific DNS Suffix..:
           Link-local IPv6 Address.........: ::
           IPv6 Address....................: ::
           IPv4 Address....................: 0.0.0.0
           Subnet Mask.....................: 0.0.0.0
           Default Gateway.................: ::
                                             0.0.0.0
        Wireless0 Connection:
           Connection-specific DNS Suffix..:
           Link-local IPv6 Address.........: FE80::202:4AFF:FE66:D186
           IPv6 Address....................: ::
           IPv4 Address....................: 192.168.0.100
           Subnet Mask.....................: 255.255.255.0
           Default Gateway.................: ::
                                             192.168.0.1
    C:\>ping 192.168.0.1
        ...
        Ping statistics for 192.168.0.1:
            Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),
        Approximate round trip times in milli-seconds:
            Minimum = 23ms, Maximum = 122ms, Average = 56ms
    C:\>ping 210.210.0.1
        ...
        Ping statistics for 210.210.0.1:
            Packets: Sent = 4, Received = 3, Lost = 1 (25% loss),
        Approximate round trip times in milli-seconds:
            Minimum = 17ms, Maximum = 32ms, Average = 26ms
    pc0 - ip conf - dbhcp - command prompt
    C:\>ping 192.168.0.1
        ...
        Ping statistics for 192.168.0.1:
            Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),
        Approximate round trip times in milli-seconds:
            Minimum = 0ms, Maximum = 1ms, Average = 0ms
    C:\>ping 210.210.0.1
        ...
        Ping statistics for 210.210.0.1:
            Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),
        Approximate round trip times in milli-seconds:
            Minimum = 0ms, Maximum = 1ms, Average = 0ms
    C:\>ping 192.168.0.100
        ...
        Ping statistics for 192.168.0.100:
            Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),
        Approximate round trip times in milli-seconds:
            Minimum = 11ms, Maximum = 158ms, Average = 62ms
    Switch0 - CLI
    Switch>en
    Switch#show run
        Building configuration...
        Current configuration : 1436 bytes
        !
        version 12.2
        no service timestamps log datetime msec
        no service timestamps debug datetime msec
        no service password-encryption
        !
        hostname Switch
        !
        !
        !
        !
        !
        !
        spanning-tree mode pvst
        spanning-tree extend system-id
        !
        interface FastEthernet0/1
         description ToRouter
         switchport trunk allowed vlan 2-4
         switchport mode trunk
        !
        interface FastEthernet0/2
         description Users
         switchport access vlan 2
         switchport mode access
        !
        interface FastEthernet0/3
         description Users
         switchport access vlan 2
         switchport mode access
        !
        interface FastEthernet0/4
         description srv
         switchport access vlan 3
         switchport mode access
        !
        interface FastEthernet0/5
         description WiFi-AP
         switchport access vlan 4
         switchport mode access
        !
        interface FastEthernet0/6
        !

        router1-cli
    Switch>en
    Switch#show run
        Building configuration...
        Current configuration : 1436 bytes
        !
        version 12.2
        no service timestamps log datetime msec
        no service timestamps debug datetime msec
        no service password-encryption
        !
        hostname Switch
        !
        !
        !
        !
        !
        !
        spanning-tree mode pvst
        spanning-tree extend system-id
        !
        interface FastEthernet0/1
         description ToRouter
         switchport trunk allowed vlan 2-4
         switchport mode trunk
        !
        interface FastEthernet0/2
         description Users
         switchport access vlan 2
         switchport mode access
        !
        interface FastEthernet0/3
         description Users
         switchport access vlan 2
         switchport mode access
        !
        interface FastEthernet0/4
         description srv
         switchport access vlan 3
         switchport mode access
        !
        interface FastEthernet0/5
         description WiFi-AP
         switchport access vlan 4
         switchport mode access
        !
        interface FastEthernet0/6
        !
    access point0 - config - port1 - ssid: netskill - wpa2-psk - pass phrase: ciscocisco
    switch-cli
    #en
    # conf t
    # vlan 4
    # name wifi
    # end
    #show run
        ...
        switchport trunk allowed vlan 2-4
    router1 - cli
    #en
    # conf t
    # int fa0/1.4
    # encapsulation dot1Q 4
    # ip address 192.168.4.1 255.255.255.0
    # no shutdown
    # end
    # conf t
    # ip dhcp pool Wifi-pool
    # network 192.168.4.0 255.255.255.0
    # default-router 192.168.4.1
    # exit
    # ip dhcp excluded-address 192.168.4.1
    # end
    # wr mem
    Laptop1 - выключаем - убираем (перетаскиваем) подключенный сетевой модуль - заместо него wrc300n - включаем ноутбук - desktop - pc wireless - connect - refresh - netskills - save setting - wireless security - wpa2 personal - passphase: ciscocisco - save setting - command prompt
    switch0-cli
    #en
    # conf t
    # switchport mode access
    # switchport access vlan 4
    # description WiFi-AP
    # end
    # wr mem
    Router1-cli
    # en
    # conf t
    # ip access-list standart FOR-NAT
    # permit 192.168.4.0  0.0.0.255
    # exit
    # int fa0/1.4
    # ip nat inside
    # end
    # wr mem
    lalptop1 - ip configuration - dhcp - ok - command prompt
    C:\>ipconfig
        Bluetooth Connection:(default port)
           Connection-specific DNS Suffix..:
           Link-local IPv6 Address.........: ::
           IPv6 Address....................: ::
           IPv4 Address....................: 0.0.0.0
           Subnet Mask.....................: 0.0.0.0
           Default Gateway.................: ::
                                             0.0.0.0
        Wireless0 Connection:
           Connection-specific DNS Suffix..:
           Link-local IPv6 Address.........: FE80::203:E4FF:FE34:D1E7
           IPv6 Address....................: ::
           IPv4 Address....................: 192.168.4.2
           Subnet Mask.....................: 255.255.255.0
           Default Gateway.................: ::
                                             192.168.4.1
    C:\>ping 192.168.4.1
        ...
        Ping statistics for 192.168.4.1:
            Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),
        Approximate round trip times in milli-seconds:
            Minimum = 48ms, Maximum = 171ms, Average = 96ms
    C:\>ping 192.168.2.3
        ...
        Ping statistics for 192.168.2.3:
            Packets: Sent = 4, Received = 3, Lost = 1 (25% loss),
        Approximate round trip times in milli-seconds:
            Minimum = 27ms, Maximum = 60ms, Average = 39ms
    C:\>ping 210.210.0.1
        ...
        Ping statistics for 210.210.0.1:
            Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),
        Approximate round trip times in milli-seconds:
            Minimum = 38ms, Maximum = 69ms, Average = 52ms
    Task:
    Install VirtualBox
    Decision:
    $ sudo dnf config-manager --add-repo=https://download.virtualbox.org/virtualbox/rpm/el/virtualbox.repo
    $ sudo rpm --import https://www.virtualbox.org/download/oracle_vbox.asc
    $ sudo dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm
    $ sudo dnf install binutils kernel-devel kernel-headers libgomp make patch gcc glibc-headers glibc-devel dkm
    $ sudo dnf search virtualbox
    $ sudo dnf install VirtualBox-7.0
    $ sudo usermod -aG vboxusers $USER
    $ newgrp vboxusers
    $ wget https://download.virtualbox.org/virtualbox/7.0.10/Oracle_VM_VirtualBox_Extension_Pack-7.0.10-158379.vbox-extpack
    $ sudo VBoxManage extpack install Oracle_VM_VirtualBox_Extension_Pack-7.0.10-158379.vbox-extpack
    Task:
    Использовать Virtualbox
    Decision:
    $ VBoxManage list ostypes
    ...
    ID:          Windows2012_64
    Description: Windows 2012 (64-bit)
    Family ID:   Windows
    Family Desc: Microsoft Windows
    64 bit:      true
    ...
    $ VBoxManage createvm --name "Windows2012" --ostype "Windows2012_64" --register --basefolder /vboximages/
    $ vboxmanage modifyvm Windows2012 --memory 2048 --vram 128
    $ vboxmanage modifyvm Windows2012 --nic1 bridged
    $ vboxmanage createhd --filename /vboximages/Windows2012/Windows2012_DISK.vdi --size 75000 --format VDI 
    $ vboxmanage storagectl Windows2012 --name "SATA Controller" --add sata --controller IntelAhci
    $ vboxmanage storageattach Windows2012 --storagectl "SATA Controller" --port 0 --device 0 --type hdd --medium /vboximages/Windows2012/Windows2012_DISK.vdi
    $ vboxmanage storagectl Windows2012 --name "IDE Controller" --add ide --controller PIIX4
    $ vboxmanage storageattach Windows2012 --storagectl "IDE Controller" --port 1 --device 0 --type dvddrive --medium /iso/Server2012.iso
    $ vboxmanage modifyvm Windows2012 --boot1 dvd --boot2 disk --boot3 none --boot4 none
    $ vboxmanage modifyvm Windows2012 --vrde on
    $ vboxmanage modifyvm Windows2012 --vrdemulticon on --vrdeport 10001
    $ vboxmanage modifyvm Windows2012 --graphicscontroller vboxsvga
    $ vboxmanage sharedfolder add Windows2012 --name "Загрузки" --hostpath /Загрузки/ --automount
    $ vboxmanage startvm --type gui Windows2012 &
    Task:
    Установить DHCP и DNS сервера, добавить в домен клиентского компьютера, и проверить их работоспособность. Делать это все мы будем в виртуальной машине KVM с установленными операционными системами Windows Server 2019 и Windows 10. В KVM когда настраиваем сеть операционных систем, нужно подключить внутреннюю сеть и в неразборчивом режиме разрешить все. После чего для настройки сервера запускаем Windows Server 2019
    Decision:
    Диспетчер серверов - Локальная сеть - ipv4 - уберем галочку ipv6 - нажмем на ipv4 - настроить - прописать адреса: 
    ip-адрес 192.168.0.1
    маска подсети 255.255.255.0
    Основной шлюз 192.168.0.1
    Предпочитаемый Днс-сервер 8.8.8.8
    Диспетчер серверов - Локальная сеть - имя компьютера - изменить (например, Server) - перезагрузиться
    Диспетчер серверов - Локальная сеть - управление - добавить роли и компоненты - тип установки - установка ролей и компонентов - выбор сервера - видим наш сервер (Server) и выбираем его - роли сервера ставим флажок на DNS-сервер и доменные службы Active Directory - компоненты - Windows Powershell -	подтверждаем - ставим галочку на автоматический перезапуск - устанавливаем
	Диспетчер серверов - Локальная сеть - видим восклицательный знак -	повысить роли и компоненты - в конфигурации развертывания добавить новый лес - даем имя корневого раздела (на моем примере - server.local) - придумываем пароль, имя домена - устанавливаем
	Диспетчер серверов - Локальная сеть - Средства - DNS - в зоне обратного просмотра правой кнопкой мыши создаем новую зону - добавляем идентификатор сети 192.168.0. 
    Сеть и интернет - центр управления сетями - дополнительные параметры - +включить сетевые обнаружения - +включить общий доступ к файлам и принтерам - вернемся обратно и снова зайдем в дополнительные парметры, увидиим что сохранения не изменинились. Для этого нужно зайти в диспетчер задач - службы - находим Публикация ресурсов обнаружения функции, Обнаружение SSDP - тип запуска у всех Автоматически - запускаем - Сеть и интернет - центр управления сетями - дополнительные параметры - оставляем флажки на "включить сетевые обнаружения" и "включить общий доступ к файлам и принтерам"
    Диспетчер серверов - Локальная сеть - Управление - добавить роли и компоненты - тип установки - установка ролей и компонентов - роли сервера - добавим флажок на DHCP-сервер - подтверждаем - ставим галочку на автоматический перезапуск - устанавливаем - по восклицательному знаку в разделе Локальная сеть - настраиваем DHCP - имя пользователя - добавляем то самое имя учетной записи, с которой мы все это настраиваем (в моем случае - Администратор) - фиксируем 
    Диспетчер серверов - Локальная сеть - средства - DHCP - IPv4 правой кнопкой мыши создаем область - пишем имя области (в моем случае - Server) - добавляем диапозон айпи адресов и маску:
    Начальный IP адрес 192.168.0.10
    Конечный IP адрес 192.168.0.20
    Длина 24
    маска подсети 255.255.255.0
    срок действия аренды я выбрал 7 дней - добавляем айпи адрес маршрутизатора 192.168.0.1. В DHCP - IPv4 - область 192.168.0.0 - пул ардесов - должна появиться инфа по DHCP
    панельуправлени - система безопасности - брандмауер - включение и отключение брандмауера
    $ sudo virsh start Windows
    $ virt-viewer --connect qemu:///system --wait Windows
    свойства компьютера - изменить настройки,изменить - ставим флажок на добавление домена - вводим в домен (в моем случае это server.local) - вводим данные сервера (имя и пароль учетки сервера) - После перезагрузки можем спокойно работать в домене.
    Task:
    Перенос виртуальной машины VirtualBox в KVM
    Decision:
    $ vboxmanage list hdds
    UUID:           90d93e42-c567-4ff4-8ab8-5a21971604dc
    Parent UUID:    base
    State:          created
    Type:           normal (base)
    Location:       /vboximages/Windows2012/Windows2012_DISK.vdi
    Storage format: VDI
    Capacity:       75000 MBytes
    Encryption:     disabled
    $ VBoxManage clonehd /vboximages/Windows2012/Snapshots/\{41cf81da-b2f5-4dfe-9afb-e5c0d5f0d301\}.vdi /vboximages/Windows2012/Windows12static.vdi --format VDI --variant Fixed
    $ sudo qemu-img convert -f vdi /vboximages/Windows2012/Windows12static.vdi -O qcow2 /images/Windows12.qcow2
    $ sudo virt-install --osinfo list
    ...
    win2k12r2
    ...
    $ sudo virt-install \
    --name Windows12 \
    --ram 2048 \
    --vcpus=2 \
    --import \
    --disk path=/images/Windows12.qcow2,format=qcow2 \
    --vnc \
    --noautoconsole \
    --os-variant win2k12r2 \
    --accelerate \
    --network bridge=br0
    $ virt-viewer --connect qemu:///system --wait Windows12
    Decision:
    Вернемся к серверу - меню пуск - администрирование - DHCP-сервер - IPv4 - область - арендованные адреса видим наш клиентский компьютер
    $ sudo virsh shutdown Windows
    $ vboxmanage controlvm Windows12 poweroff soft
    Source:
    https://www.youtube.com/watch?v=04VpVYO7F78
    https://www.youtube.com/watch?v=kg5CalaPOLA
    https://www.youtube.com/playlist?list=PLcDkQ2Au8aVNYsqGsxRQxYyQijILa94T9
    https://www.linuxhowto.net/how-to-install-virtualbox-on-rhel-9-step-by-step/?ysclid=ln2gkjl488389400056
    https://www.dmosk.ru/miniinstruktions.php?mini=virtualbox-install
    https://devminz.github.io/posts/devops/virtualbox-cli-vm-bridged-networking/
    https://urn.su/dvps/virtualbox/vboxmanage/?ysclid=ln6wyljfpr336059458
    https://urn.su/dvps/virtualbox/vboxmanage/man/?ysclid=ln6xc4orjl822339529
    https://askubuntu.com/questions/161759/how-to-access-a-shared-folder-in-virtualbox
    https://superuser.com/questions/116946/how-do-i-press-ctrlaltdelete-in-virtualbox
    https://www.youtube.com/playlist?list=PL90TRL5CLgV68UQJZIWU_aoQhQGSXZJiS
    https://obu4alka.ru/perenos-virtualnoj-mashiny-virtualbox-v-kvm.html
    https://doc.s-terra.ru/rh_output/4.2/Virt_gate/output/mergedProjects/KVM/%D0%98%D0%BC%D0%BF%D0%BE%D1%80%D1%82_%D0%B2_KVM.htm

Использование данных с одночастотных приемников спутниковых радионавигационных систем для коррекции модели ионосферы
    Task:
    Для защиты диссертации по теме "Использование данных с одночастотных приемников спутниковых радионавигационных систем для коррекции модели ионосферы" освоил технологию приёма получения данных с одночастотных приемников спутниковых радионавигационных систем, получил данные, разработал программу на C++, которая обрабатывает и сортирует данные двух координат из файла по столбцам, рисует график, чтобы увидеть желаемый результат в точности определения координат спутников, рассмотрел способы уменьшения ошибок измерения псевдодальности и показал, что из-за нестабильности аппаратуры потребителя информация о состоянии ионосферы может быть получена в каждый момент времени по разностям ПД двух навигационных спутников
    Task:
    Установка виртуальной машины Windows в Qemu-Kvm
    Decision:
    # virt-install \
    --name Windows \
    --ram 2048 \
    --disk path=/ПУТЬ/Windows.img,size=75 \
    --vcpus 2 \
    --os-variant Windows \
    --network bridge=br0 \
    --graphics none \
    --console pty,target_type=serial \
    --location /var/lib/libvirt/iso/Windows.iso \
    --extra-args 'console=ttyS0,115200n8'
    $ sudo virsh list --all
     ID   Имя           Состояние
    -------------------------------
    ...
     -    Windows       выключен
    $ sudo virsh start Windows
    Task:
    Настройка удаленного доступа к виртуальной машине для передачи файлов
    Decision:
    Панель управления - Система - Настройка удаленного доступа - +разрешить подключения удаленного помощника к этому компьютеру - +разрешить удаленные подключения к этому компьютеру - -Разрешить подключение только с компьютеров, ...
    $ sudo dnf install rdesktop
    $ rdesktop IP -u tuser -p tpassword -g 1400x900
    PS C:\Windows\system32> Add-WindowsCapability -Online -Name OpenSSH.Client~~~~0.0.1.0
    PS C:\Windows\system32> Add-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0
    PS C:\Windows\system32> Get-WindowsCapability -Online | Where-Object Name -like 'OpenSSH*'
    Name  : OpenSSH.Client~~~~0.0.1.0
    State : Installed
    Name  : OpenSSH.Server~~~~0.0.1.0
    State : Installed
    PS C:\Windows\system32> Start-Service sshd
    PS C:\Windows\system32> Set-Service -Name sshd -StartupType 'Automatic'
    PS C:\Windows\System32\OpenSSH> notepad.exe C:\ProgramData\ssh\sshd_config
    tuser@KVM-TWINDOWS C:\Users\tuser>type C:\ProgramData\ssh\sshd_config
    ...
    PasswordAuthentication yes
    ...
    # override default of no subsystems
    Subsystem    sftp    sftp-server.exe
    ...
    Match User tuser
        ChrootDirectory C:\Users\tuser\Documents
    #    ForceCommand internal-sftp
        X11Forwarding yes
    ...
    PS C:\Windows\System32\OpenSSH> Restart-Service sshd
    $ ssh tuser@IP
    tuser@KVM-TWINDOWS C:\Users\tuser>powershell
    PS C:\Users\tuser> Start-Process powershell -Verb runAs
    PS C:\Users\tuser> exit
    $ ls
    DataProcessingInCpp  Drivers  ReceivingDataFromSatellite
    $ sftp tuser@IP
    sftp> put -r ReceivingDataFromSatellite
    sftp> put -r DataProcessingInCpp
    sftp> ls
    DataProcessingInCpp                                                             
    ReceivingDataFromSatellite
    sftp> exit
    $ rdesktop IP -u tuser -p tpassword -g 1400x900 &
    Task:
    Получение данных с одночастотного приемника СРНС
    Decision:
    Для проверки способа определения вариации ионосферной задержки 19 мая 2015 года провел обработку данных измерения навигационных сигналов от нескольких НС. Для измерений использовался 20 канальный  одночастотный  приемник BU-353 фирмы  GlobalSat. Этот приемник предназначен для ГНСС GPS, работает на частоте L1 по C/A коду и имеет высокую чувствительность 159 дБм. Данные о псевдодальностях доступны только при использовании бинарного протокола, поэтому при наблюдениях применялась программа SirfTech версии 2.20 и данные записывались в RINEX файл.
    Task:
    Установка Borland C++ Builder под Windows
    Decision:
    $ ssh tuser@IP
    tuser@KVM-TWINDOWS C:\Users\tuser>powershell
    PS C:\users\tuser\documents> Mount-DiskImage -ImagePath "C:\users\tuser\documents\DataProcessingInCpp\Borland C++ Builder 6.0 Enterprise [cd1].iso"
    zPS C:\users\tuser\documents> Get-PSDrive -PSProvider FileSystem
    Name           Used (GB)     Free (GB) Provider      Root                                                                                               CurrentLocation
    ----           ---------     --------- --------      ----                                                                                               ---------------
    C                  25,27         49,16 FileSystem    C:\                                                                                          users\tuser\documents
    D                   4,32          0,00 FileSystem    D:\
    E                                      FileSystem    E:\
    F                   0,61          0,00 FileSystem    F:\
    PS C:\users\tuser\documents> ls F:\
        Каталог: F:\
    Mode                 LastWriteTime         Length Name
    ----                 -------------         ------ ----
    d-----        01.02.2002     17:00                Install
    --r---        01.02.2002     17:00          29550 license.txt
    --r---        01.02.2002     17:00          54878 readme.rtf
    --r---        01.02.2002     17:00          33959 readme.txt
    --r---        01.02.2002     17:00          29550 license.txt
    --r---        01.02.2002     17:00          54878 readme.rtf
    --r---        01.02.2002     17:00          33959 readme.txt
    --r---        01.02.2002     17:00           5337 remote.rtf
    --r---        01.02.2002     17:00           1750 remote.txt
    --r---        01.02.2002     17:00           1750 install.exe
    Запустим от администратора install.exe - выбираем С++ Builder 6 - нужно ввести сгенерированный ключ программы, зайдя в папку Borland - запустить генератор ключей Borland KeyGen - копируем - вставляем серийный номер - ключ программы в установщик
    PS C:\users\tuser\documents> Dismount-DiskImage -ImagePath "C:\users\tuser\documents\DataProcessingInCpp\Borland C++ Builder 6.0 Enterprise [cd1].iso"
    PS C:\users\tuser\documents> Mount-DiskImage -ImagePath "C:\users\tuser\documents\DataProcessingInCpp\Borland C++ Builder 6.0 Enterprise [cd2].iso"
    Ок - finish - NO (Ни в коем случае не перезагружаться)
    PS C:\users\tuser\documents> Dismount-DiskImage -ImagePath "C:\users\tuser\documents\DataProcessingInCpp\Borland C++ Builder 6.0 Enterprise [cd2].iso"
    C:\Programms file (x86)\Bornald\Builder\Bin\ - свойства совместимости bcb.exe - +всегда выполнять от администратора это программу - Запускаем bcb.exe - +регистрируем ее с помощью генератора ключей Borland KeyGe - копировать - вставить - перезагрузиться
    Task:
    Мы хотим чтобы наша программа с графическим интерфейсом с нами поздоровалась в Bornald Builder
    Decision:
    //---------------------------------------------------------------------------
    #include <vcl.h>
    #pragma hdrstop
    #include "Unit1.h"
    //---------------------------------------------------------------------------
    #pragma package(smart_init)
    #pragma resource "*.dfm"
    TForm1 *Form1;
    //---------------------------------------------------------------------------
    __fastcall TForm1::TForm1(TComponent* Owner)
            : TForm(Owner)
    {
    }
    //---------------------------------------------------------------------------
    void __fastcall TForm1::Button1Click(TObject *Sender)
    {
            ShowMessage("Hello world!");
    }
    //---------------------------------------------------------------------------
    Task:
    Для дальнейшей обработки и сортировки данных мною была написана программа на языке С++, которая считывает rinex файлы в текстовом формате и сортирует псевдодальности по каждому НС, что упрощает дальнейшую обработку.
    Task:
    Попробуем в файле irkj1380.txt вывести всю информацию до END OF HEADER в out.txt.
    Decision:
         2              OBSERVATION DATA    M (MIXED)           RINEX VERSION / TYPE
    CCRINEXO V2.4.1 LH  imvp                19-MAY-15 06:30     PGM / RUN BY / DATE
    no comments                                                 COMMENT
    JPS2RIN 1.40 LNX    IMVP                18-MAY-15 01:08     COMMENT
    BUILD MAY 14 2009 (C) TOPCON POSITIONING SYSTEMS            COMMENT
    IRKJ40190000.JPS                                            COMMENT
    SE TPS 00000000                                             COMMENT
    IRKJ                                                        MARKER NAME
    12313M002                                                   MARKER NUMBER
                        VS NIIFTRI, IRKUTSK                     OBSERVER / AGENCY
    00517               JPS LEGACY          2.6.0 OCT,24,2007   REC # / TYPE / VERS
    RA0225              JPSREGANT_SD_E1 NONE                    ANT # / TYPE
      -968328.4915  3794423.9665  5018165.4637                  APPROX POSITION XYZ
            0.1280        0.0000        0.0000                  ANTENNA: DELTA H/E/N
         1     1                                                WAVELENGTH FACT L1/2
         7    C1    P1    P2    L1    L2    D1    D2            # / TYPES OF OBSERV
        30                                                      INTERVAL
      2015     5    18     0     0    0.000000      GPS         TIME OF FIRST OBS
                                                                END OF HEADER
    Task:
    Вывести следующую строку после END OF HEADER
    Decision:
    15  5 18  0  0  0.0000000  1 19G01G04G11G12G14G18G22G24G25G31G32R05
    Task:
    Выведем количество спутников и названия спутников в строке
    Decision:
    19
    G01G04G11G12G14G18G22G24G25G31G32R05
    Task:
    нужно сделать так, что бы строка G01G04G11G12G14G18G22G24G25G31G32R05 читалась буквам, то есть создаем динамический массив с указателем.
    Decision:
    19
    G01G04G11G12G14G18G22G24G25G31G32R05
    G
    G
    G
    G
    G
    G
    G
    G
    G
    G
    G
    R
    Task:
    Вывести вторую строку.
    Decision:
    19
    G01G04G11G12G14G18G22G24G25G31G32R05
    R06R07R09R15R16R22R23
    Task:
    Вывести буквы спутников в столбец.
    Decision:
    19
    G01G04G11G12G14G18G22G24G25G31G32R05
    R
    R
    R
    R
    R
    R
    R
    Task:
    вывести по буквам и по цифрам. если увидим пробел, тогда операция не должна выполняться.
    Decision:
    19
    G01G04G11G12G14G18G22G24G25G31G32R05
    R
    0
    6
    R
    0
    7
    R
    0
    9
    R
    1
    5
    R
    1
    6
    R
    2
    2
    R
    2
    3
    Task:
    вывести все спутники в столбец.
    Decision:
    19
    G01
    G04
    G11
    G12
    G14
    G18
    G22
    G24
    G25
    G31
    G32
    R05
    R06
    R07
    R09
    R15
    R16
    R22
    R23
    Task:
    Нужно вывести псевдодальности для одной/первой эпохи (время - 0:0).
    Decision:
    19
    G01 G04 G11 G12 G14 G18 G22 G24 G25 G31 G32 R05 R06 R07 R09 R15 R16 R22 R23 2.40771e+07
    2.28061e+07
    2.4023e+07
    2.23889e+07
    2.06979e+07
    2.17159e+07
    2.04115e+07
    2.22393e+07
    2.35478e+07
    2.41313e+07
    2.3745e+07
    2.20981e+07
    1.91242e+07
    2.1007e+07
    2.22533e+07
    2.12016e+07
    1.94745e+07
    2.35659e+07
    2.28647e+07
    Task:
    мы вывели все псевдодальности для каждого спутника, но только для первой эпохи (время - 0:0). Чтобы посчитать для каждой эпохи, давайте переделаем программу, используя векторы. Сначала попробуем также считать и вывести названия спутников для первой эпохи.
    Decision:
    19
    G01
    G04
    G11
    G12
    G14
    G18
    G22
    G24
    G25
    G31
    G32
    R05
    R06
    R07
    R09
    R15
    R16
    R22
    Task:
    вывести псевдодальности для каждого спутника.
    Decision:
    19
    G01 G04 G11 G12 G14 G18 G22 G24 G25 G31 G32 R05 R06 R07 R09 R15 R16 R22 R23
    24077128.1194 22806124.8054 24022983.9984 22388929.9144 20697892.8884 21715906.8854 20411541.2704 22239301.1624 23547808.7034 24131268.6434 23745039.8404 22098086.832 19124242.27 21007031.526 22253250.468 21201583.22 19474489.084 23565923.703 22864670.122 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
    Task:
    Попробуем и для первой и для второй эпохи (время: 0:00 - 0:30) проделать такие операции. Выводится название спутника, и расстояние (дальность) спутника. первая строка - для первой эпохи, вторая строка - для второй эпохи
    Decision:
    19
    19
              G01           G04           G11           G12           G14           G18           G22           G24           G25           G31           G32           R05           R06           R07           R09           R15           R16           R22           R23
    24077128.1194 22806124.8054 24022983.9984 22388929.9144 20697892.8884 21715906.8854 20411541.2704 22239301.1624 23547808.7034 24131268.6434 23745039.8404  22098086.832   19124242.27  21007031.526  22253250.468   21201583.22  19474489.084  23565923.703  22864670.122             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0
    24065818.3394 22804492.2684 24023768.7094 22378483.9394 20691105.4344 21730480.5744 20415751.1104 22255260.6984 23528148.4594 24109572.7604 23726475.0794  22118176.666  19126075.998  20989472.459  22228716.936  21220136.852  19466441.002   23584399.66  22865445.005             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0
    Task:
    наведем теперь порядок в коде, чтобы все переменные были наверху, во вторых, чтобы постоянно вручную не считывать дальность для каждого спутника и для каждой эпохи, давайте изменим программу. Создадим цикл. 4 - 4 эпохи (время 0:00-1:30).
    Decision:
    19
    19
    19
    19
              G01           G04           G11           G12           G14           G18           G22           G24           G25           G31           G32           R05           R06           R07           R09           R15           R16           R22           R23
    24077128.1194 22806124.8054 24022983.9984 22388929.9144 20697892.8884 21715906.8854 20411541.2704 22239301.1624 23547808.7034 24131268.6434 23745039.8404  22098086.832   19124242.27  21007031.526  22253250.468   21201583.22  19474489.084  23565923.703  22864670.122             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0
    24065818.3394 22804492.2684 24023768.7094 22378483.9394 20691105.4344 21730480.5744 20415751.1104 22255260.6984 23528148.4594 24109572.7604 23726475.0794  22118176.666  19126075.998  20989472.459  22228716.936  21220136.852  19466441.002   23584399.66  22865445.005             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0
    24054604.4524 22802978.2284 24024661.7794 22368141.6094 20684380.5444 21745121.8054 20420058.9764 22271281.9174 23508530.9834 24087901.1804 23707941.2624 118341978.858 102070810.568 112264586.751 118569402.732   21238782.99 103944062.756 125994130.633 122319666.917             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0
    24043483.7924 22801583.0244 24025665.3284 22357904.6154 20677717.7854 21759831.8404 20424464.4854 22287364.6624 23488954.4584 24066257.0444 23689442.3374 118449919.114 102081788.249 112171158.621 118438955.294  21257519.389 103902612.851 126093535.929 122325408.502             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0             0
    Task:
    Попробуем вывести для всех эпох данные спутников. Просто регулируем цикл.
    мы отсортировали данные псевдодальности к каждому спутник для всех эпох. нули в столбцах означают, что в спутниках отсутсвуют нужные данные. единственное, в нижнем скриншоте увидим, что появилиись какие-то нули и у них не обозначены спутники. Дело в том, что в программе мы указали размерность вектора массива 100 и в каждом файле принимаемого с приемника в нашем случае irkj1380.txt, количество эпох может быть разное. Поэтому тут нужно регулировать в нашем случае цифру 100. Я, например, посталю значение здесь 55. Уже покрасивее будет выглядеть наша таблица.
    Но давайте выведем на экран данные для двух спутников
    Decision:
    G01 G04 G11 G12 и т.д.
    24077128.119 22806124.805
    24065818.339 22804492.268
    24054604.452 22802978.228
    24043483.792 22801583.024
    24032459.125 22800306.861
    24021529.084 22799148.829
    24010696.527 22798110.763
    23999959.98 22797191.503
    23989317.858 22796390.025
    23978776.092 22795708.426
    23968330.915 22795144.69
    23957983.22 22794700.309
    23947732.989 22794375.481
    23937583.568 22794167.697
    23927530.52 22794080.594
    23917578.625 22794109.516
    23907725.007 22794258.689
    23897972.386 22794526.148
    23888318.516 22794910.502
    23878766.415 22795414.62
    ... ...
    Task:
    вывести данные именно для одного спутника, например первого. Для этого закомментируем ненужные строки.
    Decision:
    24077128.119
    24065818.339
    24054604.452
    24043483.792
    24032459.125
    24021529.084
    24010696.527
    23999959.98
    23989317.858
    23978776.092
    23968330.915
    23957983.22
    23947732.989
    ...
    Task:
    Попробуем просто сделать это все на графическом интерфейсе, вывести данные двух спутников, плюс еще нарисовать графики этих данных для двух спутников.
    Теперь нужно обработать и сортировать данные с геометрической дальностью для спутников. Файл называется igr18451.txt. Вот как выглядят данные, которые нужно отсортировать. Тут уже одна эпоха равняется 15 минутам.
    То что я выделил, это координаты спутника X,Y,Z. Именно их и нужно отсортировать. PG01 - название первого спутника. Попробуем вывести данные именно для него.
    Decision:
    PG04 16571.755106 7091.147720 19268.134634
    PG04 17081.817393 9222.831883 17918.904800
    PG04 17633.922784 11162.951350 16256.328779
    PG04 18194.986791 12883.319207 14310.615388
    PG04 18728.745598 14363.489413 12116.696076
    PG04 19197.262350 15591.290270 9713.525830
    PG04 19562.489921 16563.032391 7143.333216
    PG04 19787.827341 17283.391677 4450.837454
    PG04 19839.609715 17764.980464 1682.449322
    PG04 19688.476298 18027.631365 -1114.528992
    PG04 19310.568086 18097.427949 -3892.691773
    PG04 18688.514262 18005.524210 -6605.294077
    PG04 17812.175758 17786.800613 -9206.983073
    PG04 16679.123638 17478.408399 -11654.488894
    PG04 15294.839581 17118.255856 -13907.269035
    PG04 13672.635233 16743.490595 -15928.101701
    PG04 11833.296299 16389.030589 -17683.624513
    PG04 9804.465824 16086.194088 -19144.815686
    PG04 7619.789018 15861.474593 -20287.415200
    ...
    Task:
    Это мы вывели координаты X,Y,Z и название спутника. Но мне теперь нужно вывести данные только Х для первого спутника.
    Decision:
    14736.411108
    15414.203154
    16201.304785
    17074.555745
    18004.439905
    18956.228081
    19891.328698
    20768.792721
    21546.914135
    ...
    Task:
    Аналогично выводяся только для Y и Z выводим данные. Просто нужно поменять в коде x на y или z.
    Давайте попробуем поситать данные псевдодальности полученные с моего приемника, название файла называется sirfrin1.txt. Вот как выглядит файл:
    Здесь мы уже видим только две данные для спутников. первые, это дальность, которую как раз и нужно отсортировать по каждом НС, а вторые это, видимо, какие-то сигналы спутника, которые нас не особо не интересуют. И эпоха здесь начинается не с нуля, как мы видим, а с 41 минуты 23 секунды. Это тоже нужно учитывать, когда нужно будет сравнивать эти данные с ГД.
    Decision:
    Мы задачу выполнили (отсортировать данные ГД и ПД по каждом навигационному спутнику), остается только обработать эти данные по формулам и посторить по ним графики через Эксель.
    Source:
    https://forum.calculate-linux.org/t/windows-qemu-kvm-libvirt/9357
    https://blog.sedicomm.com/2019/07/21/rdesktop-klient-rdp-dlya-podklyucheniya-rabochego-stola-windows-iz-linux/
    https://learn.microsoft.com/ru-ru/windows-server/administration/openssh/openssh_install_firstuse
    https://learn.microsoft.com/ru-ru/powershell/scripting/learn/remoting/ssh-remoting-in-powershell-core?view=powershell-7.3
    https://learn.microsoft.com/en-us/powershell/module/storage/dismount-diskimage?view=windowsserver2022-ps
    https://superuser.com/questions/499264/how-can-i-mount-an-iso-via-powershell-programmatically
    https://winitpro.ru/index.php/2016/03/31/sftp-ssh-ftp-na-windows-server-2012-r2/
    https://mhelp.pro/ru/kak-zapustit-powershell-ot-imeni-administratora/?ysclid=lmzwqntxfi559273426
    https://remontka.pro/text-files-cmd-powershell/?ysclid=lmzy2p5172409325479
    https://www.digitalocean.com/community/tutorials/sftp-ru
    http://ftp.glonass-iac.ru/guide/navfaq.php
    https://ru.wtuseripedia.org/wtuseri/%D0%A1%D0%BF%D1%83%D1%82%D0%BD%D0%B8%D0%BA%D0%BE%D0%B2%D0%B0%D1%8F_%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D0%B0_%D0%BD%D0%B0%D0%B2%D0%B8%D0%B3%D0%B0%D1%86%D0%B8%D0%B8
    Харисов В.Н. Глобальная спутниковая радионавигационная система ГЛОНАСС
    Грудинская Г.П. Распространение радиоволн
    http://begin.clan.su/news/speciftusera_provedenija_psevdodalnomernykh_i_fazov/2013-09-18-135
    http://rrv.iszf.irk.ru/sites/default/files/conf2014/articles/tom2/17-20.pdf
    http://www.u-blox.com
    Дэвис К. Радиоволны в ионосфере
    https://www.youtube.com/watch?v=bDvVosvyVp0&list=LL&index=315

Разработка сайта
	Task:
	Для разработки своего сайта, куда публиковал все решенные мной интересные задачи, отчеты лабораторных работ и презентации, сверстал простой сайт, установил в Qemu-kvm виртуальную машину с операционной системой Ubuntu, в котором развернул тестовый веб-сервер Lamp с своей базой данных на Mysql, после развернул LAMP в сервисе Timeweb.clouds, в данный сервис перенес данные веб-сервера (база и структура сайта) с виртуальной машины и зарегистрировал домен
	Decision:
	$ wget https://ubuntu.com/download/desktop/thank-you?version=22.04.3&architecture=amd64
	$ sudo dnf install https://kojihub.stream.centos.org/kojifiles/packages/libvirt-python/8.5.0/2.el9/x86_64/
	$ sudo dnf install libguestfs-tools
	$ sudo dnf install libvirt-python
	$ sudo systemctl enable libvirtd
	$ sudo systemctl start libvirtd
	$ systemctl status libvirtd
	$ lsmod | grep kvm
	$ sudo ls -l /ПУТЬ/
	$ sudo ls -l /var/lib/libvirt/iso
	$ rpm -qa | grep bridge-utils
	$ sudo yum install bridge-utils
	$ ip a
	$ brctl show
	$ systemctl enable --now libvirtd
	$ nmcli connection add type bridge autoconnect yes con-name br0 ifname br0
	$ sudo su
	# nmcli connection modify br0 ipv4.addresses 10.0.0.30/24 ipv4.method manual
	# nmcli connection modify br0 ipv4.gateway 10.0.0.1
	# nmcli connection modify br0 ipv4.dns 10.0.0.10
	# nmcli connection modify br0 ipv4.dns-search srv.world
	# nmcli connection up enp0s21f0u1u4
	# nmcli connection del enp0s21f0u1u4
	# nmcli device
	# nmcli connection add type bridge-slave autoconnect yes con-name enp0s21f0u1u4 ifname enp0s21f0u1u4 master br0
	# nmcli device
	# reboot
	# ip addr
	...
	5: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
	    link/ether 52:54:00:b7:00:3c brd ff:ff:ff:ff:ff:ff
	    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0
	       valid_lft forever preferred_lft forever
	# sudo virsh net-list
	 Имя       Состояние   Автозапуск   Постоянный
	------------------------------------------------
	 default   активен     yes          yes
	# virsh net-edit default
	<network>
	  <name>default</name>
	  <uuid>745f121b-3c89-1281-a19a-aa62808c4241</uuid>
	  <forward mode="nat">
	  <bridge delay="0" name="virbr0" stp="on">
	  <mac address="16:18:00:21:8c:15">
	  <ip address="192.168.122.1" netmask="255.255.255.0">
	    <dhcp>
	      <range end="192.168.122.254" start="192.168.122.2">
	    </range></dhcp>
	  </ip>
	</mac></bridge></forward></network>
	# virt-install \
	--network network=default \
	--name ubuntu22.04 \
	--ram 2048 \
	--vcpus 2 \
	--disk path=/ПУТЬ/ubuntu22.04.img,size=75,format=qcow2 \
	--graphics vnc,password=tpassword \
	--location /var/lib/libvirt/iso/CentOS-Stream-9-latest-x86_64-dvd1.iso \
	--boot cdrom,hd,menu=on 
	WARNING  Не удалось подключиться к графической консоли. Для этого необходимо установить virt-viewer.
	...
	# dnf install virt-viewer
	# virt-install \
	--network network=default \
	--name ubuntu22.04 \
	--ram 2048 \
	--vcpus 2 \
	--disk path=/ПУТЬ/ubuntu22.04.img,size=50,format=qcow2 \
	--graphics vnc,password=tpassword \
	--location /var/lib/libvirt/iso/CentOS-Stream-9-latest-x86_64-dvd1.iso \
	--boot cdrom,hd,menu=on 
	Запуск установки...
	Получение «vmlinuz»                                        |  11 MB  00:00     
	Получение «initrd.img»                                     |  79 MB  00:00     
	Выделение «ubuntu22.04.img»                                    |  50 GB  00:20     
	WARNING  Переопределение объёма памяти на 3072 МиБ, необходимых для установки centos-stream9 с помощью сети.
	Удаление диска «ubuntu22.04.img»                               |         00:00     
	ERROR    конфигурация не поддерживается: VNC password is 9 characters long, only 8 permitted
	Возможно, установка домена завершилась неудачей. 
	Если вы уверены, что установка прошла нормально, перезапустите домен:
	 virsh --connect qemu:///system start ubuntu22.04
	В противном случае начните процесс установки заново.
	# virt-install \
	--network network=default \
	--name ubuntu22.04 \
	--ram 2048 \
	--vcpus 2 \
	--disk path=/ПУТЬ/ubuntu22.04.img,size=50,format=qcow2 \
	--graphics vnc \
	--location /var/lib/libvirt/iso/Ubuntu.iso \
	--boot cdrom,hd,menu=on 
	# virt-install \
	--name ubuntu22.04 \
	--ram 2048 \
	--disk path=/ПУТЬ/ubuntu22.04.img,size=50 \
	--vcpus 2 \
	--os-variant Ubuntu \
	--network bridge=br0 \
	--graphics none \
	--console pty,target_type=serial \
	--location /var/lib/libvirt/iso/Ubuntu.iso \
	--extra-args 'console=ttyS0,115200n8'
	$ sudo virsh list --all
	$ sudo virsh start ubuntu22.04
	$ virt-viewer --connect qemu:///system --wait ubuntu22.04
	$ sudo virsh vncdisplay ubuntu22.04
	127.0.0.1:0
	$ sudo virsh domblklist ubuntu22.04
	 Назначение   Источник
	---------------------------------------------------
	 vda          /ПУТЬ/ubuntu22.04.img
	 sda          -
	$ ssh -X tuser@IP
	Decision:
	$ sudo apt update
	$ sudo apt install apache2
	$ sudo ufw app list
	Available applications:
	  Apache
	  Apache Full
	  Apache Secure
	  CUPS
	  OpenSSH
	$ sudo ufw enable
	$ sudo ufw allow in "Apache"
	$ sudo ufw allow in "OpenSSH"
	$ sudo ufw status
	Status: active
	To                         Action      From
	--                         ------      ----
	Apache                     ALLOW       Anywhere                  
	Apache Full                ALLOW       Anywhere                  
	OpenSSH                    ALLOW       Anywhere                  
	Apache (v6)                ALLOW       Anywhere (v6)             
	Apache Full (v6)           ALLOW       Anywhere (v6)             
	OpenSSH (v6)               ALLOW       Anywhere (v6)
	$ firefox
	update.go:85: cannot change mount namespace according to change mount (/var/lib/snapd/hostfs/usr/share/gimp/2.0/help /usr/share/gimp/2.0/help none bind,ro 0 0): cannot open directory "/var/lib": permission denied
	update.go:85: cannot change mount namespace according to change mount (/var/lib/snapd/hostfs/usr/share/xubuntu-docs /usr/share/xubuntu-docs none bind,ro 0 0): cannot open directory "/var/lib": permission denied
	X11 connection rejected because of wrong authentication.
	X11 connection rejected because of wrong authentication.
	X11 connection rejected because of wrong authentication.
	X11 connection rejected because of wrong authentication.
	X11 connection rejected because of wrong authentication.
	X11 connection rejected because of wrong authentication.
	Error: cannot open display: localhost:10.0
	Decision:
	$ wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
	$ sudo dpkg -i --force-depends google-chrome-stable_current_amd64.deb
	$ google-chrome http://127.0.0.1
	Decision:
	$ sudo apt install mysql-server
	$ sudo mysql
	mysql> ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'tpassword';
	mysql> exit
	$ sudo mysql_secure_installation
	$ sudo mysql
	ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO)
	$ sudo mysql -u root -p
	mysql> exit
	Decision:
	$ sudo apt install php libapache2-mod-php php-mysql
	$ php -v
	$ cat /etc/php/8.1/apache2/php.ini | grep upload_max_filesize
	upload_max_filesize = 2M
	$ cat /etc/php/8.1/apache2/php.ini | grep post_max_size
	post_max_size = 8M
	$ cat /etc/php/8.1/apache2/php.ini | grep memory_limit
	memory_limit = 128M
	$ cat /etc/php/8.1/apache2/php.ini | grep max_execution_time
	max_execution_time = 30
	$ cat /etc/php/8.1/apache2/php.ini | grep max_input_vars
	;max_input_vars = 1000
	$ cat /etc/php/8.1/apache2/php.ini | grep max_input_time
	; max_input_time
	max_input_time = 60
	$ sudo vim /etc/php/8.1/apache2/php.ini
	$ cat /etc/php/8.1/apache2/php.ini | grep upload_max_filesize
	upload_max_filesize = 32M
	$ cat /etc/php/8.1/apache2/php.ini | grep post_max_size
	post_max_size = 48M
	$ cat /etc/php/8.1/apache2/php.ini | grep memory_limit
	memory_limit = 256M
	$ cat /etc/php/8.1/apache2/php.ini | grep max_execution_time
	max_execution_time = 600
	$ cat /etc/php/8.1/apache2/php.ini | grep max_input_vars
	max_input_vars = 3000
	$ cat /etc/php/8.1/apache2/php.ini | grep max_input_time
	; max_input_time
	max_input_time = 1000
	$ sudo service apache2 restart
	Decision:
	$ sudo mkdir /var/www/tdomain
	$ sudo chown -R $USER:$USER /var/www/tdomain
	$ sudo vim /etc/apache2/sites-available/tdomain.conf
	$ cat /etc/apache2/sites-available/tdomain.conf
	<VirtualHost *:80>
	    ServerName tdomain
	    ServerAlias www.tdomain
	    ServerAdmin webmaster@localhost
	    DocumentRoot /var/www/tdomain
	    ErrorLog ${APACHE_LOG_DIR}/error.log
	    CustomLog ${APACHE_LOG_DIR}/access.log combined
	</VirtualHost>
	$ sudo a2ensite tdomain
	$ sudo a2dissite 000-default
	$ sudo apache2ctl configtest
	$ sudo systemctl reload apache2
	$ vim /var/www/tdomain/index.html
	$ cat /var/www/tdomain/index.html
	<html>
	  <head>
	    <title>tdomain website</title>
	  </head>
	  <body>
	    <h1>Hello World!</h1>
	    <p>This is the landing page of <strong>tdomain</strong>.</p>
	  </body>
	</html>
	$ google-chrome http://127.0.0.1
	Decision:
	$ cat /etc/apache2/mods-enabled/dir.conf
	<IfModule mod_dir.c>
	    DirectoryIndex index.html index.cgi index.pl index.php index.xhtml index.htm
	</IfModule>
	# vim: syntax=apache ts=4 sw=4 sts=4 sr noet
	$ sudo vim /etc/apache2/mods-enabled/dir.conf
	$ cat /etc/apache2/mods-enabled/dir.conf
	<IfModule mod_dir.c>
	    DirectoryIndex index.php index.html index.cgi index.pl index.php index.xhtml index.htm
	</IfModule>
	# vim: syntax=apache ts=4 sw=4 sts=4 sr noet
	$ sudo systemctl reload apache2
	$ vim /var/www/tdomain/info.php
	$ cat /var/www/tdomain/info.php
	<?php
	phpinfo();
	$ google-chrome http://127.0.0.1/info.php
	$ sudo rm /var/www/tdomain/info.php
	Decision:
	https://dbdesigner.page.link/K8tSW7SdzLgPHV2S9
	$ sudo mysql -u root -p
	mysql> create database tbase;
	mysql> CREATE USER 'tuser'@'%' IDENTIFIED BY 'tpassword';
	mysql> GRANT ALL ON tbase.* TO 'tuser'@'%';
	mysql> exit
	$ mysql -u tuser -p
	mysql> show databases;
	+--------------------+
	| Database           |
	+--------------------+
	| information_schema |
	| performance_schema |
	| tbase              |
	+--------------------+
	3 rows in set (0.07 sec)
	mysql> create table tbase.ttable1
	    (ttable1_id INT PRIMARY KEY AUTO_INCREMENT,
	    content VARCHAR(255));
	CREATE TABLE tbase.ttable1
	    (ttable1_id INT PRIMARY KEY AUTO_INCREMENT,
	    column1 VARCHAR(255),
	    column2 VARCHAR(255),
	    column3 VARCHAR(255),
	    column4 VARCHAR(255),
	    column5 Date,
	    column6 Date);
	mysql> INSERT INTO tbase.ttable1
	    (column1, column2)
	VALUES
	    ("text1","text2"),
	    ("text3","text4"),
	    ("text5","text6"),
	    ("text7","text8");
	mysql> select * from tbase.ttable1;
	+------------+---------+---------+---------+---------+---------+---------+
	| ttable1_id | column1 | column2 | column3 | column4 | column5 | column6 |
	+------------+---------+---------+---------+---------+---------+---------+
	|          1 | text1   | text2   | NULL    | NULL    | NULL    | NULL    |
	|          2 | text3   | text4   | NULL    | NULL    | NULL    | NULL    |
	|          3 | text5   | text6   | NULL    | NULL    | NULL    | NULL    |
	|          4 | text7   | text8   | NULL    | NULL    | NULL    | NULL    |
	+------------+---------+---------+---------+---------+---------+---------+
	4 rows in set (0.00 sec)
	mysql> CREATE TABLE tbase.ttable2
	    (ttable2_id INT PRIMARY KEY AUTO_INCREMENT,
	    columnId INT,
	    column1 VARCHAR(500),
	    FOREIGN KEY (columnId) REFERENCES tbase.ttable1 (ttable1_id));
	mysql> INSERT INTO tbase.ttable2
	    (columnId, column1)
	VALUES
	    (1, "text9"),
	    (1, "text10"),
	    (2, "text11"),
	    (3, "text12"),
	    (4, "text13");
	mysql> select * from tbase.ttable2;
	+------------+----------+---------+
	| ttable2_id | columnId | column1 |
	+------------+----------+---------+
	|          1 |        1 | text9   |
	|          2 |        1 | text10  |
	|          3 |        2 | text11  |
	|          4 |        3 | text12  |
	|          5 |        4 | text13  |
	+------------+----------+---------+
	5 rows in set (0.00 sec)
	mysql> select * from tbase.ttable2
	    inner join tbase.ttable1
	    on ttable1.ttable1_id = ttable2.columnId;
	+------------+----------+---------+------------+---------+---------+---------+---------+---------+---------+
	| ttable2_id | columnId | column1 | ttable1_id | column1 | column2 | column3 | column4 | column5 | column6 |
	+------------+----------+---------+------------+---------+---------+---------+---------+---------+---------+
	|          1 |        1 | text9   |          1 | text1   | text2   | NULL    | NULL    | NULL    | NULL    |
	|          2 |        1 | text10  |          1 | text1   | text2   | NULL    | NULL    | NULL    | NULL    |
	|          3 |        2 | text11  |          2 | text3   | text4   | NULL    | NULL    | NULL    | NULL    |
	|          4 |        3 | text12  |          3 | text5   | text6   | NULL    | NULL    | NULL    | NULL    |
	|          5 |        4 | text13  |          4 | text7   | text8   | NULL    | NULL    | NULL    | NULL    |
	+------------+----------+---------+------------+---------+---------+---------+---------+---------+---------+
	5 rows in set (0.00 sec)
	mysql> exit
	$ vim /var/www/tdomain/index.php
	$ cat /var/www/tdomain/index.php
	<?php
	$user = "tuser";
	$password = "tpassword";
	$database = "tbase";
	$table1 = "ttable1";
	$table2 = "ttable2";
	try {
	    $db = new PDO("mysql:host=localhost;dbname=$database", $user, $password);
	    $query = $db->query("
	        select * from $database.$table2
	        inner join $database.$table1
	        on $table1.experience_id=$table2.experienceId;
	    ");
	    foreach($query as $row) {
	        echo '
	        <li><strong>
	        ' . $row['column1'] . '
	        -
	        ' . $row['column2'] . '
	        :</strong>
	        ' . $row['column3'] . '
	        ;
	        ' . $row['column4'] . '
	        ;
	        ' . $row['column5'] . '
	        ;
	        ' . $row['column6'] . '
	        </li>
	        ';
	    }
	} catch (PDOException $e) {
	    print "Error!: " . $e->getMessage() . "<br/>";
	    die();
	}
	$ google-chrome http://127.0.0.1/index.php
	Decision:
	$ sudo apt update
	$ sudo apt install phpmyadmin php-mbstring php-zip php-gd php-json php-curl
	$ sudo apt install php8.1-fpm php8.1 libapache2-mod-php8.1 php8.1-common php8.1-mysql php8.1-xml php8.1-xmlrpc php8.1-imagick php8.1-cli php8.1-imap php8.1-opcache php8.1-soap php8.1-intl php8.1-bcmath unzip
	...
	granting access to database phpmyadmin for phpmyadmin@localhost: failed.
	error encountered creating user:
	mysql said: ERROR 1819 (HY000) at line 1: Your password does not satisfy the cur
	rent policy requirements
	dbconfig-common: phpmyadmin configure: aborted.
	dbconfig-common: flushing administrative password
	dpkg: error processing package phpmyadmin (--configure):
	 installed phpmyadmin package post-installation script subprocess returned error
	 exit status 1
	Processing triggers for libapache2-mod-php8.1 (8.1.2-1ubuntu2.14) ...
	...
	Errors were encountered while processing:
	 phpmyadmin
	E: Sub-process /usr/bin/dpkg returned an error code (1)
	$ mysql -u root -p
	mysql> UNINSTALL COMPONENT "file://component_validate_password";
	mysql> exit
	$ sudo apt install phpmyadmin
	...
	granting access to database phpmyadmin for phpmyadmin@localhost: success.
	verifying access for phpmyadmin@localhost: success.
	creating database phpmyadmin: success.
	verifying database phpmyadmin exists: success.
	populating database via sql...  done.
	dbconfig-common: flushing administrative password
	$ mysql -u root -p
	mysql> INSTALL COMPONENT "file://component_validate_password";
	mysql> exit
	$ sudo phpenmod mbstring
	$ sudo systemctl restart apache2
	$ mysql -u root -p
	mysql> SELECT user,authentication_string,plugin,host FROM mysql.user;
	mysql> exit
	$ sudo cp /etc/phpmyadmin/apache.conf /etc/apache2/conf-available/phpmyadmin.conf
	$ sudo a2enconf phpmyadmin.conf
	$ systemctl reload apache2
	$ google-chrome http://127.0.0.1/phpmyadmin
	Decision:
	$ google-chrome https://hosting.timeweb.ru/
	$ ssh -X root@IP
	# sudo passwd root
	# sudo apt update
	# sudo apt upgrade
	# adduser tuser
	# usermod -aG sudo tuser
	# ufw app list
	Available applications:
	  OpenSSH
	# ufw allow OpenSSH
	# ufw enable
	# ufw status
	Status: active
	To                         Action      From
	--                         ------      ----
	OpenSSH                    ALLOW       Anywhere                  
	OpenSSH (v6)               ALLOW       Anywhere (v6)   
	# exit
	$ ssh -X tuser@IP
	$ sudo apt install apache2
	$ sudo ufw app list
	Available applications:
	  Apache
	  Apache Full
	  Apache Secure
	  OpenSSH
	$ sudo ufw allow in "Apache Full"
	Rule added
	Rule added (v6)
	$ google-chrome http://IP
	$ sudo apt install mysql-server
	$ sudo mysql_secure_installation
	$ sudo mysql
	mysql> ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'tP@ssw0rd';
	mysql> CREATE USER 'tuser'@'%' IDENTIFIED BY 'tP@ssw0rd';
	mysql> CREATE DATABASE tdomain;
	mysql> GRANT ALL ON tdomain.* TO 'tuser'@'%'
	mysql> exit
	$ sudo apt install php libapache2-mod-php php-mysql
	$ sudo mkdir /var/www/tdomain
	$ sudo chown -R $USER:$USER /var/www/tdomain
	$ sudo vim /etc/apache2/sites-available/tdomain.conf
	$ cat /etc/apache2/sites-available/tdomain.conf
	<VirtualHost *:80>
	    ServerName tdomain
	    ServerAlias www.tdomain
	    ServerAdmin webmaster@localhost
	    DocumentRoot /var/www/tdomain
	    ErrorLog ${APACHE_LOG_DIR}/error.log
	    CustomLog ${APACHE_LOG_DIR}/access.log combined
	</VirtualHost>
	$ sudo vim /etc/apache2/sites-available/tdomain.conf
	$ sudo a2ensite tdomain
	$ sudo a2dissite 000-default
	$ sudo systemctl reload apache2
	$ vim /var/www/tdomain/index.php
	$ google-chrome http://IP
	Task:
	Эспорт и импорт баз данных
	Decision:
	$ mysql -V
	mysql  Ver 8.0.34-0ubuntu0.22.04.1 for Linux on x86_64 ((Ubuntu))
	$ exit
	$ mysql -V
	mysql  Ver 8.0.34-0ubuntu0.22.04.1 for Linux on x86_64 ((Ubuntu))
	$ sudo mysql -u root
	mysql> SELECT User, Host FROM mysql.user;
	    +------------------+-----------+
	    | User             | Host      |
	    +------------------+-----------+
	    | root             | localhost |
	    | tuser            | localhost |
		+------------------+-----------+
	    6 rows in set (0.43 sec)
	mysql> SHOW DATABASES;
	    +--------------------+
	    | Database           |
	    +--------------------+
	    | tdomain            |
	    +--------------------+
	    6 rows in set (0.80 sec)
	mysql> exit
	$ mysqldump -u tuser -p tdb > data-dump.sql
	Enter password:
	    mysqldump: Got error: 1045: Access denied for user 'tuser'@'localhost' (using password: YES) when trying to connect
	$ mysqldump -u root -p tdomain > tdomain.sql
	Enter password:
	    mysqldump: Got error: 1698: Access denied for user 'root'@'localhost' when trying to connect
	$ mysqldump -u root -p --opt tdomain > tdomain.sql
	$ ls tdomain.sql
	tdomain.sql
	$ scp tdomain.sql tuser@IP:/home/tuser
	$ scp -r /var/www/tdomain/* tuser@IP:/var/www/tdomain/
	$ ssh -X tuser@IP
	$ mysql -u root -p tdomain < tdomain.sql
	$ mysql -u tuser -p
	mysql> show databases;
	+--------------------+
	| Database           |
	+--------------------+
	| tdomain            |
	| information_schema |
	| performance_schema |
	+--------------------+
	mysql> USE tdomain;
	$ sudo apt install phpmyadmin php-mbstring php-zip php-gd php-json php-curl
	$ mysql -u root -p
	mysql> UNINSTALL COMPONENT "file://component_validate_password";
	mysql> exit
	$ sudo apt install phpmyadmin
	$ sudo apt install php8.1-fpm php8.1 libapache2-mod-php8.1 php8.1-common php8.1-mysql php8.1-xml php8.1-xmlrpc php8.1-imagick php8.1-cli php8.1-imap php8.1-opcache php8.1-soap php8.1-intl php8.1-bcmath unzip
	$ mysql -u root -p
	mysql> INSTALL COMPONENT "file://component_validate_password";
	mysql> exit
	$ sudo phpenmod mbstring
	$ sudo systemctl restart apache2
	$ sudo cp /etc/phpmyadmin/apache.conf /etc/apache2/conf-available/phpmyadmin.conf
	$ sudo a2enconf phpmyadmin.conf
	$ exit
	$ google-chrome http://IP/ &
	$ google-chrome http://IP/phpmyadmin & 
	Decision:
	$ exit
	# virsh shutdown ubuntu22.04
	Source:
	https://winitpro.ru/index.php/2020/02/10/virsh-upravlenie-virtualnymi-mashinami-kvm/
	https://winitpro.ru/index.php/2020/02/04/ustanovka-zapusk-kvm-v-linux-centos/
	https://www.server-world.info/en/note?os=CentOS_Stream_9&p=kvm&f=1
	https://www.server-world.info/en/note?os=CentOS_Stream_9&p=initial_conf&f=3
	https://www.server-world.info/en/note?os=CentOS_Stream_9&p=kvm&f=2
	https://bozza.ru/art-260.html
	https://www.digitalocean.com/community/tutorials/how-to-install-linux-apache-mysql-php-lamp-stack-on-ubuntu-22-04
	https://losst.pro/ustanovka-chrome-v-ubuntu-18-04?ysclid=lmdflkvc3v463974954
	https://qna.habr.com/q/439469?ysclid=lmdgkb49q827401606
	https://steptuser.org/course/63054/syllabus
	https://metanit.com/sql/mysql/2.5.php
	https://metanit.com/sql/mysql/5.2.php
	https://www.digitalocean.com/community/tutorials/how-to-install-and-secure-phpmyadmin-on-ubuntu-20-04
	https://serverspace.ru/support/help/osnovnye-komandy-ufw/
	https://www.tutsmake.com/how-to-install-lamp-apache-mysql-php-in-ubuntu-22-04/
	https://www.php.net/manual/ru/faq.html.php
	https://www.8host.com/blog/kak-rabotayut-stroki-v-php/
	https://www.youtube.com/watch?v=FxwPQkP3OGY&t=611s
	https://timeweb.com/ru/community/articles/kak-ustanovit-stek-lamp-na-ubuntu-20-04
	https://wiki.merionet.ru/articles/perenos-bazy-dannyx-mysql-so-starogo-na-novyj-server